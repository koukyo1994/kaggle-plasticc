{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hidehisa/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from multiprocessing import Pool\n",
    "tqdm.pandas(desc=\"apply progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hidehisa/.kaggle/competitions/plasticc\"\n",
    "train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>ra</th>\n",
       "      <th>decl</th>\n",
       "      <th>gal_l</th>\n",
       "      <th>gal_b</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hostgal_specz</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>349.046051</td>\n",
       "      <td>-61.943836</td>\n",
       "      <td>320.796530</td>\n",
       "      <td>-51.753706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713</td>\n",
       "      <td>53.085938</td>\n",
       "      <td>-27.784405</td>\n",
       "      <td>223.525509</td>\n",
       "      <td>-54.460748</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8181</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>45.4063</td>\n",
       "      <td>0.007</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730</td>\n",
       "      <td>33.574219</td>\n",
       "      <td>-6.579593</td>\n",
       "      <td>170.455585</td>\n",
       "      <td>-61.548219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>40.2561</td>\n",
       "      <td>0.021</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>-45.586655</td>\n",
       "      <td>328.254458</td>\n",
       "      <td>-68.969298</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3037</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>40.7951</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1124</td>\n",
       "      <td>352.711273</td>\n",
       "      <td>-63.823658</td>\n",
       "      <td>316.922299</td>\n",
       "      <td>-51.059403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>40.4166</td>\n",
       "      <td>0.024</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id          ra       decl       gal_l      gal_b  ddf  \\\n",
       "0        615  349.046051 -61.943836  320.796530 -51.753706    1   \n",
       "1        713   53.085938 -27.784405  223.525509 -54.460748    1   \n",
       "2        730   33.574219  -6.579593  170.455585 -61.548219    1   \n",
       "3        745    0.189873 -45.586655  328.254458 -68.969298    1   \n",
       "4       1124  352.711273 -63.823658  316.922299 -51.059403    1   \n",
       "\n",
       "   hostgal_specz  hostgal_photoz  hostgal_photoz_err  distmod  mwebv  target  \n",
       "0         0.0000          0.0000              0.0000      NaN  0.017      92  \n",
       "1         1.8181          1.6267              0.2552  45.4063  0.007      88  \n",
       "2         0.2320          0.2262              0.0157  40.2561  0.021      42  \n",
       "3         0.3037          0.2813              1.1523  40.7951  0.007      90  \n",
       "4         0.1934          0.2415              0.0176  40.4166  0.024      90  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "    agg_flux_diff = agg_df.reset_index()[[\"object_id\", \"flux_diff\"]]\n",
    "    df2 = pd.merge(df, agg_df, how=\"left\", on=\"object_id\")\n",
    "    df2[\"flux_norm\"] = df2.flux / df2.flux_diff\n",
    "    del df2[\"flux\"]\n",
    "    fcp = {\n",
    "        'fft_coefficient': [{\n",
    "            'coeff': 0,\n",
    "            'attr': 'abs'\n",
    "        }, {\n",
    "            'coeff': 1,\n",
    "            'attr': 'abs'\n",
    "        }],\n",
    "        'kurtosis':\n",
    "        None,\n",
    "        'skewness':\n",
    "        None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp2 = {\n",
    "        \"fft_coefficient\": [{\n",
    "            \"coeff\": 0,\n",
    "            \"attr\": \"abs\"\n",
    "        }, {\n",
    "            \"coeff\": 1,\n",
    "            \"attr\": \"abs\"\n",
    "        }],\n",
    "        \"abs_energy\": None,\n",
    "        \"sample_entropy\": None\n",
    "    }\n",
    "    fcp_flux = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp_flux_by_flux_ratio_sq = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None\n",
    "    }\n",
    "    agg_df_ts = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp,\n",
    "        n_jobs=6)\n",
    "    agg_df_ts2 = extract_features(\n",
    "        df2,\n",
    "        column_id=\"object_id\",\n",
    "        column_sort=\"mjd\",\n",
    "        column_kind=\"passband\",\n",
    "        column_value=\"flux_norm\",\n",
    "        default_fc_parameters=fcp2,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_flux = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux\",\n",
    "        default_fc_parameters=fcp_flux,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_ffrs = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux_by_flux_ratio_sq\",\n",
    "        default_fc_parameters=fcp_flux_by_flux_ratio_sq,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters={\n",
    "            'maximum': None,\n",
    "            'minimum': None\n",
    "        },\n",
    "        n_jobs=8)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd[\n",
    "        'mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts2.columns = pd.Index([e + \"_norm\" for e in agg_df_ts2.columns])\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd, on='id')\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ts2, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_flux, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ffrs, on=\"id\")\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    agg_df_ts.index.rename('object_id', inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts, on='object_id')\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def cluster_mean_diff(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\"]).agg({\n",
    "        \"flux\": [\"mean\", \"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in new_df.columns.tolist()])\n",
    "    new_df[\"normalized_mean\"] = new_df[\"flux_mean\"] / (\n",
    "        new_df[\"flux_max\"] - new_df[\"flux_min\"])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df.groupby(\"object_id\").agg({\"normalized_mean\": \"std\"})\n",
    "\n",
    "\n",
    "def passband_std_difference(df):\n",
    "    std_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": \"std\"\n",
    "    }).reset_index().groupby([\"object_id\",\n",
    "                              \"passband\"])[\"flux\"].mean().reset_index()\n",
    "    std_df_max = std_df.groupby(\"object_id\")[\"flux\"].max()\n",
    "    std_df_min = std_df.groupby(\"object_id\")[\"flux\"].min()\n",
    "    return (std_df_max / std_df_min).reset_index()\n",
    "\n",
    "\n",
    "def num_outliers(df):\n",
    "    new_df = df.groupby(\"object_id\").agg({\"flux\": [\"mean\", \"std\"]})\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df[\"upper_sigma\"] = new_df[\"flux_mean\"] + new_df[\"flux_std\"]\n",
    "    new_df[\"upper_2sigma\"] = new_df[\"flux_mean\"] + 2 * new_df[\"flux_std\"]\n",
    "    new_df[\"lower_sigma\"] = new_df[\"flux_mean\"] - new_df[\"flux_std\"]\n",
    "    new_df[\"lower_2sigma\"] = new_df[\"flux_mean\"] - 2 * new_df[\"flux_std\"]\n",
    "    new_df.drop([\"flux_mean\", \"flux_std\"], axis=1, inplace=True)\n",
    "    new_df = pd.merge(df, new_df, how=\"left\", on=\"object_id\")\n",
    "    new_df[\"outside_sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_sigma\"])).astype(int)\n",
    "    new_df[\"outside_2sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_2sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_2sigma\"])).astype(int)\n",
    "\n",
    "    return_df = new_df.groupby(\"object_id\").agg({\n",
    "        \"outside_sigma\": \"sum\",\n",
    "        \"outside_2sigma\": \"sum\"\n",
    "    })\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta(meta_df):\n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full(df, meta):\n",
    "    agg_basic = basic(df)\n",
    "    cl_mean_diff = cluster_mean_diff(df)\n",
    "    ps_std_diff = passband_std_difference(df)\n",
    "    num_out = num_outliers(df)\n",
    "\n",
    "    full = pd.merge(agg_basic, cl_mean_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, ps_std_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, num_out, how=\"left\", on=\"object_id\")\n",
    "    meta = process_meta(meta)\n",
    "    full = pd.merge(full, meta, how=\"left\", on=\"object_id\")\n",
    "    if \"target\" in full.columns:\n",
    "        full.drop(\"target\", axis=1, inplace=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_data(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = meta.target\n",
    "    classes = sorted(y.unique())\n",
    "    class_weight = {c: 1 for c in classes}\n",
    "\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "    oof_df = full[[\"object_id\"]]\n",
    "    del full['object_id'], full['distmod'], full['hostgal_specz']\n",
    "    del full['ra'], full['decl'], full['gal_l'], full['gal_b'], full['ddf']\n",
    "    return full, y, classes, class_weight, oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.74it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  2.77it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 18.00it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 27.38it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 107.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 1.34 s, total: 22.5 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full, y, classes, class_weight, oof_df = train_data(train, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = full.mean(axis=0)\n",
    "full.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"flux_norm_fft_\"\n",
    "for i in range(5):\n",
    "    for j in range(1, 6):\n",
    "        if j > i:\n",
    "            full[f\"{base_name}{j}_by_{i}\"] = full[f\"{j}__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\"] / \\\n",
    "                full[f\"{i}__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flux_err_std', 'flux_err_skew', 'detected_mean', 'flux_ratio_sq_sum',\n",
       "       'flux_ratio_sq_skew', 'flux_by_flux_ratio_sq_sum',\n",
       "       'flux_by_flux_ratio_sq_skew', 'flux_diff', 'flux_dif2', 'flux_w_mean',\n",
       "       'flux_dif3', '0__cid_ce__normalize_True',\n",
       "       '0__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '0__fft_coefficient__coeff_1__attr_\"abs\"', '0__kurtosis', '0__skewness',\n",
       "       '1__cid_ce__normalize_True', '1__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '1__fft_coefficient__coeff_1__attr_\"abs\"', '1__kurtosis', '1__skewness',\n",
       "       '2__cid_ce__normalize_True', '2__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '2__fft_coefficient__coeff_1__attr_\"abs\"', '2__kurtosis', '2__skewness',\n",
       "       '3__cid_ce__normalize_True', '3__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '3__fft_coefficient__coeff_1__attr_\"abs\"', '3__kurtosis', '3__skewness',\n",
       "       '4__cid_ce__normalize_True', '4__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '4__fft_coefficient__coeff_1__attr_\"abs\"', '4__kurtosis', '4__skewness',\n",
       "       '5__cid_ce__normalize_True', '5__fft_coefficient__coeff_0__attr_\"abs\"',\n",
       "       '5__fft_coefficient__coeff_1__attr_\"abs\"', '5__kurtosis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.columns[10:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"abs_energy_\"\n",
    "for i in range(5):\n",
    "    for j in range(1, 6):\n",
    "        if j > i:\n",
    "            full[f\"{base_name}{j}_by_{i}\"] = full[f\"{j}__abs_energy_norm\"] / full[f\"{i}__abs_energy_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>...</th>\n",
       "      <th>abs_energy_2_by_1</th>\n",
       "      <th>abs_energy_3_by_1</th>\n",
       "      <th>abs_energy_4_by_1</th>\n",
       "      <th>abs_energy_5_by_1</th>\n",
       "      <th>abs_energy_3_by_2</th>\n",
       "      <th>abs_energy_4_by_2</th>\n",
       "      <th>abs_energy_5_by_2</th>\n",
       "      <th>abs_energy_4_by_3</th>\n",
       "      <th>abs_energy_5_by_3</th>\n",
       "      <th>abs_energy_5_by_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>3.835268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439020</td>\n",
       "      <td>0.248155</td>\n",
       "      <td>0.172018</td>\n",
       "      <td>0.170629</td>\n",
       "      <td>0.565249</td>\n",
       "      <td>0.391823</td>\n",
       "      <td>0.388660</td>\n",
       "      <td>0.693187</td>\n",
       "      <td>0.687591</td>\n",
       "      <td>0.991927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>1.998217</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007540</td>\n",
       "      <td>1.264472</td>\n",
       "      <td>1.242954</td>\n",
       "      <td>1.591095</td>\n",
       "      <td>1.255009</td>\n",
       "      <td>1.233652</td>\n",
       "      <td>1.579187</td>\n",
       "      <td>0.982983</td>\n",
       "      <td>1.258308</td>\n",
       "      <td>1.280091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>1.990851</td>\n",
       "      <td>...</td>\n",
       "      <td>11.193244</td>\n",
       "      <td>23.669563</td>\n",
       "      <td>39.870035</td>\n",
       "      <td>59.300528</td>\n",
       "      <td>2.114629</td>\n",
       "      <td>3.561973</td>\n",
       "      <td>5.297886</td>\n",
       "      <td>1.684443</td>\n",
       "      <td>2.505349</td>\n",
       "      <td>1.487346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>1.819875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.579402</td>\n",
       "      <td>2.027361</td>\n",
       "      <td>1.794075</td>\n",
       "      <td>1.105754</td>\n",
       "      <td>1.283626</td>\n",
       "      <td>1.135921</td>\n",
       "      <td>0.700110</td>\n",
       "      <td>0.884931</td>\n",
       "      <td>0.545416</td>\n",
       "      <td>0.616337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>2.214854</td>\n",
       "      <td>...</td>\n",
       "      <td>6.408547</td>\n",
       "      <td>9.307312</td>\n",
       "      <td>9.380549</td>\n",
       "      <td>5.696489</td>\n",
       "      <td>1.452328</td>\n",
       "      <td>1.463756</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.007869</td>\n",
       "      <td>0.612044</td>\n",
       "      <td>0.607266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      flux_min    flux_max   flux_mean  flux_median    flux_std  flux_skew  \\\n",
       "0 -1100.440063  660.626343 -123.096998   -89.477524  394.109851  -0.349540   \n",
       "1   -14.735178   14.770886   -1.423351    -0.873033    6.471144   0.014989   \n",
       "2   -19.159811   47.310059    2.267434     0.409172    8.022239   3.177854   \n",
       "3   -15.494463  220.795212    8.909206     1.035895   27.558208   4.979826   \n",
       "4   -16.543753  143.600189    7.145702     1.141288   20.051722   4.406298   \n",
       "\n",
       "   flux_err_min  flux_err_max  flux_err_mean  flux_err_median  \\\n",
       "0      2.130510     12.845472       4.482743         3.835268   \n",
       "1      0.639458      9.115748       2.359620         1.998217   \n",
       "2      0.695106     11.281384       2.471061         1.990851   \n",
       "3      0.567170     55.892746       2.555576         1.819875   \n",
       "4      0.695277     11.383690       2.753004         2.214854   \n",
       "\n",
       "         ...          abs_energy_2_by_1  abs_energy_3_by_1  abs_energy_4_by_1  \\\n",
       "0        ...                   0.439020           0.248155           0.172018   \n",
       "1        ...                   1.007540           1.264472           1.242954   \n",
       "2        ...                  11.193244          23.669563          39.870035   \n",
       "3        ...                   1.579402           2.027361           1.794075   \n",
       "4        ...                   6.408547           9.307312           9.380549   \n",
       "\n",
       "   abs_energy_5_by_1  abs_energy_3_by_2  abs_energy_4_by_2  abs_energy_5_by_2  \\\n",
       "0           0.170629           0.565249           0.391823           0.388660   \n",
       "1           1.591095           1.255009           1.233652           1.579187   \n",
       "2          59.300528           2.114629           3.561973           5.297886   \n",
       "3           1.105754           1.283626           1.135921           0.700110   \n",
       "4           5.696489           1.452328           1.463756           0.888889   \n",
       "\n",
       "   abs_energy_4_by_3  abs_energy_5_by_3  abs_energy_5_by_4  \n",
       "0           0.693187           0.687591           0.991927  \n",
       "1           0.982983           1.258308           1.280091  \n",
       "2           1.684443           2.505349           1.487346  \n",
       "3           0.884931           0.545416           0.616337  \n",
       "4           1.007869           0.612044           0.607266  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(\n",
    "        x='gain',\n",
    "        y='feature',\n",
    "        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances_10.png')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def save_cm(y, oof_preds, path):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i, val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "\n",
    "    y_map = np.zeros((y.shape[0], ))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds, axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    sample_sub = pd.read_csv(path)\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title='Confusion matrix')\n",
    "    plt.savefig(\"confusion_matrix_10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.70288\ttraining's wloss: 0.6945\tvalid_1's multi_logloss: 1.0489\tvalid_1's wloss: 0.853112\n",
      "[200]\ttraining's multi_logloss: 0.44443\ttraining's wloss: 0.434243\tvalid_1's multi_logloss: 0.811885\tvalid_1's wloss: 0.644413\n",
      "[300]\ttraining's multi_logloss: 0.342697\ttraining's wloss: 0.331887\tvalid_1's multi_logloss: 0.729089\tvalid_1's wloss: 0.592465\n",
      "[400]\ttraining's multi_logloss: 0.284303\ttraining's wloss: 0.273542\tvalid_1's multi_logloss: 0.687724\tvalid_1's wloss: 0.578917\n",
      "[500]\ttraining's multi_logloss: 0.242597\ttraining's wloss: 0.232315\tvalid_1's multi_logloss: 0.660091\tvalid_1's wloss: 0.578167\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's multi_logloss: 0.242597\ttraining's wloss: 0.232315\tvalid_1's multi_logloss: 0.660091\tvalid_1's wloss: 0.578167\n",
      "0.5781669789363778\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.708466\ttraining's wloss: 0.700295\tvalid_1's multi_logloss: 1.027\tvalid_1's wloss: 0.83659\n",
      "[200]\ttraining's multi_logloss: 0.447491\ttraining's wloss: 0.437255\tvalid_1's multi_logloss: 0.794929\tvalid_1's wloss: 0.630175\n",
      "[300]\ttraining's multi_logloss: 0.345761\ttraining's wloss: 0.334615\tvalid_1's multi_logloss: 0.711766\tvalid_1's wloss: 0.574843\n",
      "[400]\ttraining's multi_logloss: 0.286676\ttraining's wloss: 0.275431\tvalid_1's multi_logloss: 0.670693\tvalid_1's wloss: 0.56129\n",
      "[500]\ttraining's multi_logloss: 0.244635\ttraining's wloss: 0.233719\tvalid_1's multi_logloss: 0.642283\tvalid_1's wloss: 0.558628\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's multi_logloss: 0.244635\ttraining's wloss: 0.233719\tvalid_1's multi_logloss: 0.642283\tvalid_1's wloss: 0.558628\n",
      "0.5586284891615009\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.698614\ttraining's wloss: 0.690328\tvalid_1's multi_logloss: 1.04426\tvalid_1's wloss: 0.949574\n",
      "[200]\ttraining's multi_logloss: 0.440054\ttraining's wloss: 0.429209\tvalid_1's multi_logloss: 0.81282\tvalid_1's wloss: 0.752621\n",
      "[300]\ttraining's multi_logloss: 0.33934\ttraining's wloss: 0.327762\tvalid_1's multi_logloss: 0.733829\tvalid_1's wloss: 0.705836\n",
      "[400]\ttraining's multi_logloss: 0.281805\ttraining's wloss: 0.270559\tvalid_1's multi_logloss: 0.69388\tvalid_1's wloss: 0.695287\n",
      "[500]\ttraining's multi_logloss: 0.241294\ttraining's wloss: 0.230641\tvalid_1's multi_logloss: 0.66835\tvalid_1's wloss: 0.696058\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's multi_logloss: 0.241294\ttraining's wloss: 0.230641\tvalid_1's multi_logloss: 0.66835\tvalid_1's wloss: 0.696058\n",
      "0.6960584902421014\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.705674\ttraining's wloss: 0.700675\tvalid_1's multi_logloss: 1.02555\tvalid_1's wloss: 0.823995\n",
      "[200]\ttraining's multi_logloss: 0.445461\ttraining's wloss: 0.437135\tvalid_1's multi_logloss: 0.801853\tvalid_1's wloss: 0.610046\n",
      "[300]\ttraining's multi_logloss: 0.343036\ttraining's wloss: 0.333491\tvalid_1's multi_logloss: 0.72205\tvalid_1's wloss: 0.559844\n",
      "[400]\ttraining's multi_logloss: 0.284637\ttraining's wloss: 0.274865\tvalid_1's multi_logloss: 0.680828\tvalid_1's wloss: 0.544895\n",
      "Early stopping, best iteration is:\n",
      "[428]\ttraining's multi_logloss: 0.271781\ttraining's wloss: 0.261941\tvalid_1's multi_logloss: 0.672199\tvalid_1's wloss: 0.541769\n",
      "0.5417693133673241\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.703737\ttraining's wloss: 0.697052\tvalid_1's multi_logloss: 1.04931\tvalid_1's wloss: 0.863447\n",
      "[200]\ttraining's multi_logloss: 0.442957\ttraining's wloss: 0.434047\tvalid_1's multi_logloss: 0.823236\tvalid_1's wloss: 0.66269\n",
      "[300]\ttraining's multi_logloss: 0.340847\ttraining's wloss: 0.330647\tvalid_1's multi_logloss: 0.742331\tvalid_1's wloss: 0.614154\n",
      "[400]\ttraining's multi_logloss: 0.282547\ttraining's wloss: 0.272163\tvalid_1's multi_logloss: 0.697995\tvalid_1's wloss: 0.602244\n",
      "[500]\ttraining's multi_logloss: 0.241902\ttraining's wloss: 0.231664\tvalid_1's multi_logloss: 0.669444\tvalid_1's wloss: 0.600249\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's multi_logloss: 0.241902\ttraining's wloss: 0.231664\tvalid_1's multi_logloss: 0.669444\tvalid_1's wloss: 0.600249\n",
      "0.6002487134706186\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.702414\ttraining's wloss: 0.69462\tvalid_1's multi_logloss: 1.03411\tvalid_1's wloss: 0.883732\n",
      "[200]\ttraining's multi_logloss: 0.443474\ttraining's wloss: 0.433505\tvalid_1's multi_logloss: 0.803007\tvalid_1's wloss: 0.669667\n",
      "[300]\ttraining's multi_logloss: 0.3418\ttraining's wloss: 0.331201\tvalid_1's multi_logloss: 0.724671\tvalid_1's wloss: 0.621968\n",
      "[400]\ttraining's multi_logloss: 0.283434\ttraining's wloss: 0.273028\tvalid_1's multi_logloss: 0.683237\tvalid_1's wloss: 0.61111\n",
      "Early stopping, best iteration is:\n",
      "[397]\ttraining's multi_logloss: 0.284812\ttraining's wloss: 0.274381\tvalid_1's multi_logloss: 0.684028\tvalid_1's wloss: 0.610937\n",
      "0.6109367284709969\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.701505\ttraining's wloss: 0.692822\tvalid_1's multi_logloss: 1.05205\tvalid_1's wloss: 0.900561\n",
      "[200]\ttraining's multi_logloss: 0.442547\ttraining's wloss: 0.43228\tvalid_1's multi_logloss: 0.823069\tvalid_1's wloss: 0.707194\n",
      "[300]\ttraining's multi_logloss: 0.34148\ttraining's wloss: 0.330712\tvalid_1's multi_logloss: 0.743324\tvalid_1's wloss: 0.666591\n",
      "[400]\ttraining's multi_logloss: 0.283756\ttraining's wloss: 0.272971\tvalid_1's multi_logloss: 0.700904\tvalid_1's wloss: 0.649685\n",
      "[500]\ttraining's multi_logloss: 0.242978\ttraining's wloss: 0.232626\tvalid_1's multi_logloss: 0.670823\tvalid_1's wloss: 0.641706\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's multi_logloss: 0.242978\ttraining's wloss: 0.232626\tvalid_1's multi_logloss: 0.670823\tvalid_1's wloss: 0.641706\n",
      "0.6417063440593709\n",
      "MULTI WEIGHTED LOG LOSS : 0.60328 \n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=7, shuffle=True, random_state=7)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "\n",
    "lgb_params = {\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': -1, \n",
    "    'max_depth': 7, \n",
    "    'n_estimators': 500, \n",
    "    'subsample_freq': 2, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10, \n",
    "    'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75\n",
    "}\n",
    "\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i: np.sum(w) / w[i] for i in w.index}\n",
    "oof_preds = np.zeros((len(full), np.unique(y).shape[0]))\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = full.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = full.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x,\n",
    "        trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights))\n",
    "    oof_preds[val_, :] = clf.predict_proba(\n",
    "        val_x, num_iteration=clf.best_iteration_)\n",
    "    print(multi_weighted_logloss(val_y, oof_preds[val_, :]))\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = full.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    clfs.append(clf)\n",
    "\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(\n",
    "    y_true=y, y_preds=oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    }
   ],
   "source": [
    "save_importances(importances_=importances)\n",
    "save_cm(y, oof_preds, data_dir + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(full_test, clfs_, features):\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    return preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 22.180764198303223 minutes\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "start = time.time()\n",
    "full_test5 = pd.read_csv(\"full_test5.csv\")\n",
    "base_name = \"flux_norm_fft_\"\n",
    "for i in range(5):\n",
    "    for j in range(1, 6):\n",
    "        if j > i:\n",
    "            full_test5[f\"{base_name}{j}_by_{i}\"] = full_test5[f\"{j}__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\"] / \\\n",
    "                full_test5[f\"{i}__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\"]\n",
    "            \n",
    "base_name = \"abs_energy_\"\n",
    "for i in range(5):\n",
    "    for j in range(1, 6):\n",
    "        if j > i:\n",
    "            full_test5[f\"{base_name}{j}_by_{i}\"] = full_test5[f\"{j}__abs_energy_norm\"] / full_test5[f\"{i}__abs_energy_norm\"]\n",
    "full_test5.to_csv(\"full_test6.csv\", index=False)\n",
    "preds_df = predict_test(\n",
    "    full_test=full_test5,\n",
    "    clfs_=clfs,\n",
    "    features=full.columns\n",
    ")\n",
    "preds_df.to_csv(\"predictions10.csv\", index=False)\n",
    "del preds_df, full_test5\n",
    "print(f'done in {(time.time() - start) / 60} minutes', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions10.csv (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "!zip predictions10.csv.zip predictions10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
