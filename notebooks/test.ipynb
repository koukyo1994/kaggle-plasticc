{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from multiprocessing import Pool\n",
    "tqdm.pandas(desc=\"apply progress\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ整形の関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq': ['sum', 'skew'],\n",
    "    'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "}\n",
    "\n",
    "# agg diff features\n",
    "diff_aggs = {\n",
    "    'flux_diff': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "}\n",
    "\n",
    "# tsfresh features\n",
    "fcp = {\n",
    "    'flux': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "        'length': None,\n",
    "        'mean': None,\n",
    "        'maximum': None,\n",
    "        'minimum': None,\n",
    "        # additional\n",
    "        'absolute_sum_of_changes': None,\n",
    "        'autocorrelation': [{'lag': 3}],\n",
    "        'binned_entropy': [{'max_bins': 10}],\n",
    "        'cid_ce': [{'normalize': True}],\n",
    "        'count_above_mean': None,\n",
    "        'first_location_of_maximum': None,\n",
    "        'first_location_of_minimum': None,\n",
    "        'last_location_of_maximum': None,\n",
    "        'last_location_of_minimum': None,\n",
    "        'mean_second_derivative_central': None,\n",
    "        'median': None,\n",
    "        'ratio_beyond_r_sigma': [{'r': 2}],\n",
    "        'sample_entropy': None,\n",
    "        'time_reversal_asymmetry_statistic': [{'lag': 4}]\n",
    "    },\n",
    "\n",
    "    'flux_by_flux_ratio_sq': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        # additional\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "        'length': None,\n",
    "        'mean': None,\n",
    "        'maximum': None,\n",
    "        'minimum': None,\n",
    "        'abs_energy': None,\n",
    "        'absolute_sum_of_changes': None,\n",
    "        'autocorrelation': [{'lag': 3}],\n",
    "        'binned_entropy': [{'max_bins': 10}],\n",
    "        'cid_ce': [{'normalize': True}],\n",
    "        'count_above_mean': None,\n",
    "        'count_below_mean': None,\n",
    "        'first_location_of_maximum': None,\n",
    "        'first_location_of_minimum': None,\n",
    "        'kurtosis': None,\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        'mean_second_derivative_central': None,\n",
    "        'median': None,\n",
    "        'sample_entropy': None,\n",
    "        'standard_deviation': None,\n",
    "        'time_reversal_asymmetry_statistic': [{'lag': 3}]\n",
    "    },\n",
    "\n",
    "    'flux_passband': {\n",
    "        'fft_coefficient': [\n",
    "                {'coeff': 0, 'attr': 'abs'},\n",
    "                {'coeff': 1, 'attr': 'abs'}\n",
    "            ],\n",
    "        'kurtosis': None,\n",
    "        'skewness': None,\n",
    "        'maximum': None,\n",
    "        'mean': None,\n",
    "        'minimum': None,\n",
    "        # additional\n",
    "        'abs_energy': None,\n",
    "        'autocorrelation': [{'lag': 3}],\n",
    "        'binned_entropy': [{'max_bins': 10}],\n",
    "        'cid_ce': [{'normalize': True}],\n",
    "        'mean_second_derivative_central': None,\n",
    "        'median': None,\n",
    "        'sample_entropy': None,\n",
    "        'standard_deviation': None,\n",
    "        'sum_values': None,\n",
    "        'time_reversal_asymmetry_statistic': [{'lag': 4}]\n",
    "    },\n",
    "\n",
    "    'fcp2':  {\n",
    "        \"fft_coefficient\": [{\n",
    "            \"coeff\": 0,\n",
    "            \"attr\": \"abs\"\n",
    "        }, {\n",
    "            \"coeff\": 1,\n",
    "            \"attr\": \"abs\"\n",
    "        }],\n",
    "        \"abs_energy\": None,\n",
    "        \"sample_entropy\": None\n",
    "    },\n",
    "\n",
    "    'mjd': {\n",
    "        'maximum': None,\n",
    "        'minimum': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees) from\n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Implementing Haversine Formula:\n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(\n",
    "        np.power(np.sin(np.divide(dlat, 2)), 2),\n",
    "        np.multiply(\n",
    "            np.cos(lat1),\n",
    "            np.multiply(np.cos(lat2), np.power(np.sin(np.divide(dlon, 2)),\n",
    "                                               2))))\n",
    "\n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine,\n",
    "        'latlon1': np.subtract(\n",
    "            np.multiply(lon1, lat1), np.multiply(lon2, lat2)),\n",
    "    }\n",
    "\n",
    "\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame(\n",
    "        {\n",
    "            'flux_ratio_sq': flux_ratio_sq,\n",
    "            'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,\n",
    "        },\n",
    "        index=df.index)\n",
    "\n",
    "    return pd.concat([df, df_flux], axis=1)\n",
    "\n",
    "\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df[\n",
    "        'flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "\n",
    "    df_flux_agg = pd.DataFrame(\n",
    "        {\n",
    "            'flux_w_mean': flux_w_mean,\n",
    "            'flux_diff1': flux_diff,\n",
    "            'flux_diff2': flux_diff / df['flux_mean'].values,\n",
    "            'flux_diff3': flux_diff / flux_w_mean,\n",
    "        },\n",
    "        index=df.index)\n",
    "\n",
    "    return pd.concat([df, df_flux_agg], axis=1)\n",
    "\n",
    "\n",
    "def make_diff_feature(df):\n",
    "    tmp = df.groupby(['object_id', 'passband',\n",
    "                      'mjd'])['flux'].sum().reset_index()\n",
    "\n",
    "    tmp['flux_diff'] = tmp['flux'] - tmp['flux'].shift(1)\n",
    "    multi_id_list = (tmp['object_id'].astype(str) + '-' +\n",
    "                     tmp['passband'].astype(str)).values\n",
    "\n",
    "    drop_index = []\n",
    "    prev_val = 'hoge'\n",
    "    for index, val in enumerate(multi_id_list):\n",
    "        if val != prev_val:\n",
    "            drop_index.append(index)\n",
    "        prev_val = val\n",
    "\n",
    "    use_index = list(set(tmp.index) - set(drop_index))\n",
    "    tmp = tmp.iloc[use_index, :]\n",
    "    diff_df = tmp.drop('flux', axis=1)\n",
    "\n",
    "    return diff_df\n",
    "\n",
    "\n",
    "def featurize(df, df_meta, aggs, fcp, n_jobs=72):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "\n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [\n",
    "        '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]\n",
    "    ]\n",
    "    agg_df = process_flux_agg(agg_df)  # new feature to play with tsfresh\n",
    "    per_passband_aggs = {\n",
    "        \"flux\": [\"min\", \"max\", \"mean\", \"std\"],\n",
    "        \"flux_ratio_sq\": [\"sum\", \"skew\"],\n",
    "        \"flux_by_flux_ratio_sq\": [\"sum\", \"skew\"]\n",
    "    }\n",
    "    per_pass_agg_df = df.groupby([\"object_id\",\n",
    "                                  \"passband\"]).agg(per_passband_aggs)\n",
    "    per_pass_agg_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in per_pass_agg_df.columns])\n",
    "    per_pass_agg_df[\"flux_diff\"] = per_pass_agg_df[\n",
    "        \"flux_max\"] - per_pass_agg_df[\"flux_min\"]\n",
    "    per_pass_agg_df[\"flux_diff2\"] = (\n",
    "        per_pass_agg_df[\"flux_max\"] -\n",
    "        per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_mean\"]\n",
    "    per_pass_agg_df[\"flux_w_mean\"] = per_pass_agg_df[\n",
    "        \"flux_by_flux_ratio_sq_sum\"] / per_pass_agg_df[\"flux_ratio_sq_sum\"]\n",
    "    per_pass_agg_df[\"flux_dif3\"] = (\n",
    "        per_pass_agg_df[\"flux_max\"] -\n",
    "        per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_w_mean\"]\n",
    "    per_pass_agg_df = per_pass_agg_df.unstack()\n",
    "    per_pass_agg_df.columns = pd.Index(\n",
    "        [str(e[1]) + \"__\" + e[0] for e in per_pass_agg_df.columns])\n",
    "\n",
    "    basic_columns = [\n",
    "        f\"{i}__{j}\" for i in range(6) for j in [\n",
    "            \"flux_min\", \"flux_max\", \"flux_mean\", \"flux_std\",\n",
    "            \"flux_ratio_sq_sum\", \"flux_ratio_sq_skew\", \"flux_w_mean\",\n",
    "            \"flux_diff2\"\n",
    "        ]\n",
    "    ]\n",
    "    per_pass_agg_df.drop(basic_columns, axis=1, inplace=True)\n",
    "\n",
    "    agg_df = pd.merge(agg_df, per_pass_agg_df, how=\"left\", on=\"object_id\")\n",
    "\n",
    "    agg_flux_diff = agg_df.reset_index()[[\"object_id\", \"flux_diff1\"]]\n",
    "    df2 = pd.merge(df, agg_flux_diff, how=\"left\", on=\"object_id\")\n",
    "    df2[\"flux_norm\"] = df2.flux / df2.flux_diff1\n",
    "    del df2[\"flux\"]\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp['flux_passband'],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp['flux'],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts2 = extract_features(\n",
    "        df2,\n",
    "        column_id=\"object_id\",\n",
    "        column_sort=\"mjd\",\n",
    "        column_kind=\"passband\",\n",
    "        column_value=\"flux_norm\",\n",
    "        default_fc_parameters=fcp[\"fcp2\"],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_value='flux_by_flux_ratio_sq',\n",
    "        default_fc_parameters=fcp['flux_by_flux_ratio_sq'],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters=fcp['mjd'],\n",
    "        n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd[\n",
    "        'mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts2.columns = pd.Index([e + \"_norm\" for e in agg_df_ts2.columns])\n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True)\n",
    "    agg_df.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True)\n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)\n",
    "    agg_df_ts = pd.concat([\n",
    "        agg_df, agg_df_ts2, agg_df_ts_flux_passband, agg_df_ts_flux,\n",
    "        agg_df_ts_flux_by_flux_ratio_sq, agg_df_mjd\n",
    "    ],\n",
    "                          axis=1).reset_index()\n",
    "\n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result\n",
    "\n",
    "\n",
    "def diff_featurize(diff_df, df_meta, diff_aggs, fcp, n_jobs=72):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "\n",
    "    # df = train.copy()\n",
    "\n",
    "    # diff_df = process_flux(diff_df)\n",
    "\n",
    "    diff_agg_df = diff_df.groupby('object_id').agg(diff_aggs)\n",
    "    diff_agg_df.columns = [\n",
    "        '{}_{}'.format(k, agg) for k in diff_aggs.keys()\n",
    "        for agg in diff_aggs[k]\n",
    "    ]\n",
    "    # diff_agg_df = process_flux_agg(diff_agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    diff_agg_df_ts_flux_passband = extract_features(\n",
    "        diff_df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux_diff',\n",
    "        default_fc_parameters=fcp['flux_passband'],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    diff_agg_df_ts_flux = extract_features(\n",
    "        diff_df,\n",
    "        column_id='object_id',\n",
    "        column_value='flux_diff',\n",
    "        default_fc_parameters=fcp['flux'],\n",
    "        n_jobs=n_jobs)\n",
    "\n",
    "    diff_agg_df_ts_flux_passband.index.rename('object_id', inplace=True)\n",
    "    diff_agg_df_ts_flux_passband.columns = [\n",
    "        column + '_diff' for column in diff_agg_df_ts_flux_passband.columns\n",
    "    ]\n",
    "    diff_agg_df_ts_flux.index.rename('object_id', inplace=True)\n",
    "    # agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True)\n",
    "    # agg_df_mjd.index.rename('object_id', inplace=True)\n",
    "    diff_agg_df_ts = pd.concat(\n",
    "        [\n",
    "            diff_agg_df,\n",
    "            diff_agg_df_ts_flux_passband,\n",
    "            diff_agg_df_ts_flux,\n",
    "            # agg_df_ts_flux_by_flux_ratio_sq,\n",
    "            # agg_df_mjd\n",
    "        ],\n",
    "        axis=1).reset_index()\n",
    "\n",
    "    # result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    result = diff_agg_df_ts\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_meta(meta_df):\n",
    "    meta_dict = dict()\n",
    "\n",
    "    # id trick\n",
    "    for i in [22, 27]:\n",
    "        meta_dict['object_id_div_{}'.format(i)] = np.mod(\n",
    "            meta_df['object_id'].values, i)\n",
    "\n",
    "    # distance\n",
    "    meta_dict.update(\n",
    "        haversine_plus(meta_df['ra'].values, meta_df['decl'].values,\n",
    "                       meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "        meta_df['hostgal_photoz'].values,\n",
    "        np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "\n",
    "    meta_df = pd.concat(\n",
    "        [meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df\n",
    "\n",
    "\n",
    "def add_rank_bottom_and_top(df, feature_name):\n",
    "    objid = [\"object_id\"]\n",
    "    columns = [f\"{i}{feature_name}\" for i in range(6)]\n",
    "    partial_df = df[objid + columns]\n",
    "    partial_values = partial_df.melt(\n",
    "        id_vars=objid, value_vars=columns).sort_values([\"object_id\", \"value\"])\n",
    "\n",
    "    top_and_bottom = partial_values.groupby(\"object_id\").agg({\n",
    "        \"variable\": [\"first\", \"last\"]\n",
    "    })\n",
    "    top_and_bottom.columns = [\"top\" + feature_name, \"bottom\" + feature_name]\n",
    "    for i, n in zip([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"], columns):\n",
    "        top_and_bottom = top_and_bottom.replace(n, i)\n",
    "    top_and_bottom = top_and_bottom.astype(int)\n",
    "    return top_and_bottom\n",
    "\n",
    "\n",
    "def add_by_features(df, feature_name, new_feat_name):\n",
    "    for i in range(5):\n",
    "        for j in range(1, 6):\n",
    "            if j > i:\n",
    "                df[f\"{new_feat_name}{j}_by_{i}\"] = df[\n",
    "                    f\"{j}{feature_name}\"] / df[f\"{i}{feature_name}\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測関連の関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "def multi_weighted_logloss_novae(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [15, 42, 52, 62, 67, 90]\n",
    "    class_weight = {\n",
    "        15: 2,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        62: 1,\n",
    "        67: 1,\n",
    "        90: 1,\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss_novae(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [15, 42, 52, 62, 67, 90]\n",
    "    class_weight = {\n",
    "        15: 2,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        62: 1,\n",
    "        67: 1,\n",
    "        90: 1,\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def multi_weighted_logloss_nonnovae(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 16, 53, 64, 65, 88, 92, 95]\n",
    "    class_weight = {6: 1, 16: 1, 53: 1, 64: 2, 65: 1, 88: 1, 92: 1, 95: 1}\n",
    "    \n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss_nonnovae(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 16, 53, 64, 65, 88, 92, 95]\n",
    "    class_weight = {6: 1, 16: 1, 53: 1, 64: 2, 65: 1, 88: 1, 92: 1, 95: 1}\n",
    "    \n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_superの特徴量整形用関数\n",
    "def train_data_super(train, meta_train):\n",
    "    y = meta_train.target\n",
    "    meta_train = process_meta(meta_train)\n",
    "    \n",
    "    # 特徴量作成\n",
    "    full_train = featurize(train, meta_train, aggs, fcp, n_jobs=4)\n",
    "    # print('full train shape :', full_train.shape)\n",
    "\n",
    "    # 差分特徴量（ver0で作成）\n",
    "    diff_train = make_diff_feature(train)\n",
    "    diff_full_train = diff_featurize(diff_train, meta_train, diff_aggs, fcp, n_jobs=4)\n",
    "\n",
    "    # 特徴量のマージ\n",
    "    full_train = pd.merge(full_train, diff_full_train, on='object_id')\n",
    "\n",
    "    # ランクの特徴量を追加\n",
    "    full_train = add_by_features(\n",
    "        full_train, \"__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\",\n",
    "        \"flux_norm_fft_\")\n",
    "    full_train = add_by_features(full_train, \"__abs_energy_norm\", \"abs_energy_\")\n",
    "    full_train = add_by_features(full_train, \"__flux_diff\", \"flux_diff_\")\n",
    "    abs_energy = add_rank_bottom_and_top(full_train, \"__abs_energy_norm\")\n",
    "    flux_diff = add_rank_bottom_and_top(full_train, \"__flux_diff\")\n",
    "    flux_dif3 = add_rank_bottom_and_top(full_train, \"__flux_dif3\")\n",
    "\n",
    "    full_train = pd.merge(full_train, abs_energy, how=\"left\", on=\"object_id\")\n",
    "    full_train = pd.merge(full_train, flux_diff, how=\"left\", on=\"object_id\")\n",
    "    full_train = pd.merge(full_train, flux_dif3, how=\"left\", on=\"object_id\")\n",
    "    \n",
    "    if 'target' in full_train:\n",
    "        # y = full_train['target']\n",
    "        del full_train['target']\n",
    "\n",
    "    classes = sorted(y.unique())\n",
    "    \n",
    "    class_weights = {c: 1 for c in classes}\n",
    "    class_weights.update({c: 2 for c in [64, 15]})\n",
    "    \n",
    "    oof_df = full_train[[\"object_id\"]]\n",
    "    \n",
    "    # モデル前整形\n",
    "    if 'object_id' in full_train:\n",
    "        del full_train['object_id']\n",
    "        # del full_train['distmod']\n",
    "        del full_train['hostgal_specz']\n",
    "        del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "        del full_train['ddf']\n",
    "    \n",
    "    full_train = full_train.replace(np.inf, np.nan)\n",
    "    full_train = full_train.replace(-np.inf, np.nan)\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    full_train = pd.DataFrame(scl.fit_transform(full_train), index=full_train.index, columns=full_train.columns)\n",
    "    full_train.fillna(0, inplace=True)\n",
    "    \n",
    "    use_features = pd.read_csv('./use_features_5-0.csv')['use_feature'].tolist()\n",
    "    full_train = full_train[use_features]\n",
    "    \n",
    "    return full_train, y, classes, class_weights, oof_df, scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(\n",
    "        x='gain',\n",
    "        y='feature',\n",
    "        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances_novae.png')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def save_cm(y, oof_preds):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i, val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "\n",
    "    y_map = np.zeros((y.shape[0], ))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds, axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    class_names = [\"class_15\", \"class_42\", \"class_52\", \"class_62\", \"class_67\", \"class_90\"]\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title='Confusion matrix')\n",
    "    plt.savefig(\"confusion_matrix_novae.png\")\n",
    "    \n",
    "def plot_importances(clfs, features):\n",
    "    importances = []\n",
    "    for clf in clfs:\n",
    "        importances.append(clf.feature_importances_)\n",
    "    importances = pd.Series(np.array(importances).mean(axis=0), index=features)\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(14,14))\n",
    "    importances.iloc[:40].plot(kind='barh', ax=ax)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoteAdataset(Xig_train, yig_train, Xig_test, yig_test):     \n",
    "    sm=SMOTE(random_state=2)\n",
    "    Xig_train_res, yig_train_res = sm.fit_sample(Xig_train, yig_train.ravel())\n",
    "    return Xig_train_res, pd.Series(yig_train_res), Xig_test, pd.Series(yig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train, \n",
    "                                   y, \n",
    "                                   classes, \n",
    "                                   class_weights,\n",
    "                                   lgb_metric,\n",
    "                                   metric,\n",
    "                                   nr_fold=12, \n",
    "                                   random_state=1, \n",
    "                                   save_plot=True, \n",
    "                                   show_log=True,):\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "   # print(weights)\n",
    "   # weights=class_weights\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "        \n",
    "                \n",
    "        trn_xa, trn_y, val_xa, val_y=smoteAdataset(trn_x.values, trn_y.values, val_x.values, val_y.values)\n",
    "        trn_x=pd.DataFrame(data=trn_xa, columns=trn_x.columns)\n",
    "    \n",
    "        val_x=pd.DataFrame(data=val_xa, columns=val_x.columns)\n",
    "        \n",
    "        categorical_feature = []\n",
    "        categorical_feature = list(set(categorical_feature) & set(full_train.columns))\n",
    "        \n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_metric,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights),\n",
    "            categorical_feature=categorical_feature\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        \n",
    "        if show_log:\n",
    "            print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "                  metric(val_y, oof_preds[val_, :])))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = metric(y_true=y, y_preds=oof_preds)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    \n",
    "    if save_plot:\n",
    "        df_importances = save_importances(importances_=importances)\n",
    "    \n",
    "    return clfs, score, oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ読み込んだり諸々"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/home/hidehisa/.kaggle/competitions/plasticc\"\n",
    "data_dir = \"/Users/hidehisa/.kaggle/competitions/plasticc\"\n",
    "# train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "train = pd.read_csv(data_dir + \"/training_set.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova = [15, 42, 52, 62, 67, 90]\n",
    "nonnova = [6, 16, 53, 64, 65, 88, 92, 95]\n",
    "novaes = meta.query(\"target == @nova\")\n",
    "nonnovaes = meta.query(\"target == @nonnova\")\n",
    "object_ids_novae = novaes['object_id'].tolist()\n",
    "object_ids_nonnovae = nonnovaes['object_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:48<00:00,  1.11s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [03:13<00:00,  7.23s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:35<00:00,  1.17it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [04:51<00:00,  6.80s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 19.06it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:34<00:00,  1.05it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:08<00:00,  5.58s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./use_features_5-0.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4a647e4f650d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c30f78aca8d5>\u001b[0m in \u001b[0;36mtrain_data_super\u001b[0;34m(train, meta_train)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mfull_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0muse_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./use_features_5-0.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mfull_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./use_features_5-0.csv' does not exist"
     ]
    }
   ],
   "source": [
    "full, y, classes, class_weight, oof_df, scl = train_data_super(train, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train, full_test, y_train, y_test = train_test_split(full, y, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_novae = oof_df[oof_df['object_id'].isin(object_ids_novae)].index\n",
    "index_nonnovae = oof_df[oof_df['object_id'].isin(object_ids_nonnovae)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二値分類用のカラムを追加\n",
    "train_bin = train.copy()\n",
    "train_bin[\"novae\"] = 0\n",
    "ind = train_bin.query(\"object_id in @novaes.object_id\").index\n",
    "train_bin.loc[ind, \"novae\"] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
