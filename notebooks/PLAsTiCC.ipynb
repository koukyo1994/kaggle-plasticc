{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTmvWpds4lB5"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "qWYyvfOg5R_d",
    "outputId": "f08af9d9-0ff9-4163-e333-c96f12c0a9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tsfresh in /home/hidehisa/anaconda3/lib/python3.6/site-packages (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.10.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (4.28.1)\n",
      "Requirement already satisfied: requests>=2.9.1 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (2.18.4)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.4.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.19.1)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (1.14.5)\n",
      "Requirement already satisfied: future>=0.16.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.16.0)\n",
      "Requirement already satisfied: distributed>=1.18.3 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (1.19.1)\n",
      "Requirement already satisfied: statsmodels>=0.8.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (1.11.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.19.1)\n",
      "Requirement already satisfied: dask>=0.15.2 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from tsfresh) (0.15.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (2018.4.16)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from pandas>=0.20.3->tsfresh) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from pandas>=0.20.3->tsfresh) (2017.2)\n",
      "Requirement already satisfied: tornado>=4.5.1 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (4.5.2)\n",
      "Requirement already satisfied: toolz>=0.7.4 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.8.2)\n",
      "Requirement already satisfied: msgpack-python in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.4.8)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.4.0)\n",
      "Requirement already satisfied: click>=6.6 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (6.7)\n",
      "Requirement already satisfied: tblib in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (1.3.2)\n",
      "Requirement already satisfied: psutil in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (5.4.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.1.3)\n",
      "Requirement already satisfied: sortedcontainers in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (1.5.7)\n",
      "Requirement already satisfied: heapdict in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from zict>=0.1.3->distributed>=1.18.3->tsfresh) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "AznDz3PNI3Cn",
    "outputId": "c89a95a6-37b9-47e6-834a-e2ab75155ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /home/hidehisa/anaconda3/lib/python3.6/site-packages (0.11)\n",
      "Requirement already satisfied: ipython>=5.0.0; python_version >= \"3.3\" in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipdb) (6.1.0)\n",
      "Requirement already satisfied: setuptools in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipdb) (36.5.0.post20170921)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.10.2)\n",
      "Requirement already satisfied: decorator in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.1.2)\n",
      "Requirement already satisfied: pickleshare in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.3.2)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (1.0.15)\n",
      "Requirement already satisfied: pygments in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (2.2.0)\n",
      "Requirement already satisfied: pexpect in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.2.1)\n",
      "Requirement already satisfied: ipython_genutils in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: six in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (1.11.0)\n",
      "Requirement already satisfied: wcwidth in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "-hDnjEBJ6BLz",
    "outputId": "40f94c96-3420-4e19-fc6d-c8c9e87546d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/hidehisa/anaconda3/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: scipy in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from lightgbm) (0.19.1)\n",
      "Requirement already satisfied: scikit-learn in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from lightgbm) (0.19.1)\n",
      "Requirement already satisfied: numpy in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from lightgbm) (1.14.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "5mWaECEjHlke",
    "outputId": "c795993e-3f6e-4efa-ee35-6b7444107798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /home/hidehisa/anaconda3/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from pandas) (2017.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from pandas) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from pandas) (1.14.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/hidehisa/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tqdm in /home/hidehisa/anaconda3/lib/python3.6/site-packages (4.28.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cK8rb4cy4K5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hidehisa/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "tqdm.pandas(desc=\"apply progress\")\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EUmUBq3m-3dJ",
    "outputId": "1974b0c7-4bbe-4065-dd38-699d4f1ba32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k1ERNuLo_Dm6",
    "outputId": "22950a62-2483-4254-ecdc-c823c41ab6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !cp /content/drive/My\\ Drive/plasticc/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyxxtBk4_4th"
   },
   "outputs": [],
   "source": [
    "# !unzip -q sample_submission.csv.zip\n",
    "# !unzip -q test_set.csv.zip\n",
    "# !unzip -q test_set_metadata.csv.zip\n",
    "# !unzip -q training_set.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVrYEq8aAxEz"
   },
   "source": [
    "## Open training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fidpdfCcA1VY"
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/hidehisa/.kaggle/competitions/plasticc\"\n",
    "train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8R5UxFwApPH"
   },
   "source": [
    "## Add Cluster to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYY2gDJwgehY"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBwKcp2O_YH3"
   },
   "outputs": [],
   "source": [
    "def elbow(d):\n",
    "    data = d.mjd.values.reshape([-1, 1])\n",
    "    kms = [KMeans(n_clusters=i).fit(data) for i in range(2, 6)]\n",
    "    inertias = [km.inertia_ for km in kms]\n",
    "    diff1 = inertias[0] - inertias[1]\n",
    "    diff2 = inertias[1] - inertias[2]\n",
    "    diff3 = inertias[2] - inertias[3]\n",
    "    if diff1 / diff2 > diff2 / diff3:\n",
    "        return kms[1].predict(data)\n",
    "    else:\n",
    "        return kms[2].predict(data)\n",
    "\n",
    "def add_cluster(df):\n",
    "    new_df = (df.groupby(\"object_id\").progress_apply(lambda x: elbow(x))\n",
    "                .to_frame(\"cluster\")\n",
    "                .apply(lambda x: x.apply(pd.Series).stack())\n",
    "                .reset_index()\n",
    "                .drop(\"level_1\", axis=1)\n",
    "             )\n",
    "    new_df = new_df.astype({\"cluster\": int})\n",
    "    df = pd.concat([df, new_df.drop(\"object_id\", axis=1)], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cluster_multi(d):\n",
    "    n_record = d.shape[0]\n",
    "    default_chunk = n_record // 8\n",
    "    head = 0\n",
    "    df_pool = []\n",
    "    for _ in range(7):\n",
    "        new_df = d.loc[head:head+default_chunk, :]\n",
    "        last_id = new_df.object_id.unique()[-1]\n",
    "        len_last = new_df.query(\"object_id == @last_id\").shape[0]\n",
    "        new_df = new_df.loc[head:head+default_chunk-len_last, :]\n",
    "        df_pool.append(new_df)\n",
    "        head = head + default_chunk - len_last+1\n",
    "    df_pool.append(d.loc[head:, :])\n",
    "    pool = Pool(8)\n",
    "    dfs = pool.map(add_cluster, df_pool)\n",
    "    pool.close()\n",
    "    return pd.concat(dfs)})\n",
    "    df = pd.concat([df, new_df.drop(\"object_id\", axis=1)], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cluster_multi(d):\n",
    "    n_record = d.shape[0]\n",
    "    default_chunk = n_record // 8\n",
    "    head = 0\n",
    "    df_pool = []\n",
    "    for _ in range(7):\n",
    "        new_df = d.loc[head:head+default_chunk, :]\n",
    "        last_id = new_df.object_id.unique()[-1]\n",
    "        len_last = new_df.query(\"object_id == @last_id\").shape[0]\n",
    "        new_df = new_df.loc[head:head+default_chunk-len_last, :]\n",
    "        df_pool.append(new_df)\n",
    "        head = head + default_chunk - len_last+1\n",
    "    df_pool.append(d.loc[head:, :])\n",
    "    pool = Pool(8)\n",
    "    dfs = pool.map(add_cluster, df_pool)\n",
    "    pool.close()\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "j_U0QDopAM5-",
    "outputId": "5ad0974e-21b7-4170-be65-c3c5930e9c00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply progress: 100%|██████████| 7848/7848 [04:42<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 1.47 s, total: 4min 46s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = add_cluster(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVxTBkiqd12A"
   },
   "outputs": [],
   "source": [
    "# train.to_csv(\"train_with_cluster.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5gTAbSt1eCHH",
    "outputId": "f4fa6817-34d8-499c-ae36-9aa4e18eae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: train_with_cluster.csv (deflated 67%)\n"
     ]
    }
   ],
   "source": [
    "# !zip train_with_cluster.csv.zip train_with_cluster.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYzvaxAoej5O"
   },
   "outputs": [],
   "source": [
    "# !cp train_with_cluster.csv.zip /content/drive/My\\ Drive/plasticc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMsGaM0bCBX-"
   },
   "source": [
    "## Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80dNYR2xB1x1"
   },
   "outputs": [],
   "source": [
    "def basic(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "\n",
    "    aggs = {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['mjd_diff'] = agg_df['mjd_max'] - agg_df['mjd_min']\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "\n",
    "    del agg_df['mjd_max'], agg_df['mjd_min']\n",
    "\n",
    "    fcp = {\n",
    "        'fft_coefficient': [{\n",
    "            'coeff': 0,\n",
    "            'attr': 'abs'\n",
    "        }, {\n",
    "            'coeff': 1,\n",
    "            'attr': 'abs'\n",
    "        }],\n",
    "        'kurtosis':\n",
    "        None,\n",
    "        'skewness':\n",
    "        None\n",
    "    }\n",
    "    agg_df_ts = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp,\n",
    "        n_jobs=2)\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters={\n",
    "            'maximum': None,\n",
    "            'minimum': None\n",
    "        },\n",
    "        n_jobs=2)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd[\n",
    "        'mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd, on='id')\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    agg_df_ts.index.rename('object_id', inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts, on='object_id')\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def with_cluster(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "    aggs = {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby(['object_id', \"cluster\"]).agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['mjd_diff'] = agg_df['mjd_max'] - agg_df['mjd_min']\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    del agg_df['mjd_max'], agg_df['mjd_min']\n",
    "    agg_df.drop(\"cluster\", axis=1, inplace=True)\n",
    "    agg_df = agg_df.groupby(\"object_id\").agg([\"min\", \"max\", \"std\", \"skew\"])\n",
    "    agg_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg_df.columns])\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def cluster_mean_diff(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\"]).agg({\n",
    "        \"flux\": [\"mean\", \"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in new_df.columns.tolist()])\n",
    "    new_df[\"normalized_mean\"] = new_df[\"flux_mean\"] / (\n",
    "        new_df[\"flux_max\"] - new_df[\"flux_min\"])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df.groupby(\"object_id\").agg({\"normalized_mean\": \"std\"})\n",
    "\n",
    "\n",
    "def passband_std_difference(df):\n",
    "    std_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": \"std\"\n",
    "    }).reset_index().groupby([\"object_id\",\n",
    "                              \"passband\"])[\"flux\"].mean().reset_index()\n",
    "    std_df_max = std_df.groupby(\"object_id\")[\"flux\"].max()\n",
    "    std_df_min = std_df.groupby(\"object_id\")[\"flux\"].min()\n",
    "    return (std_df_max / std_df_min).reset_index()\n",
    "\n",
    "\n",
    "def linear_slope(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": [\"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df[\"flux_range\"] = new_df[\"flux_max\"] - new_df[\"flux_min\"]\n",
    "    new_df = pd.merge(\n",
    "        df, new_df, how=\"left\", on=[\"object_id\", \"cluster\", \"passband\"])\n",
    "    new_df[\"flux_normalized\"] = new_df[\"flux\"] / new_df[\"flux_range\"]\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    def get_coef_(x):\n",
    "        if x.shape[0] <= 1:\n",
    "            return 0\n",
    "        lr.fit(\n",
    "            x.mjd.values.reshape([-1, 1]),\n",
    "            x.flux_normalized.values.reshape([-1, 1]))\n",
    "        return lr.coef_[0][0]\n",
    "\n",
    "    new_df = new_df.groupby(\n",
    "        [\"object_id\", \"cluster\", \"passband\"],\n",
    "        as_index=False)[\"object_id\", \"cluster\", \"passband\", \"mjd\",\n",
    "                        \"flux_normalized\"].progress_apply(\n",
    "                            lambda x: get_coef_(x)).unstack().groupby(\n",
    "                                \"object_id\").mean().reset_index()\n",
    "    new_df.columns = pd.Index(\n",
    "        [new_df.columns.name + str(e) for e in new_df.columns])\n",
    "    new_df.rename(columns={\"passbandobject_id\": \"object_id\"}, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def linear_slope_multi(d):\n",
    "    n_record = d.shape[0]\n",
    "    default_chunk = n_record // 8\n",
    "    head = 0\n",
    "    df_pool = []\n",
    "    for _ in range(7):\n",
    "        new_df = d.loc[head:head+default_chunk, :]\n",
    "        last_id = new_df.object_id.unique()[-1]\n",
    "        len_last = new_df.query(\"object_id == @last_id\").shape[0]\n",
    "        new_df = new_df.loc[head:head+default_chunk-len_last, :]\n",
    "        df_pool.append(new_df)\n",
    "        head = head + default_chunk - len_last+1\n",
    "    df_pool.append(d.loc[head:, :])\n",
    "    pool = Pool(8)\n",
    "    dfs = pool.map(linear_slope, df_pool)\n",
    "    pool.close()\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "\n",
    "def num_outliers(df):\n",
    "    new_df = df.groupby(\"object_id\").agg({\"flux\": [\"mean\", \"std\"]})\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df[\"upper_sigma\"] = new_df[\"flux_mean\"] + new_df[\"flux_std\"]\n",
    "    new_df[\"upper_2sigma\"] = new_df[\"flux_mean\"] + 2 * new_df[\"flux_std\"]\n",
    "    new_df[\"lower_sigma\"] = new_df[\"flux_mean\"] - new_df[\"flux_std\"]\n",
    "    new_df[\"lower_2sigma\"] = new_df[\"flux_mean\"] - 2 * new_df[\"flux_std\"]\n",
    "    new_df.drop([\"flux_mean\", \"flux_std\"], axis=1, inplace=True)\n",
    "    new_df = pd.merge(df, new_df, how=\"left\", on=\"object_id\")\n",
    "    new_df[\"outside_sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_sigma\"])).astype(int)\n",
    "    new_df[\"outside_2sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_2sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_2sigma\"])).astype(int)\n",
    "\n",
    "    return_df = new_df.groupby(\"object_id\").agg({\n",
    "        \"outside_sigma\": \"sum\",\n",
    "        \"outside_2sigma\": \"sum\"\n",
    "    })\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LKMf0xQChG8"
   },
   "outputs": [],
   "source": [
    "def get_full(df, meta):\n",
    "    agg_basic = basic(df)\n",
    "    agg_cluster = with_cluster(df)\n",
    "\n",
    "    cl_mean_diff = cluster_mean_diff(df)\n",
    "    ps_std_diff = passband_std_difference(df)\n",
    "    lin_sl = linear_slope(df)\n",
    "    num_out = num_outliers(df)\n",
    "\n",
    "    full = pd.merge(agg_basic, agg_cluster, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, cl_mean_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, ps_std_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, lin_sl, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, num_out, how=\"left\", on=\"object_id\")\n",
    "\n",
    "    full = pd.merge(full, meta, how=\"left\", on=\"object_id\")\n",
    "    if \"target\" in full.columns:\n",
    "        full.drop(\"target\", axis=1, inplace=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_data(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = meta.target\n",
    "    classes = sorted(y.unique())\n",
    "    class_weight = {c: 1 for c in classes}\n",
    "\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "    oof_df = full[[\"object_id\"]]\n",
    "    del full['object_id'], full['distmod'], full['hostgal_specz']\n",
    "    del full['ra'], full['decl'], full['gal_l'], full['gal_b'], full['ddf']\n",
    "    return full, y, classes, class_weight, oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Jeawd1ovCpWG",
    "outputId": "767f62f9-a28a-431e-feaa-8bd4b452376f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:00<00:00, 21.81it/s]\n",
      "apply progress: 100%|██████████| 148786/148786 [00:40<00:00, 3648.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 1.05 s, total: 1min 25s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full, y, classes, class_weight, oof_df = train_data(train, meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsC3-yUuaQS7"
   },
   "outputs": [],
   "source": [
    "train_mean = full.mean(axis=0)\n",
    "full.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHE2eKPaeur3"
   },
   "outputs": [],
   "source": [
    "# full.to_csv(\"full_train.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9Kcwi_EVe76_",
    "outputId": "6e267498-b599-4635-deeb-b1064c7caf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: full_train.csv (deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "# !zip full_train.csv.zip full_train.csv\n",
    "# !cp full_train.csv.zip /content/drive/My\\ Drive/plasticc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7V-R3l1IVFK"
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT3ihC7EHOjE"
   },
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqLsbJ9UXUlD"
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkxlccvqXV4V"
   },
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(\n",
    "        x='gain',\n",
    "        y='feature',\n",
    "        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances.png')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def save_cm(y, oof_preds, path):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i, val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "\n",
    "    y_map = np.zeros((y.shape[0], ))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds, axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    sample_sub = pd.read_csv(path)\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title='Confusion matrix')\n",
    "    plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJ5iBhWKXzCi"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1679
    },
    "colab_type": "code",
    "id": "05fOI0pgXvzy",
    "outputId": "1c09a6c6-1fde-4ca7-8079-0cbb8348f74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.791046\ttraining's wloss: 0.782572\tvalid_1's multi_logloss: 1.1569\tvalid_1's wloss: 0.961849\n",
      "[200]\ttraining's multi_logloss: 0.519309\ttraining's wloss: 0.507654\tvalid_1's multi_logloss: 0.931898\tvalid_1's wloss: 0.781479\n",
      "[300]\ttraining's multi_logloss: 0.395948\ttraining's wloss: 0.383851\tvalid_1's multi_logloss: 0.844591\tvalid_1's wloss: 0.740949\n",
      "[400]\ttraining's multi_logloss: 0.321824\ttraining's wloss: 0.310062\tvalid_1's multi_logloss: 0.797383\tvalid_1's wloss: 0.73397\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's multi_logloss: 0.314616\ttraining's wloss: 0.302846\tvalid_1's multi_logloss: 0.793141\tvalid_1's wloss: 0.733913\n",
      "0.7339133068049176\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.794798\ttraining's wloss: 0.788552\tvalid_1's multi_logloss: 1.14217\tvalid_1's wloss: 0.966386\n",
      "[200]\ttraining's multi_logloss: 0.520398\ttraining's wloss: 0.510283\tvalid_1's multi_logloss: 0.910448\tvalid_1's wloss: 0.779569\n",
      "[300]\ttraining's multi_logloss: 0.398767\ttraining's wloss: 0.387835\tvalid_1's multi_logloss: 0.822566\tvalid_1's wloss: 0.752474\n",
      "Early stopping, best iteration is:\n",
      "[337]\ttraining's multi_logloss: 0.367755\ttraining's wloss: 0.356637\tvalid_1's multi_logloss: 0.801615\tvalid_1's wloss: 0.751614\n",
      "0.7516137144366009\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.785985\ttraining's wloss: 0.778456\tvalid_1's multi_logloss: 1.16\tvalid_1's wloss: 0.942559\n",
      "[200]\ttraining's multi_logloss: 0.515555\ttraining's wloss: 0.504597\tvalid_1's multi_logloss: 0.931098\tvalid_1's wloss: 0.743684\n",
      "[300]\ttraining's multi_logloss: 0.394073\ttraining's wloss: 0.382874\tvalid_1's multi_logloss: 0.843563\tvalid_1's wloss: 0.695798\n",
      "[400]\ttraining's multi_logloss: 0.321287\ttraining's wloss: 0.310477\tvalid_1's multi_logloss: 0.795194\tvalid_1's wloss: 0.683799\n",
      "[500]\ttraining's multi_logloss: 0.269587\ttraining's wloss: 0.25931\tvalid_1's multi_logloss: 0.761341\tvalid_1's wloss: 0.680886\n",
      "Early stopping, best iteration is:\n",
      "[494]\ttraining's multi_logloss: 0.272359\ttraining's wloss: 0.26204\tvalid_1's multi_logloss: 0.762842\tvalid_1's wloss: 0.679958\n",
      "0.6799576871934968\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.794843\ttraining's wloss: 0.785983\tvalid_1's multi_logloss: 1.15358\tvalid_1's wloss: 0.938145\n",
      "[200]\ttraining's multi_logloss: 0.520275\ttraining's wloss: 0.509143\tvalid_1's multi_logloss: 0.918026\tvalid_1's wloss: 0.737301\n",
      "[300]\ttraining's multi_logloss: 0.397429\ttraining's wloss: 0.385636\tvalid_1's multi_logloss: 0.825149\tvalid_1's wloss: 0.686327\n",
      "[400]\ttraining's multi_logloss: 0.321491\ttraining's wloss: 0.31003\tvalid_1's multi_logloss: 0.771236\tvalid_1's wloss: 0.669702\n",
      "Early stopping, best iteration is:\n",
      "[446]\ttraining's multi_logloss: 0.295504\ttraining's wloss: 0.284256\tvalid_1's multi_logloss: 0.753422\tvalid_1's wloss: 0.664843\n",
      "0.6648431898905458\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.791076\ttraining's wloss: 0.782479\tvalid_1's multi_logloss: 1.15887\tvalid_1's wloss: 0.98094\n",
      "[200]\ttraining's multi_logloss: 0.519397\ttraining's wloss: 0.507734\tvalid_1's multi_logloss: 0.920106\tvalid_1's wloss: 0.781038\n",
      "[300]\ttraining's multi_logloss: 0.397541\ttraining's wloss: 0.385769\tvalid_1's multi_logloss: 0.820369\tvalid_1's wloss: 0.722657\n",
      "[400]\ttraining's multi_logloss: 0.324109\ttraining's wloss: 0.312608\tvalid_1's multi_logloss: 0.764708\tvalid_1's wloss: 0.701701\n",
      "Early stopping, best iteration is:\n",
      "[433]\ttraining's multi_logloss: 0.305688\ttraining's wloss: 0.294244\tvalid_1's multi_logloss: 0.752339\tvalid_1's wloss: 0.69989\n",
      "0.6998898587930915\n",
      "MULTI WEIGHTED LOG LOSS : 0.70598 \n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 14,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': .9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'reg_alpha': .01,\n",
    "    'reg_lambda': .01,\n",
    "    'min_split_gain': 0.01,\n",
    "    'min_child_weight': 10,\n",
    "    'n_estimators': 1000,\n",
    "    'silent': -1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i: np.sum(w) / w[i] for i in w.index}\n",
    "oof_preds = np.zeros((len(full), np.unique(y).shape[0]))\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = full.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = full.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x,\n",
    "        trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights))\n",
    "    oof_preds[val_, :] = clf.predict_proba(\n",
    "        val_x, num_iteration=clf.best_iteration_)\n",
    "    print(multi_weighted_logloss(val_y, oof_preds[val_, :]))\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = full.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    clfs.append(clf)\n",
    "\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(\n",
    "    y_true=y, y_preds=oof_preds))\n",
    "save_importances(importances_=importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "UN8UkPi8YBcB",
    "outputId": "c3d939cf-bbb1-4b56-bd35-b59b96ce35e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    }
   ],
   "source": [
    "save_importances(importances_=importances)\n",
    "save_cm(y, oof_preds, data_dir + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mjd_size</th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>passband1</th>\n",
       "      <th>passband2</th>\n",
       "      <th>passband3</th>\n",
       "      <th>passband4</th>\n",
       "      <th>passband5</th>\n",
       "      <th>outside_sigma</th>\n",
       "      <th>outside_2sigma</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>mwebv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352</td>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>115</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>350</td>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.002616</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>136</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>330</td>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>-0.003559</td>\n",
       "      <td>-0.003062</td>\n",
       "      <td>-0.003432</td>\n",
       "      <td>-0.002205</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>351</td>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-0.002321</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>-0.003098</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352</td>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003188</td>\n",
       "      <td>-0.002722</td>\n",
       "      <td>-0.001763</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mjd_size     flux_min    flux_max   flux_mean  flux_median    flux_std  \\\n",
       "0       352 -1100.440063  660.626343 -123.096998   -89.477524  394.109851   \n",
       "1       350   -14.735178   14.770886   -1.423351    -0.873033    6.471144   \n",
       "2       330   -19.159811   47.310059    2.267434     0.409172    8.022239   \n",
       "3       351   -15.494463  220.795212    8.909206     1.035895   27.558208   \n",
       "4       352   -16.543753  143.600189    7.145702     1.141288   20.051722   \n",
       "\n",
       "   flux_skew  flux_err_min  flux_err_max  flux_err_mean  ...    passband1  \\\n",
       "0  -0.349540      2.130510     12.845472       4.482743  ...     0.003049   \n",
       "1   0.014989      0.639458      9.115748       2.359620  ...    -0.001209   \n",
       "2   3.177854      0.695106     11.281384       2.471061  ...    -0.002277   \n",
       "3   4.979826      0.567170     55.892746       2.555576  ...    -0.001314   \n",
       "4   4.406298      0.695277     11.383690       2.753004  ...    -0.003188   \n",
       "\n",
       "   passband2  passband3  passband4  passband5  outside_sigma  outside_2sigma  \\\n",
       "0   0.003407   0.003532   0.003279   0.002969            115              17   \n",
       "1  -0.002616  -0.002109  -0.000896  -0.001617            136               4   \n",
       "2  -0.003559  -0.003062  -0.003432  -0.002205             31              18   \n",
       "3  -0.001628  -0.002321  -0.002109  -0.003098             21              15   \n",
       "4  -0.002722  -0.001763  -0.001460  -0.002168             27              12   \n",
       "\n",
       "   hostgal_photoz  hostgal_photoz_err  mwebv  \n",
       "0          0.0000              0.0000  0.017  \n",
       "1          1.6267              0.2552  0.007  \n",
       "2          0.2262              0.0157  0.021  \n",
       "3          0.2813              1.1523  0.007  \n",
       "4          0.2415              0.0176  0.024  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUSDtkDkZa-S"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0OLVvaQY0si"
   },
   "outputs": [],
   "source": [
    "def predict_chunk(df_, clfs_, meta_, features, train_mean, i_c):\n",
    "    # Group by object id\n",
    "    agg_ = get_full(df_, meta_)\n",
    "\n",
    "    full_test = agg_.fillna(0)\n",
    "    \n",
    "    if i_c == 0:\n",
    "        full_test.to_csv('full_test.csv', header=True, mode='a', index=False)\n",
    "    else:\n",
    "        full_test.to_csv('full_test.csv', header=False, mode='a', index=False)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    return preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [06:41<00:00, 35.24s/it]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:24<00:00,  2.29s/it]\n",
      "apply progress: 100%|██████████| 7498839/7498839 [33:45<00:00, 3701.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_cluster_249999655_299999649.csv done in 76.99628765185675 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [06:45<00:00, 35.17s/it]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:24<00:00,  2.39s/it]\n",
      "apply progress: 100%|██████████| 7499693/7499693 [33:28<00:00, 3733.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_cluster_299999650_349999619.csv done in 153.7639448563258 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [06:42<00:00, 35.59s/it]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it]\n",
      "apply progress: 100%|██████████| 7494642/7494642 [33:35<00:00, 3718.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_cluster_349999620_399999534.csv done in 230.5935180266698 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [06:39<00:00, 34.83s/it]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:24<00:00,  2.27s/it]\n",
      "apply progress: 100%|██████████| 7498229/7498229 [33:29<00:00, 3731.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_cluster_399999535_449999411.csv done in 307.43774174054465 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [00:29<00:00,  2.65s/it]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:01<00:00,  5.46it/s]\n",
      "apply progress: 100%|██████████| 547528/547528 [02:23<00:00, 3807.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_cluster_449999412_end.csv done in 313.000310254097 minutes\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "start = time.time()\n",
    "indices = [ (249999655, 299999649), (299999650, 349999619), (349999620, 399999534),\n",
    "           (399999535, 449999411), (449999412, \"end\")]\n",
    "test_files = [f\"test_with_cluster_{e[0]}_{e[1]}.csv\" for e in indices]\n",
    "meta_test = pd.read_csv(data_dir + '/test_set_metadata.csv')\n",
    "for i_c, f in enumerate(test_files):\n",
    "    test = pd.read_csv(f)\n",
    "    preds_df = predict_chunk(\n",
    "        df_=test,\n",
    "        clfs_=clfs,\n",
    "        meta_=meta_test,\n",
    "        features=full.columns,\n",
    "        train_mean=train_mean,\n",
    "        i_c=i_c+1\n",
    "    )\n",
    "    if i_c+1 == 0:\n",
    "        preds_df.to_csv('predictions.csv', header=True, mode='a', index=False)\n",
    "    else:\n",
    "        preds_df.to_csv('predictions.csv', header=False, mode='a', index=False)\n",
    "\n",
    "    del preds_df\n",
    "    print(f'{f} done in {(time.time() - start) / 60} minutes', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/hidehisa/.kaggle/kaggle.json'\n",
      "usage: kaggle [-h] [-v] {competitions,c,datasets,d,config} ...\n",
      "kaggle: error: unrecognized arguments: PLAsTiCC-2018\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -f predictions.csv -m \"First LightGBM\" PLAsTiCC-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>class_42</th>\n",
       "      <th>class_52</th>\n",
       "      <th>class_53</th>\n",
       "      <th>class_62</th>\n",
       "      <th>class_64</th>\n",
       "      <th>class_65</th>\n",
       "      <th>class_67</th>\n",
       "      <th>class_88</th>\n",
       "      <th>class_90</th>\n",
       "      <th>class_92</th>\n",
       "      <th>class_95</th>\n",
       "      <th>object_id</th>\n",
       "      <th>class_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.385006</td>\n",
       "      <td>0.372684</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.046661</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.182646</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>13</td>\n",
       "      <td>0.154530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.096338</td>\n",
       "      <td>0.049360</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.044348</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>14</td>\n",
       "      <td>0.098824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.006785</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.064716</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.052370</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.715034</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>17</td>\n",
       "      <td>0.110738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.034082</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.119353</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.268141</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.415477</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.069284</td>\n",
       "      <td>23</td>\n",
       "      <td>0.160039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.082547</td>\n",
       "      <td>0.127206</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.049893</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.707199</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>34</td>\n",
       "      <td>0.112184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_6  class_15  class_16  class_42  class_52  class_53  class_62  \\\n",
       "0  0.000194  0.004572  0.000182  0.385006  0.372684  0.000175  0.046661   \n",
       "1  0.000168  0.011042  0.000254  0.096338  0.049360  0.000162  0.044348   \n",
       "2  0.000169  0.006785  0.000275  0.064716  0.057129  0.000162  0.052370   \n",
       "3  0.000183  0.003858  0.000228  0.081600  0.034082  0.000171  0.119353   \n",
       "4  0.000116  0.004194  0.000109  0.082547  0.127206  0.000106  0.049893   \n",
       "\n",
       "   class_64  class_65  class_67  class_88  class_90  class_92  class_95  \\\n",
       "0  0.000207  0.000206  0.002123  0.000330  0.182646  0.000213  0.004800   \n",
       "1  0.001312  0.000907  0.026488  0.001935  0.755897  0.000227  0.011561   \n",
       "2  0.003755  0.000631  0.055153  0.002003  0.715034  0.000165  0.041656   \n",
       "3  0.006298  0.000564  0.268141  0.000587  0.415477  0.000176  0.069284   \n",
       "4  0.000237  0.000176  0.027143  0.000123  0.707199  0.000105  0.000848   \n",
       "\n",
       "   object_id  class_99  \n",
       "0         13  0.154530  \n",
       "1         14  0.098824  \n",
       "2         17  0.110738  \n",
       "3         23  0.160039  \n",
       "4         34  0.112184  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions.csv (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "!zip predictions.csv.zip predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1162
    },
    "colab_type": "code",
    "id": "g7i-xAr5Zrn_",
    "outputId": "dd88a71b-5b07-4949-c45f-b637860566cd"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "meta_test = pd.read_csv(data_dir + '/test_set_metadata.csv')\n",
    "# meta_test.set_index('object_id',inplace=True)\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks = 10000000\n",
    "remain_df = None\n",
    "\n",
    "for i_c, df in enumerate(pd.read_csv(data_dir + '/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "    # Check object_ids\n",
    "    # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "    unique_ids = np.unique(df['object_id'])\n",
    "    new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "    if remain_df is None:\n",
    "        df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "    else:\n",
    "        df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "    # Create remaining samples df\n",
    "    remain_df = new_remain_df\n",
    "    preds_df = predict_chunk(df_=df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=full.columns,\n",
    "                             train_mean=train_mean)\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    print('%15d done in %5.1f minutes' % (chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "\n",
    "# Compute last object in remain_df\n",
    "preds_df = predict_chunk(df_=remain_df,\n",
    "                         clfs_=clfs,\n",
    "                         meta_=meta_test,\n",
    "                         features=full_train.columns,\n",
    "                         train_mean=train_mean)\n",
    "\n",
    "preds_df.to_csv('predictions.csv', header=False, mode='a', index=False)\n",
    "#!zip test_with_cluster.csv.zip test_with_cluster.csv\n",
    "#!zip full_test.csv.zip full_test.csv\n",
    "#!cp test_with_cluster.csv.zip /content/drive/My\\ Drive/plasticc/\n",
    "#!cp full_test.csv.zip /content/drive/My\\ Drive/plasticc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PLAsTiCC.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
