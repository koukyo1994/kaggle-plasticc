{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hidehisa/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from multiprocessing import Pool\n",
    "tqdm.pandas(desc=\"apply progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hidehisa/.kaggle/competitions/plasticc\"\n",
    "train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "    agg_flux_diff = agg_df.reset_index()[[\"object_id\", \"flux_diff\"]]\n",
    "    df2 = pd.merge(df, agg_df, how=\"left\", on=\"object_id\")\n",
    "    df2[\"flux_norm\"] = df2.flux / df2.flux_diff\n",
    "    del df2[\"flux\"]\n",
    "    fcp = {\n",
    "        'fft_coefficient': [{\n",
    "            'coeff': 0,\n",
    "            'attr': 'abs'\n",
    "        }, {\n",
    "            'coeff': 1,\n",
    "            'attr': 'abs'\n",
    "        }],\n",
    "        'kurtosis':\n",
    "        None,\n",
    "        'skewness':\n",
    "        None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp2 = {\n",
    "        \"fft_coefficient\": [{\n",
    "            \"coeff\": 0,\n",
    "            \"attr\": \"abs\"\n",
    "        }, {\n",
    "            \"coeff\": 1,\n",
    "            \"attr\": \"abs\"\n",
    "        }],\n",
    "        \"abs_energy\": None,\n",
    "        \"sample_entropy\": None\n",
    "    }\n",
    "    fcp_flux = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp_flux_by_flux_ratio_sq = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None\n",
    "    }\n",
    "    agg_df_ts = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp,\n",
    "        n_jobs=6)\n",
    "    agg_df_ts2 = extract_features(\n",
    "        df2,\n",
    "        column_id=\"object_id\",\n",
    "        column_sort=\"mjd\",\n",
    "        column_kind=\"passband\",\n",
    "        column_value=\"flux_norm\",\n",
    "        default_fc_parameters=fcp2,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_flux = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux\",\n",
    "        default_fc_parameters=fcp_flux,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_ffrs = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux_by_flux_ratio_sq\",\n",
    "        default_fc_parameters=fcp_flux_by_flux_ratio_sq,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters={\n",
    "            'maximum': None,\n",
    "            'minimum': None\n",
    "        },\n",
    "        n_jobs=8)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd[\n",
    "        'mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts2.columns = pd.Index([e + \"_norm\" for e in agg_df_ts2.columns])\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd, on='id')\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ts2, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_flux, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ffrs, on=\"id\")\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    agg_df_ts.index.rename('object_id', inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts, on='object_id')\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def cluster_mean_diff(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\"]).agg({\n",
    "        \"flux\": [\"mean\", \"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in new_df.columns.tolist()])\n",
    "    new_df[\"normalized_mean\"] = new_df[\"flux_mean\"] / (\n",
    "        new_df[\"flux_max\"] - new_df[\"flux_min\"])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df.groupby(\"object_id\").agg({\"normalized_mean\": \"std\"})\n",
    "\n",
    "\n",
    "def passband_std_difference(df):\n",
    "    std_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": \"std\"\n",
    "    }).reset_index().groupby([\"object_id\",\n",
    "                              \"passband\"])[\"flux\"].mean().reset_index()\n",
    "    std_df_max = std_df.groupby(\"object_id\")[\"flux\"].max()\n",
    "    std_df_min = std_df.groupby(\"object_id\")[\"flux\"].min()\n",
    "    return (std_df_max / std_df_min).reset_index()\n",
    "\n",
    "\n",
    "def num_outliers(df):\n",
    "    new_df = df.groupby(\"object_id\").agg({\"flux\": [\"mean\", \"std\"]})\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df[\"upper_sigma\"] = new_df[\"flux_mean\"] + new_df[\"flux_std\"]\n",
    "    new_df[\"upper_2sigma\"] = new_df[\"flux_mean\"] + 2 * new_df[\"flux_std\"]\n",
    "    new_df[\"lower_sigma\"] = new_df[\"flux_mean\"] - new_df[\"flux_std\"]\n",
    "    new_df[\"lower_2sigma\"] = new_df[\"flux_mean\"] - 2 * new_df[\"flux_std\"]\n",
    "    new_df.drop([\"flux_mean\", \"flux_std\"], axis=1, inplace=True)\n",
    "    new_df = pd.merge(df, new_df, how=\"left\", on=\"object_id\")\n",
    "    new_df[\"outside_sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_sigma\"])).astype(int)\n",
    "    new_df[\"outside_2sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_2sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_2sigma\"])).astype(int)\n",
    "\n",
    "    return_df = new_df.groupby(\"object_id\").agg({\n",
    "        \"outside_sigma\": \"sum\",\n",
    "        \"outside_2sigma\": \"sum\"\n",
    "    })\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "\n",
    "\n",
    "def process_meta(meta_df):\n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full(df, meta):\n",
    "    agg_basic = basic(df)\n",
    "    cl_mean_diff = cluster_mean_diff(df)\n",
    "    ps_std_diff = passband_std_difference(df)\n",
    "    num_out = num_outliers(df)\n",
    "\n",
    "    full = pd.merge(agg_basic, cl_mean_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, ps_std_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, num_out, how=\"left\", on=\"object_id\")\n",
    "    meta = process_meta(meta)\n",
    "    full = pd.merge(full, meta, how=\"left\", on=\"object_id\")\n",
    "    if \"target\" in full.columns:\n",
    "        full.drop(\"target\", axis=1, inplace=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_data(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = meta.target\n",
    "    classes = sorted(y.unique())\n",
    "    class_weight = {c: 1 for c in classes}\n",
    "\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "    oof_df = full[[\"object_id\"]]\n",
    "    del full['object_id'], full['distmod'], full['hostgal_specz']\n",
    "    del full['ra'], full['decl'], full['gal_l'], full['gal_b'], full['ddf']\n",
    "    return full, y, classes, class_weight, oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.90it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:16<00:00,  2.44it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 12.26it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 29.77it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 94.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 1.28 s, total: 22.4 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full, y, classes, class_weight, oof_df = train_data(train, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = full.mean(axis=0)\n",
    "full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "full.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def multi_logloss_tf(y_true, y_pred):\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    y_p = tf.clip_by_value(y_pred, 1e-15, 1-1e-15)\n",
    "    y_p_log = tf.log(y_p)\n",
    "    y_log_ones = tf.reduce_sum(y_true * y_p_log, axis=0)\n",
    "    nb_pos = tf.reduce_sum(y_true, axis=0)\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "    loss = - tf.reduce_sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def build_model(dropout_rate=0.25,activation='relu'):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//2,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//4,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//8,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_new = full.copy()\n",
    "ss = StandardScaler()\n",
    "full_train_ss = ss.fit_transform(full_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = np.unique(y)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y])\n",
    "y_categorical = to_categorical(y_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_count = Counter(y_map)\n",
    "wtable = np.zeros((len(unique_y),))\n",
    "for i in range(len(unique_y)):\n",
    "    wtable[i] = y_count[i]/y_map.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6274 samples, validate on 1574 samples\n",
      "Epoch 1/600\n",
      "6274/6274 [==============================] - 1s 237us/step - loss: 2.5560 - mywloss: 2.5560 - val_loss: 1.4634 - val_mywloss: 1.4634\n",
      "Epoch 2/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 1.8947 - mywloss: 1.8947 - val_loss: 1.2534 - val_mywloss: 1.2534\n",
      "Epoch 3/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 1.6245 - mywloss: 1.6245 - val_loss: 1.1496 - val_mywloss: 1.1496\n",
      "Epoch 4/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 1.5152 - mywloss: 1.5152 - val_loss: 1.1361 - val_mywloss: 1.1361\n",
      "Epoch 5/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 1.4439 - mywloss: 1.4439 - val_loss: 1.0447 - val_mywloss: 1.0447\n",
      "Epoch 6/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 1.3406 - mywloss: 1.3406 - val_loss: 1.0138 - val_mywloss: 1.0138\n",
      "Epoch 7/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 1.2753 - mywloss: 1.2753 - val_loss: 1.0038 - val_mywloss: 1.0038\n",
      "Epoch 8/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 1.2467 - mywloss: 1.2467 - val_loss: 0.9711 - val_mywloss: 0.9711\n",
      "Epoch 9/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.2038 - mywloss: 1.2038 - val_loss: 0.9737 - val_mywloss: 0.9737\n",
      "Epoch 10/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.1643 - mywloss: 1.1643 - val_loss: 0.9340 - val_mywloss: 0.9340\n",
      "Epoch 11/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.1502 - mywloss: 1.1502 - val_loss: 0.9152 - val_mywloss: 0.9152\n",
      "Epoch 12/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 1.1516 - mywloss: 1.1516 - val_loss: 0.9038 - val_mywloss: 0.9038\n",
      "Epoch 13/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.0878 - mywloss: 1.0878 - val_loss: 0.9015 - val_mywloss: 0.9015\n",
      "Epoch 14/600\n",
      "6274/6274 [==============================] - 0s 73us/step - loss: 1.0723 - mywloss: 1.0723 - val_loss: 0.8758 - val_mywloss: 0.8758\n",
      "Epoch 15/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.0523 - mywloss: 1.0523 - val_loss: 0.8731 - val_mywloss: 0.8731\n",
      "Epoch 16/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 1.0520 - mywloss: 1.0520 - val_loss: 0.8708 - val_mywloss: 0.8708\n",
      "Epoch 17/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 1.0292 - mywloss: 1.0292 - val_loss: 0.8562 - val_mywloss: 0.8562\n",
      "Epoch 18/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 1.0007 - mywloss: 1.0007 - val_loss: 0.8604 - val_mywloss: 0.8604\n",
      "Epoch 19/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 1.0079 - mywloss: 1.0079 - val_loss: 0.8549 - val_mywloss: 0.8549\n",
      "Epoch 20/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 1.0147 - mywloss: 1.0147 - val_loss: 0.8582 - val_mywloss: 0.8582\n",
      "Epoch 21/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.9659 - mywloss: 0.9659 - val_loss: 0.8532 - val_mywloss: 0.8532\n",
      "Epoch 22/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.9479 - mywloss: 0.9479 - val_loss: 0.8382 - val_mywloss: 0.8382\n",
      "Epoch 23/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.9619 - mywloss: 0.9619 - val_loss: 0.8441 - val_mywloss: 0.8441\n",
      "Epoch 24/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.9674 - mywloss: 0.9674 - val_loss: 0.8356 - val_mywloss: 0.8356\n",
      "Epoch 25/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.9423 - mywloss: 0.9423 - val_loss: 0.8477 - val_mywloss: 0.8477\n",
      "Epoch 26/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.9406 - mywloss: 0.9406 - val_loss: 0.8359 - val_mywloss: 0.8359\n",
      "Epoch 27/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.9441 - mywloss: 0.9441 - val_loss: 0.8326 - val_mywloss: 0.8326\n",
      "Epoch 28/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.9192 - mywloss: 0.9192 - val_loss: 0.8230 - val_mywloss: 0.8230\n",
      "Epoch 29/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.9065 - mywloss: 0.9065 - val_loss: 0.8168 - val_mywloss: 0.8168\n",
      "Epoch 30/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.9068 - mywloss: 0.9068 - val_loss: 0.8169 - val_mywloss: 0.8169\n",
      "Epoch 31/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.9090 - mywloss: 0.9090 - val_loss: 0.8145 - val_mywloss: 0.8145\n",
      "Epoch 32/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.9019 - mywloss: 0.9019 - val_loss: 0.7984 - val_mywloss: 0.7984\n",
      "Epoch 33/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.8938 - mywloss: 0.8938 - val_loss: 0.8001 - val_mywloss: 0.8001\n",
      "Epoch 34/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8831 - mywloss: 0.8831 - val_loss: 0.7994 - val_mywloss: 0.7994\n",
      "Epoch 35/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8790 - mywloss: 0.8790 - val_loss: 0.7957 - val_mywloss: 0.7957\n",
      "Epoch 36/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8752 - mywloss: 0.8752 - val_loss: 0.7937 - val_mywloss: 0.7937\n",
      "Epoch 37/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8487 - mywloss: 0.8487 - val_loss: 0.7714 - val_mywloss: 0.7714\n",
      "Epoch 38/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.8478 - mywloss: 0.8478 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 39/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8597 - mywloss: 0.8597 - val_loss: 0.7845 - val_mywloss: 0.7845\n",
      "Epoch 40/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.8426 - mywloss: 0.8426 - val_loss: 0.7637 - val_mywloss: 0.7637\n",
      "Epoch 41/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.8615 - mywloss: 0.8615 - val_loss: 0.8058 - val_mywloss: 0.8058\n",
      "Epoch 42/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8423 - mywloss: 0.8423 - val_loss: 0.7790 - val_mywloss: 0.7790\n",
      "Epoch 43/600\n",
      "6274/6274 [==============================] - 0s 72us/step - loss: 0.8398 - mywloss: 0.8398 - val_loss: 0.7832 - val_mywloss: 0.7832\n",
      "Epoch 44/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.8420 - mywloss: 0.8420 - val_loss: 0.8001 - val_mywloss: 0.8001\n",
      "Epoch 45/600\n",
      "6274/6274 [==============================] - 0s 74us/step - loss: 0.8327 - mywloss: 0.8327 - val_loss: 0.7838 - val_mywloss: 0.7838\n",
      "Epoch 46/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.8403 - mywloss: 0.8403 - val_loss: 0.8059 - val_mywloss: 0.8059\n",
      "Epoch 47/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7906 - mywloss: 0.7906 - val_loss: 0.7826 - val_mywloss: 0.7826\n",
      "Epoch 48/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8235 - mywloss: 0.8235 - val_loss: 0.7649 - val_mywloss: 0.7649\n",
      "Epoch 49/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8020 - mywloss: 0.8020 - val_loss: 0.7711 - val_mywloss: 0.7711\n",
      "Epoch 50/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8109 - mywloss: 0.8109 - val_loss: 0.7667 - val_mywloss: 0.7667\n",
      "Epoch 51/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8111 - mywloss: 0.8111 - val_loss: 0.7698 - val_mywloss: 0.7698\n",
      "Epoch 52/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.8004 - mywloss: 0.8004 - val_loss: 0.7593 - val_mywloss: 0.7593\n",
      "Epoch 53/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8227 - mywloss: 0.8227 - val_loss: 0.7512 - val_mywloss: 0.7512\n",
      "Epoch 54/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7932 - mywloss: 0.7932 - val_loss: 0.7631 - val_mywloss: 0.7631\n",
      "Epoch 55/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.7902 - mywloss: 0.7902 - val_loss: 0.7625 - val_mywloss: 0.7625\n",
      "Epoch 56/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8026 - mywloss: 0.8026 - val_loss: 0.7635 - val_mywloss: 0.7635\n",
      "Epoch 57/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.8088 - mywloss: 0.8088 - val_loss: 0.7899 - val_mywloss: 0.7899\n",
      "Epoch 58/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7883 - mywloss: 0.7883 - val_loss: 0.7711 - val_mywloss: 0.7711\n",
      "Epoch 59/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7815 - mywloss: 0.7815 - val_loss: 0.7453 - val_mywloss: 0.7453\n",
      "Epoch 60/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7904 - mywloss: 0.7904 - val_loss: 0.7635 - val_mywloss: 0.7635\n",
      "Epoch 61/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7847 - mywloss: 0.7847 - val_loss: 0.7613 - val_mywloss: 0.7613\n",
      "Epoch 62/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7811 - mywloss: 0.7811 - val_loss: 0.7482 - val_mywloss: 0.7482\n",
      "Epoch 63/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7854 - mywloss: 0.7854 - val_loss: 0.7529 - val_mywloss: 0.7529\n",
      "Epoch 64/600\n",
      "6274/6274 [==============================] - 0s 77us/step - loss: 0.7652 - mywloss: 0.7652 - val_loss: 0.7587 - val_mywloss: 0.7587\n",
      "Epoch 65/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7664 - mywloss: 0.7664 - val_loss: 0.7311 - val_mywloss: 0.7311\n",
      "Epoch 66/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7682 - mywloss: 0.7682 - val_loss: 0.7440 - val_mywloss: 0.7440\n",
      "Epoch 67/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7690 - mywloss: 0.7690 - val_loss: 0.7610 - val_mywloss: 0.7610\n",
      "Epoch 68/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.7691 - mywloss: 0.7691 - val_loss: 0.7360 - val_mywloss: 0.7360\n",
      "Epoch 69/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7553 - mywloss: 0.7553 - val_loss: 0.7331 - val_mywloss: 0.7331\n",
      "Epoch 70/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.7753 - mywloss: 0.7753 - val_loss: 0.7430 - val_mywloss: 0.7430\n",
      "Epoch 71/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7529 - mywloss: 0.7529 - val_loss: 0.7404 - val_mywloss: 0.7404\n",
      "Epoch 72/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7523 - mywloss: 0.7523 - val_loss: 0.7422 - val_mywloss: 0.7422\n",
      "Epoch 73/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7454 - mywloss: 0.7454 - val_loss: 0.7359 - val_mywloss: 0.7359\n",
      "Epoch 74/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7391 - mywloss: 0.7391 - val_loss: 0.7328 - val_mywloss: 0.7328\n",
      "Epoch 75/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7418 - mywloss: 0.7418 - val_loss: 0.7363 - val_mywloss: 0.7363\n",
      "Epoch 76/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.7304 - mywloss: 0.7304 - val_loss: 0.7434 - val_mywloss: 0.7434\n",
      "Epoch 77/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7447 - mywloss: 0.7447 - val_loss: 0.7381 - val_mywloss: 0.7381\n",
      "Epoch 78/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7440 - mywloss: 0.7440 - val_loss: 0.7317 - val_mywloss: 0.7317\n",
      "Epoch 79/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7429 - mywloss: 0.7429 - val_loss: 0.7235 - val_mywloss: 0.7235\n",
      "Epoch 80/600\n",
      "6274/6274 [==============================] - 0s 73us/step - loss: 0.7253 - mywloss: 0.7253 - val_loss: 0.7199 - val_mywloss: 0.7199\n",
      "Epoch 81/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.7304 - mywloss: 0.7304 - val_loss: 0.7260 - val_mywloss: 0.7260\n",
      "Epoch 82/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7270 - mywloss: 0.7270 - val_loss: 0.7436 - val_mywloss: 0.7436\n",
      "Epoch 83/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7347 - mywloss: 0.7347 - val_loss: 0.7363 - val_mywloss: 0.7363\n",
      "Epoch 84/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7222 - mywloss: 0.7222 - val_loss: 0.7341 - val_mywloss: 0.7341\n",
      "Epoch 85/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.7266 - mywloss: 0.7266 - val_loss: 0.7773 - val_mywloss: 0.7773\n",
      "Epoch 86/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7363 - mywloss: 0.7363 - val_loss: 0.7519 - val_mywloss: 0.7519\n",
      "Epoch 87/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.7334 - mywloss: 0.7334 - val_loss: 0.7368 - val_mywloss: 0.7368\n",
      "Epoch 88/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.7114 - mywloss: 0.7114 - val_loss: 0.7334 - val_mywloss: 0.7334\n",
      "Epoch 89/600\n",
      "6274/6274 [==============================] - 0s 77us/step - loss: 0.7258 - mywloss: 0.7258 - val_loss: 0.7242 - val_mywloss: 0.7242\n",
      "Epoch 90/600\n",
      "6274/6274 [==============================] - 0s 72us/step - loss: 0.7157 - mywloss: 0.7157 - val_loss: 0.7240 - val_mywloss: 0.7240\n",
      "Epoch 91/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7071 - mywloss: 0.7071 - val_loss: 0.7369 - val_mywloss: 0.7369\n",
      "Epoch 92/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.7017 - mywloss: 0.7017 - val_loss: 0.7283 - val_mywloss: 0.7283\n",
      "Epoch 93/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6913 - mywloss: 0.6913 - val_loss: 0.7455 - val_mywloss: 0.7455\n",
      "Epoch 94/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7220 - mywloss: 0.7220 - val_loss: 0.7489 - val_mywloss: 0.7489\n",
      "Epoch 95/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7010 - mywloss: 0.7010 - val_loss: 0.7686 - val_mywloss: 0.7686\n",
      "Epoch 96/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7271 - mywloss: 0.7271 - val_loss: 0.7449 - val_mywloss: 0.7449\n",
      "Epoch 97/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7280 - mywloss: 0.7280 - val_loss: 0.7419 - val_mywloss: 0.7419\n",
      "Epoch 98/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.7289 - mywloss: 0.7289 - val_loss: 0.7244 - val_mywloss: 0.7244\n",
      "Epoch 99/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7064 - mywloss: 0.7064 - val_loss: 0.7422 - val_mywloss: 0.7422\n",
      "Epoch 100/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6820 - mywloss: 0.6820 - val_loss: 0.7250 - val_mywloss: 0.7250\n",
      "Epoch 101/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6872 - mywloss: 0.6872 - val_loss: 0.7197 - val_mywloss: 0.7197\n",
      "Epoch 102/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.7033 - mywloss: 0.7033 - val_loss: 0.7479 - val_mywloss: 0.7479\n",
      "Epoch 103/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.7039 - mywloss: 0.7039 - val_loss: 0.7478 - val_mywloss: 0.7478\n",
      "Epoch 104/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6819 - mywloss: 0.6819 - val_loss: 0.6999 - val_mywloss: 0.6999\n",
      "Epoch 105/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.6827 - mywloss: 0.6827 - val_loss: 0.7357 - val_mywloss: 0.7357\n",
      "Epoch 106/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.7192 - mywloss: 0.7192 - val_loss: 0.7511 - val_mywloss: 0.7511\n",
      "Epoch 107/600\n",
      "6274/6274 [==============================] - 0s 78us/step - loss: 0.6738 - mywloss: 0.6738 - val_loss: 0.7476 - val_mywloss: 0.7476\n",
      "Epoch 108/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6894 - mywloss: 0.6894 - val_loss: 0.7286 - val_mywloss: 0.7286\n",
      "Epoch 109/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6928 - mywloss: 0.6928 - val_loss: 0.7118 - val_mywloss: 0.7118\n",
      "Epoch 110/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6924 - mywloss: 0.6924 - val_loss: 0.6966 - val_mywloss: 0.6966\n",
      "Epoch 111/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6878 - mywloss: 0.6878 - val_loss: 0.7172 - val_mywloss: 0.7172\n",
      "Epoch 112/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6866 - mywloss: 0.6866 - val_loss: 0.7216 - val_mywloss: 0.7216\n",
      "Epoch 113/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6960 - mywloss: 0.6960 - val_loss: 0.7108 - val_mywloss: 0.7108\n",
      "Epoch 114/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6897 - mywloss: 0.6897 - val_loss: 0.7319 - val_mywloss: 0.7319\n",
      "Epoch 115/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6689 - mywloss: 0.6689 - val_loss: 0.7330 - val_mywloss: 0.7330\n",
      "Epoch 116/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.6808 - mywloss: 0.6808 - val_loss: 0.7153 - val_mywloss: 0.7153\n",
      "Epoch 117/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6880 - mywloss: 0.6880 - val_loss: 0.7124 - val_mywloss: 0.7124\n",
      "Epoch 118/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6688 - mywloss: 0.6688 - val_loss: 0.7130 - val_mywloss: 0.7130\n",
      "Epoch 119/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6733 - mywloss: 0.6733 - val_loss: 0.7189 - val_mywloss: 0.7189\n",
      "Epoch 120/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6562 - mywloss: 0.6562 - val_loss: 0.7076 - val_mywloss: 0.7076\n",
      "Epoch 121/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6842 - mywloss: 0.6842 - val_loss: 0.7231 - val_mywloss: 0.7231\n",
      "Epoch 122/600\n",
      "6274/6274 [==============================] - 0s 75us/step - loss: 0.6829 - mywloss: 0.6829 - val_loss: 0.7161 - val_mywloss: 0.7161\n",
      "Epoch 123/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6895 - mywloss: 0.6895 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 124/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6804 - mywloss: 0.6804 - val_loss: 0.7141 - val_mywloss: 0.7141\n",
      "Epoch 125/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6791 - mywloss: 0.6791 - val_loss: 0.7096 - val_mywloss: 0.7096\n",
      "Epoch 126/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6670 - mywloss: 0.6670 - val_loss: 0.7350 - val_mywloss: 0.7350\n",
      "Epoch 127/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6668 - mywloss: 0.6668 - val_loss: 0.7086 - val_mywloss: 0.7086\n",
      "Epoch 128/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6435 - mywloss: 0.6435 - val_loss: 0.7496 - val_mywloss: 0.7496\n",
      "Epoch 129/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6826 - mywloss: 0.6826 - val_loss: 0.7071 - val_mywloss: 0.7071\n",
      "Epoch 130/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6553 - mywloss: 0.6553 - val_loss: 0.7283 - val_mywloss: 0.7283\n",
      "Epoch 131/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.6539 - mywloss: 0.6539 - val_loss: 0.6958 - val_mywloss: 0.6958\n",
      "Epoch 132/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6592 - mywloss: 0.6592 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 133/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6631 - mywloss: 0.6631 - val_loss: 0.7149 - val_mywloss: 0.7149\n",
      "Epoch 134/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6723 - mywloss: 0.6723 - val_loss: 0.7111 - val_mywloss: 0.7111\n",
      "Epoch 135/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6691 - mywloss: 0.6691 - val_loss: 0.7234 - val_mywloss: 0.7234\n",
      "Epoch 136/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6919 - mywloss: 0.6919 - val_loss: 0.7113 - val_mywloss: 0.7113\n",
      "Epoch 137/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6573 - mywloss: 0.6573 - val_loss: 0.7379 - val_mywloss: 0.7379\n",
      "Epoch 138/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6601 - mywloss: 0.6601 - val_loss: 0.7290 - val_mywloss: 0.7290\n",
      "Epoch 139/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6518 - mywloss: 0.6518 - val_loss: 0.7460 - val_mywloss: 0.7460\n",
      "Epoch 140/600\n",
      "6274/6274 [==============================] - 0s 78us/step - loss: 0.6298 - mywloss: 0.6298 - val_loss: 0.7063 - val_mywloss: 0.7063\n",
      "Epoch 141/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6626 - mywloss: 0.6626 - val_loss: 0.6974 - val_mywloss: 0.6974\n",
      "Epoch 142/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6771 - mywloss: 0.6771 - val_loss: 0.7286 - val_mywloss: 0.7286\n",
      "Epoch 143/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6534 - mywloss: 0.6534 - val_loss: 0.7182 - val_mywloss: 0.7182\n",
      "Epoch 144/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6457 - mywloss: 0.6457 - val_loss: 0.7144 - val_mywloss: 0.7144\n",
      "Epoch 145/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6478 - mywloss: 0.6478 - val_loss: 0.7371 - val_mywloss: 0.7371\n",
      "Epoch 146/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6289 - mywloss: 0.6289 - val_loss: 0.7636 - val_mywloss: 0.7636\n",
      "Epoch 147/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6554 - mywloss: 0.6554 - val_loss: 0.7279 - val_mywloss: 0.7279\n",
      "Epoch 148/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6424 - mywloss: 0.6424 - val_loss: 0.7243 - val_mywloss: 0.7243\n",
      "Epoch 149/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6452 - mywloss: 0.6452 - val_loss: 0.7038 - val_mywloss: 0.7038\n",
      "Epoch 150/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6516 - mywloss: 0.6516 - val_loss: 0.7050 - val_mywloss: 0.7050\n",
      "Epoch 151/600\n",
      "6274/6274 [==============================] - 0s 77us/step - loss: 0.6379 - mywloss: 0.6379 - val_loss: 0.6876 - val_mywloss: 0.6876\n",
      "Epoch 152/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6282 - mywloss: 0.6282 - val_loss: 0.7040 - val_mywloss: 0.7040\n",
      "Epoch 153/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6340 - mywloss: 0.6340 - val_loss: 0.7071 - val_mywloss: 0.7071\n",
      "Epoch 154/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6556 - mywloss: 0.6556 - val_loss: 0.6835 - val_mywloss: 0.6835\n",
      "Epoch 155/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.6374 - mywloss: 0.6374 - val_loss: 0.6987 - val_mywloss: 0.6987\n",
      "Epoch 156/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6337 - mywloss: 0.6337 - val_loss: 0.7032 - val_mywloss: 0.7032\n",
      "Epoch 157/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6313 - mywloss: 0.6313 - val_loss: 0.7227 - val_mywloss: 0.7227\n",
      "Epoch 158/600\n",
      "6274/6274 [==============================] - 0s 76us/step - loss: 0.6154 - mywloss: 0.6154 - val_loss: 0.7256 - val_mywloss: 0.7256\n",
      "Epoch 159/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6414 - mywloss: 0.6414 - val_loss: 0.7484 - val_mywloss: 0.7484\n",
      "Epoch 160/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6300 - mywloss: 0.6300 - val_loss: 0.7322 - val_mywloss: 0.7322\n",
      "Epoch 161/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6354 - mywloss: 0.6354 - val_loss: 0.7343 - val_mywloss: 0.7343\n",
      "Epoch 162/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6484 - mywloss: 0.6484 - val_loss: 0.7134 - val_mywloss: 0.7134\n",
      "Epoch 163/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6092 - mywloss: 0.6092 - val_loss: 0.7128 - val_mywloss: 0.7128\n",
      "Epoch 164/600\n",
      "6274/6274 [==============================] - 0s 73us/step - loss: 0.6306 - mywloss: 0.6306 - val_loss: 0.7032 - val_mywloss: 0.7032\n",
      "Epoch 165/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6352 - mywloss: 0.6352 - val_loss: 0.7178 - val_mywloss: 0.7178\n",
      "Epoch 166/600\n",
      "6274/6274 [==============================] - 0s 71us/step - loss: 0.6359 - mywloss: 0.6359 - val_loss: 0.6985 - val_mywloss: 0.6985\n",
      "Epoch 167/600\n",
      "6274/6274 [==============================] - 0s 70us/step - loss: 0.6233 - mywloss: 0.6233 - val_loss: 0.7189 - val_mywloss: 0.7189\n",
      "Epoch 168/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6441 - mywloss: 0.6441 - val_loss: 0.7079 - val_mywloss: 0.7079\n",
      "Epoch 169/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6296 - mywloss: 0.6296 - val_loss: 0.7579 - val_mywloss: 0.7579\n",
      "Epoch 170/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6180 - mywloss: 0.6180 - val_loss: 0.7285 - val_mywloss: 0.7285\n",
      "Epoch 171/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.6231 - mywloss: 0.6231 - val_loss: 0.7151 - val_mywloss: 0.7151\n",
      "Epoch 172/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6254 - mywloss: 0.6254 - val_loss: 0.7285 - val_mywloss: 0.7285\n",
      "Epoch 173/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6298 - mywloss: 0.6298 - val_loss: 0.7209 - val_mywloss: 0.7209\n",
      "Epoch 174/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6219 - mywloss: 0.6219 - val_loss: 0.7527 - val_mywloss: 0.7527\n",
      "Epoch 175/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6095 - mywloss: 0.6095 - val_loss: 0.7397 - val_mywloss: 0.7397\n",
      "Epoch 176/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6307 - mywloss: 0.6307 - val_loss: 0.7186 - val_mywloss: 0.7186\n",
      "Epoch 177/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6251 - mywloss: 0.6251 - val_loss: 0.7131 - val_mywloss: 0.7131\n",
      "Epoch 178/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6391 - mywloss: 0.6391 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 179/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.6052 - mywloss: 0.6052 - val_loss: 0.7307 - val_mywloss: 0.7307\n",
      "Epoch 180/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6212 - mywloss: 0.6212 - val_loss: 0.7056 - val_mywloss: 0.7056\n",
      "Epoch 181/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6143 - mywloss: 0.6143 - val_loss: 0.7164 - val_mywloss: 0.7164\n",
      "Epoch 182/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6037 - mywloss: 0.6037 - val_loss: 0.7239 - val_mywloss: 0.7239\n",
      "Epoch 183/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6199 - mywloss: 0.6199 - val_loss: 0.7277 - val_mywloss: 0.7277\n",
      "Epoch 184/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6031 - mywloss: 0.6031 - val_loss: 0.7175 - val_mywloss: 0.7175\n",
      "Epoch 185/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6000 - mywloss: 0.6000 - val_loss: 0.7115 - val_mywloss: 0.7115\n",
      "Epoch 186/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6409 - mywloss: 0.6409 - val_loss: 0.7003 - val_mywloss: 0.7003\n",
      "Epoch 187/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6070 - mywloss: 0.6070 - val_loss: 0.7199 - val_mywloss: 0.7199\n",
      "Epoch 188/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6187 - mywloss: 0.6187 - val_loss: 0.7175 - val_mywloss: 0.7175\n",
      "Epoch 189/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5993 - mywloss: 0.5993 - val_loss: 0.7336 - val_mywloss: 0.7336\n",
      "Epoch 190/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6118 - mywloss: 0.6118 - val_loss: 0.7436 - val_mywloss: 0.7436\n",
      "Epoch 191/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.6056 - mywloss: 0.6056 - val_loss: 0.7353 - val_mywloss: 0.7353\n",
      "Epoch 192/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.6274 - mywloss: 0.6274 - val_loss: 0.7441 - val_mywloss: 0.7441\n",
      "Epoch 193/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5927 - mywloss: 0.5927 - val_loss: 0.7431 - val_mywloss: 0.7431\n",
      "Epoch 194/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6142 - mywloss: 0.6142 - val_loss: 0.7265 - val_mywloss: 0.7265\n",
      "Epoch 195/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6161 - mywloss: 0.6161 - val_loss: 0.7289 - val_mywloss: 0.7289\n",
      "Epoch 196/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6262 - mywloss: 0.6262 - val_loss: 0.7246 - val_mywloss: 0.7246\n",
      "Epoch 197/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5868 - mywloss: 0.5868 - val_loss: 0.7290 - val_mywloss: 0.7290\n",
      "Epoch 198/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5961 - mywloss: 0.5961 - val_loss: 0.6932 - val_mywloss: 0.6932\n",
      "Epoch 199/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6425 - mywloss: 0.6425 - val_loss: 0.7332 - val_mywloss: 0.7332\n",
      "Epoch 200/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5961 - mywloss: 0.5961 - val_loss: 0.7303 - val_mywloss: 0.7303\n",
      "Epoch 201/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5995 - mywloss: 0.5995 - val_loss: 0.7133 - val_mywloss: 0.7133\n",
      "Epoch 202/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5961 - mywloss: 0.5961 - val_loss: 0.7448 - val_mywloss: 0.7448\n",
      "Epoch 203/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5964 - mywloss: 0.5964 - val_loss: 0.7275 - val_mywloss: 0.7275\n",
      "Epoch 204/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5967 - mywloss: 0.5967 - val_loss: 0.7415 - val_mywloss: 0.7415\n",
      "Epoch 205/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6083 - mywloss: 0.6083 - val_loss: 0.7319 - val_mywloss: 0.7319\n",
      "Epoch 206/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5872 - mywloss: 0.5872 - val_loss: 0.7219 - val_mywloss: 0.7219\n",
      "Epoch 207/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6047 - mywloss: 0.6047 - val_loss: 0.7211 - val_mywloss: 0.7211\n",
      "Epoch 208/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.6159 - mywloss: 0.6159 - val_loss: 0.7329 - val_mywloss: 0.7329\n",
      "Epoch 209/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5870 - mywloss: 0.5870 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 210/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5931 - mywloss: 0.5931 - val_loss: 0.7416 - val_mywloss: 0.7416\n",
      "Epoch 211/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5985 - mywloss: 0.5985 - val_loss: 0.7484 - val_mywloss: 0.7484\n",
      "Epoch 212/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5892 - mywloss: 0.5892 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 213/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6004 - mywloss: 0.6004 - val_loss: 0.7460 - val_mywloss: 0.7460\n",
      "Epoch 214/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5999 - mywloss: 0.5999 - val_loss: 0.7281 - val_mywloss: 0.7281\n",
      "Epoch 215/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5943 - mywloss: 0.5943 - val_loss: 0.7415 - val_mywloss: 0.7415\n",
      "Epoch 216/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5746 - mywloss: 0.5746 - val_loss: 0.7578 - val_mywloss: 0.7578\n",
      "Epoch 217/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.6075 - mywloss: 0.6075 - val_loss: 0.7427 - val_mywloss: 0.7427\n",
      "Epoch 218/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5875 - mywloss: 0.5875 - val_loss: 0.7270 - val_mywloss: 0.7270\n",
      "Epoch 219/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5891 - mywloss: 0.5891 - val_loss: 0.6982 - val_mywloss: 0.6982\n",
      "Epoch 220/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5933 - mywloss: 0.5933 - val_loss: 0.7098 - val_mywloss: 0.7098\n",
      "Epoch 221/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5833 - mywloss: 0.5833 - val_loss: 0.7097 - val_mywloss: 0.7097\n",
      "Epoch 222/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5750 - mywloss: 0.5750 - val_loss: 0.7352 - val_mywloss: 0.7352\n",
      "Epoch 223/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5657 - mywloss: 0.5657 - val_loss: 0.7271 - val_mywloss: 0.7271\n",
      "Epoch 224/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5591 - mywloss: 0.5591 - val_loss: 0.7381 - val_mywloss: 0.7381\n",
      "Epoch 225/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5819 - mywloss: 0.5819 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 226/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5995 - mywloss: 0.5995 - val_loss: 0.7352 - val_mywloss: 0.7352\n",
      "Epoch 227/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5736 - mywloss: 0.5736 - val_loss: 0.7321 - val_mywloss: 0.7321\n",
      "Epoch 228/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5804 - mywloss: 0.5804 - val_loss: 0.7631 - val_mywloss: 0.7631\n",
      "Epoch 229/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5598 - mywloss: 0.5598 - val_loss: 0.7701 - val_mywloss: 0.7701\n",
      "Epoch 230/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5873 - mywloss: 0.5873 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 231/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5534 - mywloss: 0.5534 - val_loss: 0.7304 - val_mywloss: 0.7304\n",
      "Epoch 232/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5636 - mywloss: 0.5636 - val_loss: 0.7591 - val_mywloss: 0.7591\n",
      "Epoch 233/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5823 - mywloss: 0.5823 - val_loss: 0.7459 - val_mywloss: 0.7459\n",
      "Epoch 234/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5943 - mywloss: 0.5943 - val_loss: 0.7452 - val_mywloss: 0.7452\n",
      "Epoch 235/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5852 - mywloss: 0.5852 - val_loss: 0.7489 - val_mywloss: 0.7489\n",
      "Epoch 236/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5695 - mywloss: 0.5695 - val_loss: 0.7584 - val_mywloss: 0.7584\n",
      "Epoch 237/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5969 - mywloss: 0.5969 - val_loss: 0.7703 - val_mywloss: 0.7703\n",
      "Epoch 238/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5668 - mywloss: 0.5668 - val_loss: 0.7855 - val_mywloss: 0.7855\n",
      "Epoch 239/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5754 - mywloss: 0.5754 - val_loss: 0.7331 - val_mywloss: 0.7331\n",
      "Epoch 240/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5525 - mywloss: 0.5525 - val_loss: 0.7656 - val_mywloss: 0.7656\n",
      "Epoch 241/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5812 - mywloss: 0.5812 - val_loss: 0.7773 - val_mywloss: 0.7773\n",
      "Epoch 242/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5831 - mywloss: 0.5831 - val_loss: 0.7670 - val_mywloss: 0.7670\n",
      "Epoch 243/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5688 - mywloss: 0.5688 - val_loss: 0.7727 - val_mywloss: 0.7727\n",
      "Epoch 244/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5705 - mywloss: 0.5705 - val_loss: 0.7734 - val_mywloss: 0.7734\n",
      "Epoch 245/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5902 - mywloss: 0.5902 - val_loss: 0.7620 - val_mywloss: 0.7620\n",
      "Epoch 246/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5894 - mywloss: 0.5894 - val_loss: 0.7304 - val_mywloss: 0.7304\n",
      "Epoch 247/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5827 - mywloss: 0.5827 - val_loss: 0.7683 - val_mywloss: 0.7683\n",
      "Epoch 248/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5825 - mywloss: 0.5825 - val_loss: 0.7455 - val_mywloss: 0.7455\n",
      "Epoch 249/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5779 - mywloss: 0.5779 - val_loss: 0.7458 - val_mywloss: 0.7458\n",
      "Epoch 250/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5837 - mywloss: 0.5837 - val_loss: 0.7533 - val_mywloss: 0.7533\n",
      "Epoch 251/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.6152 - mywloss: 0.6152 - val_loss: 0.7546 - val_mywloss: 0.7546\n",
      "Epoch 252/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5696 - mywloss: 0.5696 - val_loss: 0.7296 - val_mywloss: 0.7296\n",
      "Epoch 253/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5745 - mywloss: 0.5745 - val_loss: 0.7224 - val_mywloss: 0.7224\n",
      "Epoch 254/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5601 - mywloss: 0.5601 - val_loss: 0.7258 - val_mywloss: 0.7258\n",
      "Epoch 255/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5809 - mywloss: 0.5809 - val_loss: 0.7280 - val_mywloss: 0.7280\n",
      "Epoch 256/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5575 - mywloss: 0.5575 - val_loss: 0.7453 - val_mywloss: 0.7453\n",
      "Epoch 257/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5609 - mywloss: 0.5609 - val_loss: 0.7442 - val_mywloss: 0.7442\n",
      "Epoch 258/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5564 - mywloss: 0.5564 - val_loss: 0.7588 - val_mywloss: 0.7588\n",
      "Epoch 259/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5416 - mywloss: 0.5416 - val_loss: 0.7707 - val_mywloss: 0.7707\n",
      "Epoch 260/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5604 - mywloss: 0.5604 - val_loss: 0.7443 - val_mywloss: 0.7443\n",
      "Epoch 261/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5757 - mywloss: 0.5757 - val_loss: 0.7679 - val_mywloss: 0.7679\n",
      "Epoch 262/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5474 - mywloss: 0.5474 - val_loss: 0.7526 - val_mywloss: 0.7526\n",
      "Epoch 263/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5363 - mywloss: 0.5363 - val_loss: 0.7548 - val_mywloss: 0.7548\n",
      "Epoch 264/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5653 - mywloss: 0.5653 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 265/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5789 - mywloss: 0.5789 - val_loss: 0.7304 - val_mywloss: 0.7304\n",
      "Epoch 266/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5769 - mywloss: 0.5769 - val_loss: 0.7139 - val_mywloss: 0.7139\n",
      "Epoch 267/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5671 - mywloss: 0.5671 - val_loss: 0.7140 - val_mywloss: 0.7140\n",
      "Epoch 268/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5694 - mywloss: 0.5694 - val_loss: 0.7165 - val_mywloss: 0.7165\n",
      "Epoch 269/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5673 - mywloss: 0.5673 - val_loss: 0.7372 - val_mywloss: 0.7372\n",
      "Epoch 270/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5412 - mywloss: 0.5412 - val_loss: 0.7363 - val_mywloss: 0.7363\n",
      "Epoch 271/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5444 - mywloss: 0.5444 - val_loss: 0.7628 - val_mywloss: 0.7628\n",
      "Epoch 272/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5476 - mywloss: 0.5476 - val_loss: 0.7731 - val_mywloss: 0.7731\n",
      "Epoch 273/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5563 - mywloss: 0.5563 - val_loss: 0.7767 - val_mywloss: 0.7767\n",
      "Epoch 274/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5551 - mywloss: 0.5551 - val_loss: 0.7360 - val_mywloss: 0.7360\n",
      "Epoch 275/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5550 - mywloss: 0.5550 - val_loss: 0.7557 - val_mywloss: 0.7557\n",
      "Epoch 276/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5340 - mywloss: 0.5340 - val_loss: 0.7545 - val_mywloss: 0.7545\n",
      "Epoch 277/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5565 - mywloss: 0.5565 - val_loss: 0.7298 - val_mywloss: 0.7298\n",
      "Epoch 278/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5546 - mywloss: 0.5546 - val_loss: 0.7167 - val_mywloss: 0.7167\n",
      "Epoch 279/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5530 - mywloss: 0.5530 - val_loss: 0.6832 - val_mywloss: 0.6832\n",
      "Epoch 280/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5493 - mywloss: 0.5493 - val_loss: 0.7119 - val_mywloss: 0.7119\n",
      "Epoch 281/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5567 - mywloss: 0.5567 - val_loss: 0.7249 - val_mywloss: 0.7249\n",
      "Epoch 282/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5765 - mywloss: 0.5765 - val_loss: 0.7710 - val_mywloss: 0.7710\n",
      "Epoch 283/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5431 - mywloss: 0.5431 - val_loss: 0.7175 - val_mywloss: 0.7175\n",
      "Epoch 284/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5329 - mywloss: 0.5329 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 285/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5461 - mywloss: 0.5461 - val_loss: 0.7699 - val_mywloss: 0.7699\n",
      "Epoch 286/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5560 - mywloss: 0.5560 - val_loss: 0.7472 - val_mywloss: 0.7472\n",
      "Epoch 287/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5482 - mywloss: 0.5482 - val_loss: 0.7377 - val_mywloss: 0.7377\n",
      "Epoch 288/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5467 - mywloss: 0.5467 - val_loss: 0.7506 - val_mywloss: 0.7506\n",
      "Epoch 289/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5666 - mywloss: 0.5666 - val_loss: 0.7583 - val_mywloss: 0.7583\n",
      "Epoch 290/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5534 - mywloss: 0.5534 - val_loss: 0.7561 - val_mywloss: 0.7561\n",
      "Epoch 291/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5241 - mywloss: 0.5241 - val_loss: 0.7236 - val_mywloss: 0.7236\n",
      "Epoch 292/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5199 - mywloss: 0.5199 - val_loss: 0.7485 - val_mywloss: 0.7485\n",
      "Epoch 293/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5394 - mywloss: 0.5394 - val_loss: 0.7662 - val_mywloss: 0.7662\n",
      "Epoch 294/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5463 - mywloss: 0.5463 - val_loss: 0.7875 - val_mywloss: 0.7875\n",
      "Epoch 295/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5350 - mywloss: 0.5350 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 296/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5504 - mywloss: 0.5504 - val_loss: 0.7215 - val_mywloss: 0.7215\n",
      "Epoch 297/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5401 - mywloss: 0.5401 - val_loss: 0.7322 - val_mywloss: 0.7322\n",
      "Epoch 298/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5373 - mywloss: 0.5373 - val_loss: 0.7356 - val_mywloss: 0.7356\n",
      "Epoch 299/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5256 - mywloss: 0.5256 - val_loss: 0.7462 - val_mywloss: 0.7462\n",
      "Epoch 300/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5258 - mywloss: 0.5258 - val_loss: 0.7298 - val_mywloss: 0.7298\n",
      "Epoch 301/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5357 - mywloss: 0.5357 - val_loss: 0.7463 - val_mywloss: 0.7463\n",
      "Epoch 302/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5270 - mywloss: 0.5270 - val_loss: 0.7258 - val_mywloss: 0.7258\n",
      "Epoch 303/600\n",
      "6274/6274 [==============================] - 0s 63us/step - loss: 0.5462 - mywloss: 0.5462 - val_loss: 0.7479 - val_mywloss: 0.7479\n",
      "Epoch 304/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5243 - mywloss: 0.5243 - val_loss: 0.7389 - val_mywloss: 0.7389\n",
      "Epoch 305/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5494 - mywloss: 0.5494 - val_loss: 0.7091 - val_mywloss: 0.7091\n",
      "Epoch 306/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5235 - mywloss: 0.5235 - val_loss: 0.7244 - val_mywloss: 0.7244\n",
      "Epoch 307/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5309 - mywloss: 0.5309 - val_loss: 0.7433 - val_mywloss: 0.7433\n",
      "Epoch 308/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5350 - mywloss: 0.5350 - val_loss: 0.7042 - val_mywloss: 0.7042\n",
      "Epoch 309/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5306 - mywloss: 0.5306 - val_loss: 0.7037 - val_mywloss: 0.7037\n",
      "Epoch 310/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5135 - mywloss: 0.5135 - val_loss: 0.7311 - val_mywloss: 0.7311\n",
      "Epoch 311/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5312 - mywloss: 0.5312 - val_loss: 0.7457 - val_mywloss: 0.7457\n",
      "Epoch 312/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5477 - mywloss: 0.5477 - val_loss: 0.7268 - val_mywloss: 0.7268\n",
      "Epoch 313/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5291 - mywloss: 0.5291 - val_loss: 0.7477 - val_mywloss: 0.7477\n",
      "Epoch 314/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5306 - mywloss: 0.5306 - val_loss: 0.7093 - val_mywloss: 0.7093\n",
      "Epoch 315/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5376 - mywloss: 0.5376 - val_loss: 0.7422 - val_mywloss: 0.7422\n",
      "Epoch 316/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5206 - mywloss: 0.5206 - val_loss: 0.7199 - val_mywloss: 0.7199\n",
      "Epoch 317/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5203 - mywloss: 0.5203 - val_loss: 0.7511 - val_mywloss: 0.7511\n",
      "Epoch 318/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5355 - mywloss: 0.5355 - val_loss: 0.7339 - val_mywloss: 0.7339\n",
      "Epoch 319/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5287 - mywloss: 0.5287 - val_loss: 0.7554 - val_mywloss: 0.7554\n",
      "Epoch 320/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5249 - mywloss: 0.5249 - val_loss: 0.7474 - val_mywloss: 0.7474\n",
      "Epoch 321/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5361 - mywloss: 0.5361 - val_loss: 0.7543 - val_mywloss: 0.7543\n",
      "Epoch 322/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.5422 - mywloss: 0.5422 - val_loss: 0.7592 - val_mywloss: 0.7592\n",
      "Epoch 323/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5460 - mywloss: 0.5460 - val_loss: 0.7539 - val_mywloss: 0.7539\n",
      "Epoch 324/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5198 - mywloss: 0.5198 - val_loss: 0.7425 - val_mywloss: 0.7425\n",
      "Epoch 325/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5254 - mywloss: 0.5254 - val_loss: 0.7415 - val_mywloss: 0.7415\n",
      "Epoch 326/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5343 - mywloss: 0.5343 - val_loss: 0.7829 - val_mywloss: 0.7829\n",
      "Epoch 327/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5460 - mywloss: 0.5460 - val_loss: 0.7453 - val_mywloss: 0.7453\n",
      "Epoch 328/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5284 - mywloss: 0.5284 - val_loss: 0.7680 - val_mywloss: 0.7680\n",
      "Epoch 329/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5324 - mywloss: 0.5324 - val_loss: 0.7699 - val_mywloss: 0.7699\n",
      "Epoch 330/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5418 - mywloss: 0.5418 - val_loss: 0.7694 - val_mywloss: 0.7694\n",
      "Epoch 331/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5234 - mywloss: 0.5234 - val_loss: 0.7520 - val_mywloss: 0.7520\n",
      "Epoch 332/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5090 - mywloss: 0.5090 - val_loss: 0.7758 - val_mywloss: 0.7758\n",
      "Epoch 333/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5256 - mywloss: 0.5256 - val_loss: 0.7917 - val_mywloss: 0.7917\n",
      "Epoch 334/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5304 - mywloss: 0.5304 - val_loss: 0.8014 - val_mywloss: 0.8014\n",
      "Epoch 335/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5058 - mywloss: 0.5058 - val_loss: 0.7815 - val_mywloss: 0.7815\n",
      "Epoch 336/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5242 - mywloss: 0.5242 - val_loss: 0.7767 - val_mywloss: 0.7767\n",
      "Epoch 337/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5409 - mywloss: 0.5409 - val_loss: 0.7549 - val_mywloss: 0.7549\n",
      "Epoch 338/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5236 - mywloss: 0.5236 - val_loss: 0.7977 - val_mywloss: 0.7977\n",
      "Epoch 339/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5234 - mywloss: 0.5234 - val_loss: 0.8068 - val_mywloss: 0.8068\n",
      "Epoch 340/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5275 - mywloss: 0.5275 - val_loss: 0.7958 - val_mywloss: 0.7958\n",
      "Epoch 341/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5171 - mywloss: 0.5171 - val_loss: 0.7391 - val_mywloss: 0.7391\n",
      "Epoch 342/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5207 - mywloss: 0.5207 - val_loss: 0.7101 - val_mywloss: 0.7101\n",
      "Epoch 343/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5133 - mywloss: 0.5133 - val_loss: 0.7571 - val_mywloss: 0.7571\n",
      "Epoch 344/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5217 - mywloss: 0.5217 - val_loss: 0.7784 - val_mywloss: 0.7784\n",
      "Epoch 345/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5109 - mywloss: 0.5109 - val_loss: 0.7662 - val_mywloss: 0.7662\n",
      "Epoch 346/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5105 - mywloss: 0.5105 - val_loss: 0.7210 - val_mywloss: 0.7210\n",
      "Epoch 347/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5004 - mywloss: 0.5004 - val_loss: 0.7635 - val_mywloss: 0.7635\n",
      "Epoch 348/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5243 - mywloss: 0.5243 - val_loss: 0.7705 - val_mywloss: 0.7705\n",
      "Epoch 349/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5292 - mywloss: 0.5292 - val_loss: 0.7692 - val_mywloss: 0.7692\n",
      "Epoch 350/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5182 - mywloss: 0.5182 - val_loss: 0.8004 - val_mywloss: 0.8004\n",
      "Epoch 351/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5107 - mywloss: 0.5107 - val_loss: 0.7759 - val_mywloss: 0.7759\n",
      "Epoch 352/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5204 - mywloss: 0.5204 - val_loss: 0.7547 - val_mywloss: 0.7547\n",
      "Epoch 353/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5141 - mywloss: 0.5141 - val_loss: 0.7207 - val_mywloss: 0.7207\n",
      "Epoch 354/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5103 - mywloss: 0.5103 - val_loss: 0.7360 - val_mywloss: 0.7360\n",
      "Epoch 355/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5116 - mywloss: 0.5116 - val_loss: 0.7222 - val_mywloss: 0.7222\n",
      "Epoch 356/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5160 - mywloss: 0.5160 - val_loss: 0.7465 - val_mywloss: 0.7465\n",
      "Epoch 357/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5249 - mywloss: 0.5249 - val_loss: 0.7753 - val_mywloss: 0.7753\n",
      "Epoch 358/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5409 - mywloss: 0.5409 - val_loss: 0.7991 - val_mywloss: 0.7991\n",
      "Epoch 359/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5066 - mywloss: 0.5066 - val_loss: 0.8026 - val_mywloss: 0.8026\n",
      "Epoch 360/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.5107 - mywloss: 0.5107 - val_loss: 0.7752 - val_mywloss: 0.7752\n",
      "Epoch 361/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5289 - mywloss: 0.5289 - val_loss: 0.7769 - val_mywloss: 0.7769\n",
      "Epoch 362/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5156 - mywloss: 0.5156 - val_loss: 0.7575 - val_mywloss: 0.7575\n",
      "Epoch 363/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5141 - mywloss: 0.5141 - val_loss: 0.7640 - val_mywloss: 0.7640\n",
      "Epoch 364/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5264 - mywloss: 0.5264 - val_loss: 0.7697 - val_mywloss: 0.7697\n",
      "Epoch 365/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5157 - mywloss: 0.5157 - val_loss: 0.7404 - val_mywloss: 0.7404\n",
      "Epoch 366/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.5139 - mywloss: 0.5139 - val_loss: 0.7672 - val_mywloss: 0.7672\n",
      "Epoch 367/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5279 - mywloss: 0.5279 - val_loss: 0.7085 - val_mywloss: 0.7085\n",
      "Epoch 368/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5088 - mywloss: 0.5088 - val_loss: 0.7531 - val_mywloss: 0.7531\n",
      "Epoch 369/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5076 - mywloss: 0.5076 - val_loss: 0.7591 - val_mywloss: 0.7591\n",
      "Epoch 370/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5026 - mywloss: 0.5026 - val_loss: 0.7750 - val_mywloss: 0.7750\n",
      "Epoch 371/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5297 - mywloss: 0.5297 - val_loss: 0.7709 - val_mywloss: 0.7709\n",
      "Epoch 372/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5047 - mywloss: 0.5047 - val_loss: 0.7713 - val_mywloss: 0.7713\n",
      "Epoch 373/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5088 - mywloss: 0.5088 - val_loss: 0.7669 - val_mywloss: 0.7669\n",
      "Epoch 374/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5145 - mywloss: 0.5145 - val_loss: 0.7874 - val_mywloss: 0.7874\n",
      "Epoch 375/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5062 - mywloss: 0.5062 - val_loss: 0.8268 - val_mywloss: 0.8268\n",
      "Epoch 376/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5246 - mywloss: 0.5246 - val_loss: 0.8129 - val_mywloss: 0.8129\n",
      "Epoch 377/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5090 - mywloss: 0.5090 - val_loss: 0.7671 - val_mywloss: 0.7671\n",
      "Epoch 378/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5111 - mywloss: 0.5111 - val_loss: 0.7841 - val_mywloss: 0.7841\n",
      "Epoch 379/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4995 - mywloss: 0.4995 - val_loss: 0.7884 - val_mywloss: 0.7884\n",
      "Epoch 380/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5172 - mywloss: 0.5172 - val_loss: 0.7782 - val_mywloss: 0.7782\n",
      "Epoch 381/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5211 - mywloss: 0.5211 - val_loss: 0.7664 - val_mywloss: 0.7664\n",
      "Epoch 382/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4986 - mywloss: 0.4986 - val_loss: 0.7709 - val_mywloss: 0.7709\n",
      "Epoch 383/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5104 - mywloss: 0.5104 - val_loss: 0.7854 - val_mywloss: 0.7854\n",
      "Epoch 384/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5203 - mywloss: 0.5203 - val_loss: 0.7620 - val_mywloss: 0.7620\n",
      "Epoch 385/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5140 - mywloss: 0.5140 - val_loss: 0.7591 - val_mywloss: 0.7591\n",
      "Epoch 386/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4840 - mywloss: 0.4840 - val_loss: 0.7595 - val_mywloss: 0.7595\n",
      "Epoch 387/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5041 - mywloss: 0.5041 - val_loss: 0.7529 - val_mywloss: 0.7529\n",
      "Epoch 388/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5263 - mywloss: 0.5263 - val_loss: 0.7568 - val_mywloss: 0.7568\n",
      "Epoch 389/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5295 - mywloss: 0.5295 - val_loss: 0.7465 - val_mywloss: 0.7465\n",
      "Epoch 390/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5174 - mywloss: 0.5174 - val_loss: 0.7481 - val_mywloss: 0.7481\n",
      "Epoch 391/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5052 - mywloss: 0.5052 - val_loss: 0.7486 - val_mywloss: 0.7486\n",
      "Epoch 392/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5062 - mywloss: 0.5062 - val_loss: 0.7673 - val_mywloss: 0.7673\n",
      "Epoch 393/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5031 - mywloss: 0.5031 - val_loss: 0.7638 - val_mywloss: 0.7638\n",
      "Epoch 394/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4991 - mywloss: 0.4991 - val_loss: 0.7745 - val_mywloss: 0.7745\n",
      "Epoch 395/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4899 - mywloss: 0.4899 - val_loss: 0.7839 - val_mywloss: 0.7839\n",
      "Epoch 396/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4936 - mywloss: 0.4936 - val_loss: 0.7561 - val_mywloss: 0.7561\n",
      "Epoch 397/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4946 - mywloss: 0.4946 - val_loss: 0.7658 - val_mywloss: 0.7658\n",
      "Epoch 398/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4985 - mywloss: 0.4985 - val_loss: 0.8041 - val_mywloss: 0.8041\n",
      "Epoch 399/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5000 - mywloss: 0.5000 - val_loss: 0.7676 - val_mywloss: 0.7676\n",
      "Epoch 400/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5013 - mywloss: 0.5013 - val_loss: 0.7804 - val_mywloss: 0.7804\n",
      "Epoch 401/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4976 - mywloss: 0.4976 - val_loss: 0.8033 - val_mywloss: 0.8033\n",
      "Epoch 402/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4948 - mywloss: 0.4948 - val_loss: 0.8099 - val_mywloss: 0.8099\n",
      "Epoch 403/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.5075 - mywloss: 0.5075 - val_loss: 0.7792 - val_mywloss: 0.7792\n",
      "Epoch 404/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4934 - mywloss: 0.4934 - val_loss: 0.7545 - val_mywloss: 0.7545\n",
      "Epoch 405/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4924 - mywloss: 0.4924 - val_loss: 0.7858 - val_mywloss: 0.7858\n",
      "Epoch 406/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4960 - mywloss: 0.4960 - val_loss: 0.7688 - val_mywloss: 0.7688\n",
      "Epoch 407/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4987 - mywloss: 0.4987 - val_loss: 0.7701 - val_mywloss: 0.7701\n",
      "Epoch 408/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4923 - mywloss: 0.4923 - val_loss: 0.7786 - val_mywloss: 0.7786\n",
      "Epoch 409/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5037 - mywloss: 0.5037 - val_loss: 0.7821 - val_mywloss: 0.7821\n",
      "Epoch 410/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4957 - mywloss: 0.4957 - val_loss: 0.7605 - val_mywloss: 0.7605\n",
      "Epoch 411/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4835 - mywloss: 0.4835 - val_loss: 0.7686 - val_mywloss: 0.7686\n",
      "Epoch 412/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4819 - mywloss: 0.4819 - val_loss: 0.7731 - val_mywloss: 0.7731\n",
      "Epoch 413/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5016 - mywloss: 0.5016 - val_loss: 0.7792 - val_mywloss: 0.7792\n",
      "Epoch 414/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5086 - mywloss: 0.5086 - val_loss: 0.7575 - val_mywloss: 0.7575\n",
      "Epoch 415/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4946 - mywloss: 0.4946 - val_loss: 0.7697 - val_mywloss: 0.7697\n",
      "Epoch 416/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5127 - mywloss: 0.5127 - val_loss: 0.7755 - val_mywloss: 0.7755\n",
      "Epoch 417/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4800 - mywloss: 0.4800 - val_loss: 0.8057 - val_mywloss: 0.8057\n",
      "Epoch 418/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4876 - mywloss: 0.4876 - val_loss: 0.8264 - val_mywloss: 0.8264\n",
      "Epoch 419/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4967 - mywloss: 0.4967 - val_loss: 0.7512 - val_mywloss: 0.7512\n",
      "Epoch 420/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4824 - mywloss: 0.4824 - val_loss: 0.7485 - val_mywloss: 0.7485\n",
      "Epoch 421/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4877 - mywloss: 0.4877 - val_loss: 0.7835 - val_mywloss: 0.7835\n",
      "Epoch 422/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5017 - mywloss: 0.5017 - val_loss: 0.8015 - val_mywloss: 0.8015\n",
      "Epoch 423/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5113 - mywloss: 0.5113 - val_loss: 0.7889 - val_mywloss: 0.7889\n",
      "Epoch 424/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4802 - mywloss: 0.4802 - val_loss: 0.7824 - val_mywloss: 0.7824\n",
      "Epoch 425/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4797 - mywloss: 0.4797 - val_loss: 0.7818 - val_mywloss: 0.7818\n",
      "Epoch 426/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4692 - mywloss: 0.4692 - val_loss: 0.8222 - val_mywloss: 0.8222\n",
      "Epoch 427/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4962 - mywloss: 0.4962 - val_loss: 0.7794 - val_mywloss: 0.7794\n",
      "Epoch 428/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4715 - mywloss: 0.4715 - val_loss: 0.7773 - val_mywloss: 0.7773\n",
      "Epoch 429/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4942 - mywloss: 0.4942 - val_loss: 0.7798 - val_mywloss: 0.7798\n",
      "Epoch 430/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4769 - mywloss: 0.4769 - val_loss: 0.7965 - val_mywloss: 0.7965\n",
      "Epoch 431/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4748 - mywloss: 0.4748 - val_loss: 0.7959 - val_mywloss: 0.7959\n",
      "Epoch 432/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4789 - mywloss: 0.4789 - val_loss: 0.7732 - val_mywloss: 0.7732\n",
      "Epoch 433/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4837 - mywloss: 0.4837 - val_loss: 0.7812 - val_mywloss: 0.7812\n",
      "Epoch 434/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4844 - mywloss: 0.4844 - val_loss: 0.7689 - val_mywloss: 0.7689\n",
      "Epoch 435/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4843 - mywloss: 0.4843 - val_loss: 0.7672 - val_mywloss: 0.7672\n",
      "Epoch 436/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.4996 - mywloss: 0.4996 - val_loss: 0.7952 - val_mywloss: 0.7952\n",
      "Epoch 437/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4870 - mywloss: 0.4870 - val_loss: 0.8258 - val_mywloss: 0.8258\n",
      "Epoch 438/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.5088 - mywloss: 0.5088 - val_loss: 0.7830 - val_mywloss: 0.7830\n",
      "Epoch 439/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4756 - mywloss: 0.4756 - val_loss: 0.7657 - val_mywloss: 0.7657\n",
      "Epoch 440/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4774 - mywloss: 0.4774 - val_loss: 0.7759 - val_mywloss: 0.7759\n",
      "Epoch 441/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.5073 - mywloss: 0.5073 - val_loss: 0.7701 - val_mywloss: 0.7701\n",
      "Epoch 442/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4776 - mywloss: 0.4776 - val_loss: 0.7939 - val_mywloss: 0.7939\n",
      "Epoch 443/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4675 - mywloss: 0.4675 - val_loss: 0.8154 - val_mywloss: 0.8154\n",
      "Epoch 444/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4911 - mywloss: 0.4911 - val_loss: 0.7945 - val_mywloss: 0.7945\n",
      "Epoch 445/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4887 - mywloss: 0.4887 - val_loss: 0.7919 - val_mywloss: 0.7919\n",
      "Epoch 446/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4675 - mywloss: 0.4675 - val_loss: 0.7657 - val_mywloss: 0.7657\n",
      "Epoch 447/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4794 - mywloss: 0.4794 - val_loss: 0.7868 - val_mywloss: 0.7868\n",
      "Epoch 448/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4813 - mywloss: 0.4813 - val_loss: 0.7906 - val_mywloss: 0.7906\n",
      "Epoch 449/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4807 - mywloss: 0.4807 - val_loss: 0.7821 - val_mywloss: 0.7821\n",
      "Epoch 450/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4718 - mywloss: 0.4718 - val_loss: 0.7935 - val_mywloss: 0.7935\n",
      "Epoch 451/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.4952 - mywloss: 0.4952 - val_loss: 0.7816 - val_mywloss: 0.7816\n",
      "Epoch 452/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4618 - mywloss: 0.4618 - val_loss: 0.8065 - val_mywloss: 0.8065\n",
      "Epoch 453/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4734 - mywloss: 0.4734 - val_loss: 0.8077 - val_mywloss: 0.8077\n",
      "Epoch 454/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4890 - mywloss: 0.4890 - val_loss: 0.7841 - val_mywloss: 0.7841\n",
      "Epoch 455/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4683 - mywloss: 0.4683 - val_loss: 0.7950 - val_mywloss: 0.7950\n",
      "Epoch 456/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4747 - mywloss: 0.4747 - val_loss: 0.7584 - val_mywloss: 0.7584\n",
      "Epoch 457/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4774 - mywloss: 0.4774 - val_loss: 0.7463 - val_mywloss: 0.7463\n",
      "Epoch 458/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4743 - mywloss: 0.4743 - val_loss: 0.7950 - val_mywloss: 0.7950\n",
      "Epoch 459/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4788 - mywloss: 0.4788 - val_loss: 0.7958 - val_mywloss: 0.7958\n",
      "Epoch 460/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4687 - mywloss: 0.4687 - val_loss: 0.7662 - val_mywloss: 0.7662\n",
      "Epoch 461/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4611 - mywloss: 0.4611 - val_loss: 0.7538 - val_mywloss: 0.7538\n",
      "Epoch 462/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4823 - mywloss: 0.4823 - val_loss: 0.7622 - val_mywloss: 0.7622\n",
      "Epoch 463/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4723 - mywloss: 0.4723 - val_loss: 0.7688 - val_mywloss: 0.7688\n",
      "Epoch 464/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4784 - mywloss: 0.4784 - val_loss: 0.7530 - val_mywloss: 0.7530\n",
      "Epoch 465/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4798 - mywloss: 0.4798 - val_loss: 0.7515 - val_mywloss: 0.7515\n",
      "Epoch 466/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4913 - mywloss: 0.4913 - val_loss: 0.7676 - val_mywloss: 0.7676\n",
      "Epoch 467/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4825 - mywloss: 0.4825 - val_loss: 0.7565 - val_mywloss: 0.7565\n",
      "Epoch 468/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4798 - mywloss: 0.4798 - val_loss: 0.7932 - val_mywloss: 0.7932\n",
      "Epoch 469/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4672 - mywloss: 0.4672 - val_loss: 0.7923 - val_mywloss: 0.7923\n",
      "Epoch 470/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4861 - mywloss: 0.4861 - val_loss: 0.7991 - val_mywloss: 0.7991\n",
      "Epoch 471/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4677 - mywloss: 0.4677 - val_loss: 0.8198 - val_mywloss: 0.8198\n",
      "Epoch 472/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4687 - mywloss: 0.4687 - val_loss: 0.8185 - val_mywloss: 0.8185\n",
      "Epoch 473/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4806 - mywloss: 0.4806 - val_loss: 0.7940 - val_mywloss: 0.7940\n",
      "Epoch 474/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4660 - mywloss: 0.4660 - val_loss: 0.7715 - val_mywloss: 0.7715\n",
      "Epoch 475/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4713 - mywloss: 0.4713 - val_loss: 0.7805 - val_mywloss: 0.7805\n",
      "Epoch 476/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4748 - mywloss: 0.4748 - val_loss: 0.7900 - val_mywloss: 0.7900\n",
      "Epoch 477/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4569 - mywloss: 0.4569 - val_loss: 0.8033 - val_mywloss: 0.8033\n",
      "Epoch 478/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4638 - mywloss: 0.4638 - val_loss: 0.8019 - val_mywloss: 0.8019\n",
      "Epoch 479/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4704 - mywloss: 0.4704 - val_loss: 0.8016 - val_mywloss: 0.8016\n",
      "Epoch 480/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4749 - mywloss: 0.4749 - val_loss: 0.7822 - val_mywloss: 0.7822\n",
      "Epoch 481/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4786 - mywloss: 0.4786 - val_loss: 0.8004 - val_mywloss: 0.8004\n",
      "Epoch 482/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4686 - mywloss: 0.4686 - val_loss: 0.8108 - val_mywloss: 0.8108\n",
      "Epoch 483/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4689 - mywloss: 0.4689 - val_loss: 0.8321 - val_mywloss: 0.8321\n",
      "Epoch 484/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4435 - mywloss: 0.4435 - val_loss: 0.8102 - val_mywloss: 0.8102\n",
      "Epoch 485/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4703 - mywloss: 0.4703 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 486/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.8074 - val_mywloss: 0.8074\n",
      "Epoch 487/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4796 - mywloss: 0.4796 - val_loss: 0.7835 - val_mywloss: 0.7835\n",
      "Epoch 488/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4780 - mywloss: 0.4780 - val_loss: 0.8045 - val_mywloss: 0.8045\n",
      "Epoch 489/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4826 - mywloss: 0.4826 - val_loss: 0.7674 - val_mywloss: 0.7674\n",
      "Epoch 490/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4677 - mywloss: 0.4677 - val_loss: 0.7400 - val_mywloss: 0.7400\n",
      "Epoch 491/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4692 - mywloss: 0.4692 - val_loss: 0.7501 - val_mywloss: 0.7501\n",
      "Epoch 492/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4690 - mywloss: 0.4690 - val_loss: 0.7748 - val_mywloss: 0.7748\n",
      "Epoch 493/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4551 - mywloss: 0.4551 - val_loss: 0.8008 - val_mywloss: 0.8008\n",
      "Epoch 494/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4628 - mywloss: 0.4628 - val_loss: 0.7962 - val_mywloss: 0.7962\n",
      "Epoch 495/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.4546 - mywloss: 0.4546 - val_loss: 0.7869 - val_mywloss: 0.7869\n",
      "Epoch 496/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4635 - mywloss: 0.4635 - val_loss: 0.7883 - val_mywloss: 0.7883\n",
      "Epoch 497/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4597 - mywloss: 0.4597 - val_loss: 0.7670 - val_mywloss: 0.7670\n",
      "Epoch 498/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4643 - mywloss: 0.4643 - val_loss: 0.7755 - val_mywloss: 0.7755\n",
      "Epoch 499/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4525 - mywloss: 0.4525 - val_loss: 0.7831 - val_mywloss: 0.7831\n",
      "Epoch 500/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4517 - mywloss: 0.4517 - val_loss: 0.8098 - val_mywloss: 0.8098\n",
      "Epoch 501/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4519 - mywloss: 0.4519 - val_loss: 0.8103 - val_mywloss: 0.8103\n",
      "Epoch 502/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4609 - mywloss: 0.4609 - val_loss: 0.8109 - val_mywloss: 0.8109\n",
      "Epoch 503/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4587 - mywloss: 0.4587 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 504/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4572 - mywloss: 0.4572 - val_loss: 0.7991 - val_mywloss: 0.7991\n",
      "Epoch 505/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4847 - mywloss: 0.4847 - val_loss: 0.8099 - val_mywloss: 0.8099\n",
      "Epoch 506/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4555 - mywloss: 0.4555 - val_loss: 0.7933 - val_mywloss: 0.7933\n",
      "Epoch 507/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4685 - mywloss: 0.4685 - val_loss: 0.7806 - val_mywloss: 0.7806\n",
      "Epoch 508/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4769 - mywloss: 0.4769 - val_loss: 0.7687 - val_mywloss: 0.7687\n",
      "Epoch 509/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4569 - mywloss: 0.4569 - val_loss: 0.7928 - val_mywloss: 0.7928\n",
      "Epoch 510/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4615 - mywloss: 0.4615 - val_loss: 0.7587 - val_mywloss: 0.7587\n",
      "Epoch 511/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4614 - mywloss: 0.4614 - val_loss: 0.7923 - val_mywloss: 0.7923\n",
      "Epoch 512/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4581 - mywloss: 0.4581 - val_loss: 0.7887 - val_mywloss: 0.7887\n",
      "Epoch 513/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4618 - mywloss: 0.4618 - val_loss: 0.8031 - val_mywloss: 0.8031\n",
      "Epoch 514/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4505 - mywloss: 0.4505 - val_loss: 0.7888 - val_mywloss: 0.7888\n",
      "Epoch 515/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4495 - mywloss: 0.4495 - val_loss: 0.7997 - val_mywloss: 0.7997\n",
      "Epoch 516/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4579 - mywloss: 0.4579 - val_loss: 0.8012 - val_mywloss: 0.8012\n",
      "Epoch 517/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4400 - mywloss: 0.4400 - val_loss: 0.8539 - val_mywloss: 0.8539\n",
      "Epoch 518/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4639 - mywloss: 0.4639 - val_loss: 0.8587 - val_mywloss: 0.8587\n",
      "Epoch 519/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4457 - mywloss: 0.4457 - val_loss: 0.8505 - val_mywloss: 0.8505\n",
      "Epoch 520/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4613 - mywloss: 0.4613 - val_loss: 0.8131 - val_mywloss: 0.8131\n",
      "Epoch 521/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4744 - mywloss: 0.4744 - val_loss: 0.8265 - val_mywloss: 0.8265\n",
      "Epoch 522/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4721 - mywloss: 0.4721 - val_loss: 0.8386 - val_mywloss: 0.8386\n",
      "Epoch 523/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4501 - mywloss: 0.4501 - val_loss: 0.8293 - val_mywloss: 0.8293\n",
      "Epoch 524/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4484 - mywloss: 0.4484 - val_loss: 0.8249 - val_mywloss: 0.8249\n",
      "Epoch 525/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4667 - mywloss: 0.4667 - val_loss: 0.8368 - val_mywloss: 0.8368\n",
      "Epoch 526/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4708 - mywloss: 0.4708 - val_loss: 0.8287 - val_mywloss: 0.8287\n",
      "Epoch 527/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4735 - mywloss: 0.4735 - val_loss: 0.8319 - val_mywloss: 0.8319\n",
      "Epoch 528/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4550 - mywloss: 0.4550 - val_loss: 0.8431 - val_mywloss: 0.8431\n",
      "Epoch 529/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4621 - mywloss: 0.4621 - val_loss: 0.8212 - val_mywloss: 0.8212\n",
      "Epoch 530/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4791 - mywloss: 0.4791 - val_loss: 0.8401 - val_mywloss: 0.8401\n",
      "Epoch 531/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4630 - mywloss: 0.4630 - val_loss: 0.8572 - val_mywloss: 0.8572\n",
      "Epoch 532/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4377 - mywloss: 0.4377 - val_loss: 0.8246 - val_mywloss: 0.8246\n",
      "Epoch 533/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4657 - mywloss: 0.4657 - val_loss: 0.8646 - val_mywloss: 0.8646\n",
      "Epoch 534/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4510 - mywloss: 0.4510 - val_loss: 0.8560 - val_mywloss: 0.8560\n",
      "Epoch 535/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4587 - mywloss: 0.4587 - val_loss: 0.8958 - val_mywloss: 0.8958\n",
      "Epoch 536/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4663 - mywloss: 0.4663 - val_loss: 0.8764 - val_mywloss: 0.8764\n",
      "Epoch 537/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4493 - mywloss: 0.4493 - val_loss: 0.8636 - val_mywloss: 0.8636\n",
      "Epoch 538/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4538 - mywloss: 0.4538 - val_loss: 0.8432 - val_mywloss: 0.8432\n",
      "Epoch 539/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4540 - mywloss: 0.4540 - val_loss: 0.8354 - val_mywloss: 0.8354\n",
      "Epoch 540/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4334 - mywloss: 0.4334 - val_loss: 0.8502 - val_mywloss: 0.8502\n",
      "Epoch 541/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4360 - mywloss: 0.4360 - val_loss: 0.8822 - val_mywloss: 0.8822\n",
      "Epoch 542/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4468 - mywloss: 0.4468 - val_loss: 0.8897 - val_mywloss: 0.8898\n",
      "Epoch 543/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4566 - mywloss: 0.4566 - val_loss: 0.8489 - val_mywloss: 0.8489\n",
      "Epoch 544/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4470 - mywloss: 0.4470 - val_loss: 0.8479 - val_mywloss: 0.8479\n",
      "Epoch 545/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4290 - mywloss: 0.4290 - val_loss: 0.8757 - val_mywloss: 0.8757\n",
      "Epoch 546/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4517 - mywloss: 0.4517 - val_loss: 0.8551 - val_mywloss: 0.8551\n",
      "Epoch 547/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4685 - mywloss: 0.4685 - val_loss: 0.8592 - val_mywloss: 0.8592\n",
      "Epoch 548/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4418 - mywloss: 0.4418 - val_loss: 0.8049 - val_mywloss: 0.8049\n",
      "Epoch 549/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4452 - mywloss: 0.4452 - val_loss: 0.8173 - val_mywloss: 0.8173\n",
      "Epoch 550/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4389 - mywloss: 0.4389 - val_loss: 0.8070 - val_mywloss: 0.8070\n",
      "Epoch 551/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4473 - mywloss: 0.4473 - val_loss: 0.8230 - val_mywloss: 0.8230\n",
      "Epoch 552/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4591 - mywloss: 0.4591 - val_loss: 0.8273 - val_mywloss: 0.8273\n",
      "Epoch 553/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4587 - mywloss: 0.4587 - val_loss: 0.8568 - val_mywloss: 0.8568\n",
      "Epoch 554/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4504 - mywloss: 0.4504 - val_loss: 0.8255 - val_mywloss: 0.8255\n",
      "Epoch 555/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4532 - mywloss: 0.4532 - val_loss: 0.8463 - val_mywloss: 0.8463\n",
      "Epoch 556/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4618 - mywloss: 0.4618 - val_loss: 0.8720 - val_mywloss: 0.8720\n",
      "Epoch 557/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4498 - mywloss: 0.4498 - val_loss: 0.8860 - val_mywloss: 0.8860\n",
      "Epoch 558/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4252 - mywloss: 0.4252 - val_loss: 0.8674 - val_mywloss: 0.8674\n",
      "Epoch 559/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4378 - mywloss: 0.4378 - val_loss: 0.8591 - val_mywloss: 0.8591\n",
      "Epoch 560/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4495 - mywloss: 0.4495 - val_loss: 0.8542 - val_mywloss: 0.8542\n",
      "Epoch 561/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4472 - mywloss: 0.4472 - val_loss: 0.8556 - val_mywloss: 0.8556\n",
      "Epoch 562/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4542 - mywloss: 0.4542 - val_loss: 0.8504 - val_mywloss: 0.8504\n",
      "Epoch 563/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4461 - mywloss: 0.4461 - val_loss: 0.8138 - val_mywloss: 0.8138\n",
      "Epoch 564/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4592 - mywloss: 0.4592 - val_loss: 0.8521 - val_mywloss: 0.8521\n",
      "Epoch 565/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4322 - mywloss: 0.4322 - val_loss: 0.8628 - val_mywloss: 0.8628\n",
      "Epoch 566/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4477 - mywloss: 0.4477 - val_loss: 0.8520 - val_mywloss: 0.8520\n",
      "Epoch 567/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4324 - mywloss: 0.4324 - val_loss: 0.8396 - val_mywloss: 0.8396\n",
      "Epoch 568/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4374 - mywloss: 0.4374 - val_loss: 0.8514 - val_mywloss: 0.8514\n",
      "Epoch 569/600\n",
      "6274/6274 [==============================] - 0s 67us/step - loss: 0.4426 - mywloss: 0.4426 - val_loss: 0.8543 - val_mywloss: 0.8543\n",
      "Epoch 570/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4391 - mywloss: 0.4391 - val_loss: 0.8582 - val_mywloss: 0.8582\n",
      "Epoch 571/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4208 - mywloss: 0.4208 - val_loss: 0.8322 - val_mywloss: 0.8322\n",
      "Epoch 572/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4353 - mywloss: 0.4353 - val_loss: 0.8722 - val_mywloss: 0.8722\n",
      "Epoch 573/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4456 - mywloss: 0.4456 - val_loss: 0.8619 - val_mywloss: 0.8619\n",
      "Epoch 574/600\n",
      "6274/6274 [==============================] - 0s 68us/step - loss: 0.4459 - mywloss: 0.4459 - val_loss: 0.8756 - val_mywloss: 0.8756\n",
      "Epoch 575/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4663 - mywloss: 0.4663 - val_loss: 0.8724 - val_mywloss: 0.8724\n",
      "Epoch 576/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4381 - mywloss: 0.4381 - val_loss: 0.8432 - val_mywloss: 0.8432\n",
      "Epoch 577/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4384 - mywloss: 0.4384 - val_loss: 0.8526 - val_mywloss: 0.8526\n",
      "Epoch 578/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4447 - mywloss: 0.4447 - val_loss: 0.8310 - val_mywloss: 0.8310\n",
      "Epoch 579/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4787 - mywloss: 0.4787 - val_loss: 0.8488 - val_mywloss: 0.8488\n",
      "Epoch 580/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4403 - mywloss: 0.4403 - val_loss: 0.8284 - val_mywloss: 0.8284\n",
      "Epoch 581/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4473 - mywloss: 0.4473 - val_loss: 0.8140 - val_mywloss: 0.8140\n",
      "Epoch 582/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4414 - mywloss: 0.4414 - val_loss: 0.8264 - val_mywloss: 0.8264\n",
      "Epoch 583/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4352 - mywloss: 0.4352 - val_loss: 0.8194 - val_mywloss: 0.8194\n",
      "Epoch 584/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4425 - mywloss: 0.4425 - val_loss: 0.8520 - val_mywloss: 0.8520\n",
      "Epoch 585/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4651 - mywloss: 0.4651 - val_loss: 0.8819 - val_mywloss: 0.8819\n",
      "Epoch 586/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4431 - mywloss: 0.4431 - val_loss: 0.8584 - val_mywloss: 0.8584\n",
      "Epoch 587/600\n",
      "6274/6274 [==============================] - 0s 64us/step - loss: 0.4368 - mywloss: 0.4368 - val_loss: 0.8487 - val_mywloss: 0.8487\n",
      "Epoch 588/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4486 - mywloss: 0.4486 - val_loss: 0.8253 - val_mywloss: 0.8253\n",
      "Epoch 589/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4538 - mywloss: 0.4538 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 590/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4486 - mywloss: 0.4486 - val_loss: 0.8437 - val_mywloss: 0.8437\n",
      "Epoch 591/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4569 - mywloss: 0.4569 - val_loss: 0.8110 - val_mywloss: 0.8110\n",
      "Epoch 592/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4413 - mywloss: 0.4413 - val_loss: 0.8128 - val_mywloss: 0.8128\n",
      "Epoch 593/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4382 - mywloss: 0.4382 - val_loss: 0.8111 - val_mywloss: 0.8111\n",
      "Epoch 594/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4329 - mywloss: 0.4329 - val_loss: 0.8273 - val_mywloss: 0.8273\n",
      "Epoch 595/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4443 - mywloss: 0.4443 - val_loss: 0.8588 - val_mywloss: 0.8588\n",
      "Epoch 596/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4312 - mywloss: 0.4312 - val_loss: 0.8643 - val_mywloss: 0.8643\n",
      "Epoch 597/600\n",
      "6274/6274 [==============================] - 0s 69us/step - loss: 0.4647 - mywloss: 0.4647 - val_loss: 0.8223 - val_mywloss: 0.8223\n",
      "Epoch 598/600\n",
      "6274/6274 [==============================] - 0s 66us/step - loss: 0.4307 - mywloss: 0.4307 - val_loss: 0.8197 - val_mywloss: 0.8197\n",
      "Epoch 599/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4318 - mywloss: 0.4318 - val_loss: 0.8629 - val_mywloss: 0.8629\n",
      "Epoch 600/600\n",
      "6274/6274 [==============================] - 0s 65us/step - loss: 0.4449 - mywloss: 0.4449 - val_loss: 0.8669 - val_mywloss: 0.8669\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc0cba2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model\n",
      "0.6713706710059744\n",
      "Train on 6276 samples, validate on 1572 samples\n",
      "Epoch 1/600\n",
      "6276/6276 [==============================] - 1s 234us/step - loss: 2.4746 - mywloss: 2.4746 - val_loss: 1.3599 - val_mywloss: 1.3599\n",
      "Epoch 2/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.8401 - mywloss: 1.8401 - val_loss: 1.1936 - val_mywloss: 1.1936\n",
      "Epoch 3/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.6490 - mywloss: 1.6490 - val_loss: 1.1020 - val_mywloss: 1.1020\n",
      "Epoch 4/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.5143 - mywloss: 1.5143 - val_loss: 1.0778 - val_mywloss: 1.0778\n",
      "Epoch 5/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.4465 - mywloss: 1.4465 - val_loss: 1.0035 - val_mywloss: 1.0035\n",
      "Epoch 6/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.3478 - mywloss: 1.3478 - val_loss: 0.9878 - val_mywloss: 0.9878\n",
      "Epoch 7/600\n",
      "6276/6276 [==============================] - 1s 212us/step - loss: 1.2691 - mywloss: 1.2691 - val_loss: 0.9608 - val_mywloss: 0.9608\n",
      "Epoch 8/600\n",
      "6276/6276 [==============================] - 1s 147us/step - loss: 1.2570 - mywloss: 1.2570 - val_loss: 0.9682 - val_mywloss: 0.9682\n",
      "Epoch 9/600\n",
      "6276/6276 [==============================] - 1s 146us/step - loss: 1.1887 - mywloss: 1.1887 - val_loss: 0.9287 - val_mywloss: 0.9287\n",
      "Epoch 10/600\n",
      "6276/6276 [==============================] - 1s 146us/step - loss: 1.1628 - mywloss: 1.1628 - val_loss: 0.9085 - val_mywloss: 0.9085\n",
      "Epoch 11/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 1.1402 - mywloss: 1.1402 - val_loss: 0.9029 - val_mywloss: 0.9029\n",
      "Epoch 12/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 1.0998 - mywloss: 1.0998 - val_loss: 0.8938 - val_mywloss: 0.8938\n",
      "Epoch 13/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 1.0892 - mywloss: 1.0892 - val_loss: 0.8854 - val_mywloss: 0.8854\n",
      "Epoch 14/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 1.0775 - mywloss: 1.0775 - val_loss: 0.9140 - val_mywloss: 0.9140\n",
      "Epoch 15/600\n",
      "6276/6276 [==============================] - 0s 75us/step - loss: 1.0430 - mywloss: 1.0430 - val_loss: 0.8855 - val_mywloss: 0.8855\n",
      "Epoch 16/600\n",
      "6276/6276 [==============================] - 0s 73us/step - loss: 1.0322 - mywloss: 1.0322 - val_loss: 0.8755 - val_mywloss: 0.8755\n",
      "Epoch 17/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 1.0209 - mywloss: 1.0209 - val_loss: 0.8507 - val_mywloss: 0.8507\n",
      "Epoch 18/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.9941 - mywloss: 0.9941 - val_loss: 0.8277 - val_mywloss: 0.8277\n",
      "Epoch 19/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9716 - mywloss: 0.9716 - val_loss: 0.8395 - val_mywloss: 0.8395\n",
      "Epoch 20/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9733 - mywloss: 0.9733 - val_loss: 0.8625 - val_mywloss: 0.8625\n",
      "Epoch 21/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9645 - mywloss: 0.9645 - val_loss: 0.8431 - val_mywloss: 0.8431\n",
      "Epoch 22/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9657 - mywloss: 0.9657 - val_loss: 0.8126 - val_mywloss: 0.8126\n",
      "Epoch 23/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9353 - mywloss: 0.9353 - val_loss: 0.8302 - val_mywloss: 0.8302\n",
      "Epoch 24/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9229 - mywloss: 0.9229 - val_loss: 0.8346 - val_mywloss: 0.8346\n",
      "Epoch 25/600\n",
      "6276/6276 [==============================] - 0s 63us/step - loss: 0.9329 - mywloss: 0.9329 - val_loss: 0.8340 - val_mywloss: 0.8340\n",
      "Epoch 26/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9402 - mywloss: 0.9402 - val_loss: 0.8266 - val_mywloss: 0.8266\n",
      "Epoch 27/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9125 - mywloss: 0.9125 - val_loss: 0.8633 - val_mywloss: 0.8633\n",
      "Epoch 28/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9096 - mywloss: 0.9096 - val_loss: 0.7980 - val_mywloss: 0.7980\n",
      "Epoch 29/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.9198 - mywloss: 0.9198 - val_loss: 0.8117 - val_mywloss: 0.8117\n",
      "Epoch 30/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.9132 - mywloss: 0.9132 - val_loss: 0.7908 - val_mywloss: 0.7908\n",
      "Epoch 31/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8801 - mywloss: 0.8801 - val_loss: 0.7874 - val_mywloss: 0.7874\n",
      "Epoch 32/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8768 - mywloss: 0.8768 - val_loss: 0.7905 - val_mywloss: 0.7905\n",
      "Epoch 33/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8607 - mywloss: 0.8607 - val_loss: 0.7852 - val_mywloss: 0.7852\n",
      "Epoch 34/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8779 - mywloss: 0.8779 - val_loss: 0.7966 - val_mywloss: 0.7966\n",
      "Epoch 35/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8884 - mywloss: 0.8884 - val_loss: 0.7884 - val_mywloss: 0.7884\n",
      "Epoch 36/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8409 - mywloss: 0.8409 - val_loss: 0.8026 - val_mywloss: 0.8026\n",
      "Epoch 37/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8581 - mywloss: 0.8581 - val_loss: 0.8017 - val_mywloss: 0.8017\n",
      "Epoch 38/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8535 - mywloss: 0.8535 - val_loss: 0.8011 - val_mywloss: 0.8011\n",
      "Epoch 39/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8511 - mywloss: 0.8511 - val_loss: 0.7778 - val_mywloss: 0.7778\n",
      "Epoch 40/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8500 - mywloss: 0.8500 - val_loss: 0.7838 - val_mywloss: 0.7838\n",
      "Epoch 41/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8484 - mywloss: 0.8484 - val_loss: 0.7883 - val_mywloss: 0.7883\n",
      "Epoch 42/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8486 - mywloss: 0.8486 - val_loss: 0.7684 - val_mywloss: 0.7684\n",
      "Epoch 43/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8175 - mywloss: 0.8175 - val_loss: 0.7605 - val_mywloss: 0.7605\n",
      "Epoch 44/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8537 - mywloss: 0.8537 - val_loss: 0.7842 - val_mywloss: 0.7842\n",
      "Epoch 45/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8218 - mywloss: 0.8218 - val_loss: 0.7672 - val_mywloss: 0.7672\n",
      "Epoch 46/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8153 - mywloss: 0.8153 - val_loss: 0.7693 - val_mywloss: 0.7693\n",
      "Epoch 47/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8249 - mywloss: 0.8249 - val_loss: 0.7496 - val_mywloss: 0.7496\n",
      "Epoch 48/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8326 - mywloss: 0.8326 - val_loss: 0.7558 - val_mywloss: 0.7558\n",
      "Epoch 49/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8007 - mywloss: 0.8007 - val_loss: 0.7600 - val_mywloss: 0.7600\n",
      "Epoch 50/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8192 - mywloss: 0.8192 - val_loss: 0.7662 - val_mywloss: 0.7662\n",
      "Epoch 51/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8007 - mywloss: 0.8007 - val_loss: 0.7683 - val_mywloss: 0.7683\n",
      "Epoch 52/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8116 - mywloss: 0.8116 - val_loss: 0.7506 - val_mywloss: 0.7506\n",
      "Epoch 53/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.8200 - mywloss: 0.8200 - val_loss: 0.7480 - val_mywloss: 0.7480\n",
      "Epoch 54/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7836 - mywloss: 0.7836 - val_loss: 0.7302 - val_mywloss: 0.7302\n",
      "Epoch 55/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.8000 - mywloss: 0.8000 - val_loss: 0.7470 - val_mywloss: 0.7470\n",
      "Epoch 56/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7745 - mywloss: 0.7745 - val_loss: 0.7468 - val_mywloss: 0.7468\n",
      "Epoch 57/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.7949 - mywloss: 0.7949 - val_loss: 0.7304 - val_mywloss: 0.7304\n",
      "Epoch 58/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7560 - mywloss: 0.7560 - val_loss: 0.7344 - val_mywloss: 0.7344\n",
      "Epoch 59/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7776 - mywloss: 0.7776 - val_loss: 0.7567 - val_mywloss: 0.7567\n",
      "Epoch 60/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7747 - mywloss: 0.7747 - val_loss: 0.7377 - val_mywloss: 0.7377\n",
      "Epoch 61/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.7714 - mywloss: 0.7714 - val_loss: 0.7256 - val_mywloss: 0.7256\n",
      "Epoch 62/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7923 - mywloss: 0.7923 - val_loss: 0.7482 - val_mywloss: 0.7482\n",
      "Epoch 63/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7866 - mywloss: 0.7866 - val_loss: 0.7375 - val_mywloss: 0.7375\n",
      "Epoch 64/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7610 - mywloss: 0.7610 - val_loss: 0.7434 - val_mywloss: 0.7434\n",
      "Epoch 65/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7584 - mywloss: 0.7584 - val_loss: 0.7370 - val_mywloss: 0.7370\n",
      "Epoch 66/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7680 - mywloss: 0.7680 - val_loss: 0.7552 - val_mywloss: 0.7552\n",
      "Epoch 67/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7589 - mywloss: 0.7589 - val_loss: 0.7699 - val_mywloss: 0.7699\n",
      "Epoch 68/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7615 - mywloss: 0.7615 - val_loss: 0.7356 - val_mywloss: 0.7356\n",
      "Epoch 69/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7881 - mywloss: 0.7881 - val_loss: 0.7274 - val_mywloss: 0.7274\n",
      "Epoch 70/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7598 - mywloss: 0.7598 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 71/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7510 - mywloss: 0.7510 - val_loss: 0.7149 - val_mywloss: 0.7149\n",
      "Epoch 72/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7306 - mywloss: 0.7306 - val_loss: 0.7097 - val_mywloss: 0.7097\n",
      "Epoch 73/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7375 - mywloss: 0.7375 - val_loss: 0.7062 - val_mywloss: 0.7062\n",
      "Epoch 74/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7498 - mywloss: 0.7498 - val_loss: 0.7188 - val_mywloss: 0.7188\n",
      "Epoch 75/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7526 - mywloss: 0.7526 - val_loss: 0.7157 - val_mywloss: 0.7157\n",
      "Epoch 76/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7374 - mywloss: 0.7374 - val_loss: 0.7271 - val_mywloss: 0.7271\n",
      "Epoch 77/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7361 - mywloss: 0.7361 - val_loss: 0.7310 - val_mywloss: 0.7310\n",
      "Epoch 78/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7259 - mywloss: 0.7259 - val_loss: 0.6986 - val_mywloss: 0.6986\n",
      "Epoch 79/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.7298 - mywloss: 0.7298 - val_loss: 0.7061 - val_mywloss: 0.7061\n",
      "Epoch 80/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7333 - mywloss: 0.7333 - val_loss: 0.7245 - val_mywloss: 0.7245\n",
      "Epoch 81/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7232 - mywloss: 0.7232 - val_loss: 0.7128 - val_mywloss: 0.7128\n",
      "Epoch 82/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.7203 - mywloss: 0.7203 - val_loss: 0.7132 - val_mywloss: 0.7132\n",
      "Epoch 83/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7260 - mywloss: 0.7260 - val_loss: 0.7027 - val_mywloss: 0.7027\n",
      "Epoch 84/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7052 - mywloss: 0.7052 - val_loss: 0.6981 - val_mywloss: 0.6981\n",
      "Epoch 85/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7331 - mywloss: 0.7331 - val_loss: 0.7039 - val_mywloss: 0.7039\n",
      "Epoch 86/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7281 - mywloss: 0.7281 - val_loss: 0.7044 - val_mywloss: 0.7044\n",
      "Epoch 87/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7015 - mywloss: 0.7015 - val_loss: 0.7114 - val_mywloss: 0.7114\n",
      "Epoch 88/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7004 - mywloss: 0.7004 - val_loss: 0.6843 - val_mywloss: 0.6843\n",
      "Epoch 89/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7163 - mywloss: 0.7163 - val_loss: 0.7074 - val_mywloss: 0.7074\n",
      "Epoch 90/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6963 - mywloss: 0.6963 - val_loss: 0.7034 - val_mywloss: 0.7034\n",
      "Epoch 91/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6922 - mywloss: 0.6922 - val_loss: 0.7055 - val_mywloss: 0.7055\n",
      "Epoch 92/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7047 - mywloss: 0.7047 - val_loss: 0.7079 - val_mywloss: 0.7079\n",
      "Epoch 93/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7139 - mywloss: 0.7139 - val_loss: 0.6986 - val_mywloss: 0.6986\n",
      "Epoch 94/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7303 - mywloss: 0.7303 - val_loss: 0.7086 - val_mywloss: 0.7086\n",
      "Epoch 95/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6916 - mywloss: 0.6916 - val_loss: 0.6853 - val_mywloss: 0.6853\n",
      "Epoch 96/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6854 - mywloss: 0.6854 - val_loss: 0.6945 - val_mywloss: 0.6945\n",
      "Epoch 97/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6981 - mywloss: 0.6981 - val_loss: 0.7237 - val_mywloss: 0.7237\n",
      "Epoch 98/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6984 - mywloss: 0.6984 - val_loss: 0.7236 - val_mywloss: 0.7236\n",
      "Epoch 99/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6968 - mywloss: 0.6968 - val_loss: 0.7027 - val_mywloss: 0.7027\n",
      "Epoch 100/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6959 - mywloss: 0.6959 - val_loss: 0.6942 - val_mywloss: 0.6942\n",
      "Epoch 101/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6834 - mywloss: 0.6834 - val_loss: 0.7258 - val_mywloss: 0.7258\n",
      "Epoch 102/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7179 - mywloss: 0.7179 - val_loss: 0.7135 - val_mywloss: 0.7135\n",
      "Epoch 103/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6875 - mywloss: 0.6875 - val_loss: 0.7058 - val_mywloss: 0.7058\n",
      "Epoch 104/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.7004 - mywloss: 0.7004 - val_loss: 0.7005 - val_mywloss: 0.7005\n",
      "Epoch 105/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6849 - mywloss: 0.6849 - val_loss: 0.6951 - val_mywloss: 0.6951\n",
      "Epoch 106/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7087 - mywloss: 0.7087 - val_loss: 0.6931 - val_mywloss: 0.6931\n",
      "Epoch 107/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.7017 - mywloss: 0.7017 - val_loss: 0.6939 - val_mywloss: 0.6939\n",
      "Epoch 108/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6695 - mywloss: 0.6695 - val_loss: 0.7039 - val_mywloss: 0.7039\n",
      "Epoch 109/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6930 - mywloss: 0.6930 - val_loss: 0.7099 - val_mywloss: 0.7099\n",
      "Epoch 110/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6655 - mywloss: 0.6655 - val_loss: 0.7384 - val_mywloss: 0.7384\n",
      "Epoch 111/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6821 - mywloss: 0.6821 - val_loss: 0.7141 - val_mywloss: 0.7141\n",
      "Epoch 112/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6756 - mywloss: 0.6756 - val_loss: 0.7269 - val_mywloss: 0.7269\n",
      "Epoch 113/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6595 - mywloss: 0.6595 - val_loss: 0.7082 - val_mywloss: 0.7082\n",
      "Epoch 114/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6721 - mywloss: 0.6721 - val_loss: 0.7208 - val_mywloss: 0.7208\n",
      "Epoch 115/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6772 - mywloss: 0.6772 - val_loss: 0.7069 - val_mywloss: 0.7069\n",
      "Epoch 116/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6878 - mywloss: 0.6878 - val_loss: 0.7066 - val_mywloss: 0.7066\n",
      "Epoch 117/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6461 - mywloss: 0.6461 - val_loss: 0.7234 - val_mywloss: 0.7234\n",
      "Epoch 118/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6616 - mywloss: 0.6616 - val_loss: 0.7318 - val_mywloss: 0.7318\n",
      "Epoch 119/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6574 - mywloss: 0.6574 - val_loss: 0.7228 - val_mywloss: 0.7228\n",
      "Epoch 120/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6619 - mywloss: 0.6619 - val_loss: 0.7220 - val_mywloss: 0.7220\n",
      "Epoch 121/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6523 - mywloss: 0.6523 - val_loss: 0.7001 - val_mywloss: 0.7001\n",
      "Epoch 122/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6491 - mywloss: 0.6491 - val_loss: 0.6980 - val_mywloss: 0.6980\n",
      "Epoch 123/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6721 - mywloss: 0.6721 - val_loss: 0.7231 - val_mywloss: 0.7231\n",
      "Epoch 124/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6525 - mywloss: 0.6525 - val_loss: 0.7146 - val_mywloss: 0.7146\n",
      "Epoch 125/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6785 - mywloss: 0.6785 - val_loss: 0.7318 - val_mywloss: 0.7318\n",
      "Epoch 126/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6524 - mywloss: 0.6524 - val_loss: 0.7371 - val_mywloss: 0.7371\n",
      "Epoch 127/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6514 - mywloss: 0.6514 - val_loss: 0.7116 - val_mywloss: 0.7116\n",
      "Epoch 128/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6552 - mywloss: 0.6552 - val_loss: 0.6907 - val_mywloss: 0.6907\n",
      "Epoch 129/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6459 - mywloss: 0.6459 - val_loss: 0.7418 - val_mywloss: 0.7418\n",
      "Epoch 130/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6370 - mywloss: 0.6370 - val_loss: 0.7220 - val_mywloss: 0.7220\n",
      "Epoch 131/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6651 - mywloss: 0.6651 - val_loss: 0.6967 - val_mywloss: 0.6967\n",
      "Epoch 132/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6376 - mywloss: 0.6376 - val_loss: 0.7654 - val_mywloss: 0.7654\n",
      "Epoch 133/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6447 - mywloss: 0.6447 - val_loss: 0.7409 - val_mywloss: 0.7409\n",
      "Epoch 134/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6442 - mywloss: 0.6442 - val_loss: 0.7106 - val_mywloss: 0.7106\n",
      "Epoch 135/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6416 - mywloss: 0.6416 - val_loss: 0.7460 - val_mywloss: 0.7460\n",
      "Epoch 136/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6553 - mywloss: 0.6553 - val_loss: 0.7526 - val_mywloss: 0.7526\n",
      "Epoch 137/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6538 - mywloss: 0.6538 - val_loss: 0.7881 - val_mywloss: 0.7881\n",
      "Epoch 138/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6606 - mywloss: 0.6606 - val_loss: 0.7570 - val_mywloss: 0.7570\n",
      "Epoch 139/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6378 - mywloss: 0.6378 - val_loss: 0.7591 - val_mywloss: 0.7591\n",
      "Epoch 140/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6702 - mywloss: 0.6702 - val_loss: 0.7747 - val_mywloss: 0.7747\n",
      "Epoch 141/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6532 - mywloss: 0.6532 - val_loss: 0.7332 - val_mywloss: 0.7332\n",
      "Epoch 142/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6508 - mywloss: 0.6508 - val_loss: 0.7502 - val_mywloss: 0.7502\n",
      "Epoch 143/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6296 - mywloss: 0.6296 - val_loss: 0.7522 - val_mywloss: 0.7522\n",
      "Epoch 144/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6431 - mywloss: 0.6431 - val_loss: 0.7216 - val_mywloss: 0.7216\n",
      "Epoch 145/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6336 - mywloss: 0.6336 - val_loss: 0.6994 - val_mywloss: 0.6994\n",
      "Epoch 146/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6425 - mywloss: 0.6425 - val_loss: 0.7033 - val_mywloss: 0.7033\n",
      "Epoch 147/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6259 - mywloss: 0.6259 - val_loss: 0.7602 - val_mywloss: 0.7602\n",
      "Epoch 148/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6522 - mywloss: 0.6522 - val_loss: 0.7193 - val_mywloss: 0.7193\n",
      "Epoch 149/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.6257 - mywloss: 0.6257 - val_loss: 0.7044 - val_mywloss: 0.7044\n",
      "Epoch 150/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6138 - mywloss: 0.6138 - val_loss: 0.7217 - val_mywloss: 0.7217\n",
      "Epoch 151/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6471 - mywloss: 0.6471 - val_loss: 0.7216 - val_mywloss: 0.7216\n",
      "Epoch 152/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.6446 - mywloss: 0.6446 - val_loss: 0.7046 - val_mywloss: 0.7046\n",
      "Epoch 153/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6349 - mywloss: 0.6349 - val_loss: 0.7149 - val_mywloss: 0.7149\n",
      "Epoch 154/600\n",
      "6276/6276 [==============================] - 0s 74us/step - loss: 0.5970 - mywloss: 0.5970 - val_loss: 0.7492 - val_mywloss: 0.7492\n",
      "Epoch 155/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 0.6404 - mywloss: 0.6404 - val_loss: 0.7359 - val_mywloss: 0.7359\n",
      "Epoch 156/600\n",
      "6276/6276 [==============================] - 0s 73us/step - loss: 0.6522 - mywloss: 0.6522 - val_loss: 0.7099 - val_mywloss: 0.7099\n",
      "Epoch 157/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.6247 - mywloss: 0.6247 - val_loss: 0.7150 - val_mywloss: 0.7150\n",
      "Epoch 158/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.6254 - mywloss: 0.6254 - val_loss: 0.7104 - val_mywloss: 0.7104\n",
      "Epoch 159/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.6449 - mywloss: 0.6449 - val_loss: 0.7144 - val_mywloss: 0.7144\n",
      "Epoch 160/600\n",
      "6276/6276 [==============================] - 0s 79us/step - loss: 0.6406 - mywloss: 0.6406 - val_loss: 0.7695 - val_mywloss: 0.7695\n",
      "Epoch 161/600\n",
      "6276/6276 [==============================] - 0s 74us/step - loss: 0.6404 - mywloss: 0.6404 - val_loss: 0.7669 - val_mywloss: 0.7669\n",
      "Epoch 162/600\n",
      "6276/6276 [==============================] - 0s 77us/step - loss: 0.6124 - mywloss: 0.6124 - val_loss: 0.7833 - val_mywloss: 0.7833\n",
      "Epoch 163/600\n",
      "6276/6276 [==============================] - 0s 73us/step - loss: 0.6127 - mywloss: 0.6127 - val_loss: 0.7649 - val_mywloss: 0.7649\n",
      "Epoch 164/600\n",
      "6276/6276 [==============================] - 0s 71us/step - loss: 0.6155 - mywloss: 0.6155 - val_loss: 0.7454 - val_mywloss: 0.7454\n",
      "Epoch 165/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.6161 - mywloss: 0.6161 - val_loss: 0.7535 - val_mywloss: 0.7535\n",
      "Epoch 166/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.6177 - mywloss: 0.6177 - val_loss: 0.7374 - val_mywloss: 0.7374\n",
      "Epoch 167/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6183 - mywloss: 0.6183 - val_loss: 0.7212 - val_mywloss: 0.7212\n",
      "Epoch 168/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6190 - mywloss: 0.6190 - val_loss: 0.6980 - val_mywloss: 0.6980\n",
      "Epoch 169/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6247 - mywloss: 0.6247 - val_loss: 0.6873 - val_mywloss: 0.6873\n",
      "Epoch 170/600\n",
      "6276/6276 [==============================] - 0s 71us/step - loss: 0.5977 - mywloss: 0.5977 - val_loss: 0.7065 - val_mywloss: 0.7065\n",
      "Epoch 171/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6415 - mywloss: 0.6415 - val_loss: 0.7349 - val_mywloss: 0.7349\n",
      "Epoch 172/600\n",
      "6276/6276 [==============================] - 0s 71us/step - loss: 0.6442 - mywloss: 0.6442 - val_loss: 0.6899 - val_mywloss: 0.6899\n",
      "Epoch 173/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6053 - mywloss: 0.6053 - val_loss: 0.6975 - val_mywloss: 0.6975\n",
      "Epoch 174/600\n",
      "6276/6276 [==============================] - 0s 71us/step - loss: 0.6111 - mywloss: 0.6111 - val_loss: 0.7197 - val_mywloss: 0.7197\n",
      "Epoch 175/600\n",
      "6276/6276 [==============================] - 0s 71us/step - loss: 0.6098 - mywloss: 0.6098 - val_loss: 0.7646 - val_mywloss: 0.7646\n",
      "Epoch 176/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.6279 - mywloss: 0.6279 - val_loss: 0.7602 - val_mywloss: 0.7602\n",
      "Epoch 177/600\n",
      "6276/6276 [==============================] - 0s 75us/step - loss: 0.6129 - mywloss: 0.6129 - val_loss: 0.7611 - val_mywloss: 0.7611\n",
      "Epoch 178/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 0.6262 - mywloss: 0.6262 - val_loss: 0.7467 - val_mywloss: 0.7467\n",
      "Epoch 179/600\n",
      "6276/6276 [==============================] - 0s 76us/step - loss: 0.6094 - mywloss: 0.6094 - val_loss: 0.7746 - val_mywloss: 0.7746\n",
      "Epoch 180/600\n",
      "6276/6276 [==============================] - 0s 77us/step - loss: 0.6206 - mywloss: 0.6206 - val_loss: 0.7532 - val_mywloss: 0.7532\n",
      "Epoch 181/600\n",
      "6276/6276 [==============================] - 0s 75us/step - loss: 0.5928 - mywloss: 0.5928 - val_loss: 0.7582 - val_mywloss: 0.7582\n",
      "Epoch 182/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.6120 - mywloss: 0.6120 - val_loss: 0.7399 - val_mywloss: 0.7399\n",
      "Epoch 183/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.5972 - mywloss: 0.5972 - val_loss: 0.7410 - val_mywloss: 0.7410\n",
      "Epoch 184/600\n",
      "6276/6276 [==============================] - 0s 75us/step - loss: 0.6170 - mywloss: 0.6170 - val_loss: 0.7519 - val_mywloss: 0.7519\n",
      "Epoch 185/600\n",
      "6276/6276 [==============================] - 1s 81us/step - loss: 0.6086 - mywloss: 0.6086 - val_loss: 0.7722 - val_mywloss: 0.7722\n",
      "Epoch 186/600\n",
      "6276/6276 [==============================] - 0s 75us/step - loss: 0.5906 - mywloss: 0.5906 - val_loss: 0.7775 - val_mywloss: 0.7775\n",
      "Epoch 187/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.6045 - mywloss: 0.6045 - val_loss: 0.7748 - val_mywloss: 0.7748\n",
      "Epoch 188/600\n",
      "6276/6276 [==============================] - 0s 73us/step - loss: 0.5942 - mywloss: 0.5942 - val_loss: 0.7674 - val_mywloss: 0.7674\n",
      "Epoch 189/600\n",
      "6276/6276 [==============================] - 0s 72us/step - loss: 0.6093 - mywloss: 0.6093 - val_loss: 0.7551 - val_mywloss: 0.7551\n",
      "Epoch 190/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5997 - mywloss: 0.5997 - val_loss: 0.7545 - val_mywloss: 0.7545\n",
      "Epoch 191/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5978 - mywloss: 0.5978 - val_loss: 0.7281 - val_mywloss: 0.7281\n",
      "Epoch 192/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5918 - mywloss: 0.5918 - val_loss: 0.7590 - val_mywloss: 0.7590\n",
      "Epoch 193/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5890 - mywloss: 0.5890 - val_loss: 0.7343 - val_mywloss: 0.7343\n",
      "Epoch 194/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5885 - mywloss: 0.5885 - val_loss: 0.7361 - val_mywloss: 0.7361\n",
      "Epoch 195/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5896 - mywloss: 0.5896 - val_loss: 0.7356 - val_mywloss: 0.7356\n",
      "Epoch 196/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6059 - mywloss: 0.6059 - val_loss: 0.7300 - val_mywloss: 0.7300\n",
      "Epoch 197/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5999 - mywloss: 0.5999 - val_loss: 0.7036 - val_mywloss: 0.7036\n",
      "Epoch 198/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5765 - mywloss: 0.5765 - val_loss: 0.7246 - val_mywloss: 0.7246\n",
      "Epoch 199/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5748 - mywloss: 0.5748 - val_loss: 0.7388 - val_mywloss: 0.7388\n",
      "Epoch 200/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5863 - mywloss: 0.5863 - val_loss: 0.7746 - val_mywloss: 0.7746\n",
      "Epoch 201/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5763 - mywloss: 0.5763 - val_loss: 0.7413 - val_mywloss: 0.7413\n",
      "Epoch 202/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5659 - mywloss: 0.5659 - val_loss: 0.7467 - val_mywloss: 0.7467\n",
      "Epoch 203/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5761 - mywloss: 0.5761 - val_loss: 0.7429 - val_mywloss: 0.7429\n",
      "Epoch 204/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5819 - mywloss: 0.5819 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 205/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5847 - mywloss: 0.5847 - val_loss: 0.7533 - val_mywloss: 0.7533\n",
      "Epoch 206/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.6000 - mywloss: 0.6000 - val_loss: 0.7808 - val_mywloss: 0.7808\n",
      "Epoch 207/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5838 - mywloss: 0.5838 - val_loss: 0.7490 - val_mywloss: 0.7490\n",
      "Epoch 208/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5884 - mywloss: 0.5884 - val_loss: 0.7816 - val_mywloss: 0.7816\n",
      "Epoch 209/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5672 - mywloss: 0.5672 - val_loss: 0.7529 - val_mywloss: 0.7529\n",
      "Epoch 210/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5947 - mywloss: 0.5947 - val_loss: 0.7593 - val_mywloss: 0.7593\n",
      "Epoch 211/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5867 - mywloss: 0.5867 - val_loss: 0.7731 - val_mywloss: 0.7731\n",
      "Epoch 212/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5903 - mywloss: 0.5903 - val_loss: 0.7328 - val_mywloss: 0.7328\n",
      "Epoch 213/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5804 - mywloss: 0.5804 - val_loss: 0.7371 - val_mywloss: 0.7371\n",
      "Epoch 214/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5880 - mywloss: 0.5880 - val_loss: 0.7694 - val_mywloss: 0.7694\n",
      "Epoch 215/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5617 - mywloss: 0.5617 - val_loss: 0.7408 - val_mywloss: 0.7408\n",
      "Epoch 216/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5852 - mywloss: 0.5852 - val_loss: 0.7610 - val_mywloss: 0.7610\n",
      "Epoch 217/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5849 - mywloss: 0.5849 - val_loss: 0.7861 - val_mywloss: 0.7861\n",
      "Epoch 218/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5694 - mywloss: 0.5694 - val_loss: 0.7604 - val_mywloss: 0.7604\n",
      "Epoch 219/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5844 - mywloss: 0.5844 - val_loss: 0.7841 - val_mywloss: 0.7841\n",
      "Epoch 220/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5822 - mywloss: 0.5822 - val_loss: 0.7454 - val_mywloss: 0.7454\n",
      "Epoch 221/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5837 - mywloss: 0.5837 - val_loss: 0.7473 - val_mywloss: 0.7473\n",
      "Epoch 222/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.6063 - mywloss: 0.6063 - val_loss: 0.7448 - val_mywloss: 0.7448\n",
      "Epoch 223/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5806 - mywloss: 0.5806 - val_loss: 0.7719 - val_mywloss: 0.7719\n",
      "Epoch 224/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5734 - mywloss: 0.5734 - val_loss: 0.7656 - val_mywloss: 0.7656\n",
      "Epoch 225/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5702 - mywloss: 0.5702 - val_loss: 0.7610 - val_mywloss: 0.7610\n",
      "Epoch 226/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5663 - mywloss: 0.5663 - val_loss: 0.7725 - val_mywloss: 0.7725\n",
      "Epoch 227/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5781 - mywloss: 0.5781 - val_loss: 0.7525 - val_mywloss: 0.7525\n",
      "Epoch 228/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5593 - mywloss: 0.5593 - val_loss: 0.7644 - val_mywloss: 0.7644\n",
      "Epoch 229/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5702 - mywloss: 0.5702 - val_loss: 0.7657 - val_mywloss: 0.7657\n",
      "Epoch 230/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5762 - mywloss: 0.5762 - val_loss: 0.7557 - val_mywloss: 0.7557\n",
      "Epoch 231/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5715 - mywloss: 0.5715 - val_loss: 0.7471 - val_mywloss: 0.7471\n",
      "Epoch 232/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5622 - mywloss: 0.5622 - val_loss: 0.7650 - val_mywloss: 0.7650\n",
      "Epoch 233/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5712 - mywloss: 0.5712 - val_loss: 0.7849 - val_mywloss: 0.7849\n",
      "Epoch 234/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5519 - mywloss: 0.5519 - val_loss: 0.7492 - val_mywloss: 0.7492\n",
      "Epoch 235/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5464 - mywloss: 0.5464 - val_loss: 0.7729 - val_mywloss: 0.7729\n",
      "Epoch 236/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5589 - mywloss: 0.5589 - val_loss: 0.7793 - val_mywloss: 0.7793\n",
      "Epoch 237/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5700 - mywloss: 0.5700 - val_loss: 0.7822 - val_mywloss: 0.7822\n",
      "Epoch 238/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5721 - mywloss: 0.5721 - val_loss: 0.7647 - val_mywloss: 0.7647\n",
      "Epoch 239/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5687 - mywloss: 0.5687 - val_loss: 0.7238 - val_mywloss: 0.7238\n",
      "Epoch 240/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5711 - mywloss: 0.5711 - val_loss: 0.7362 - val_mywloss: 0.7362\n",
      "Epoch 241/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5652 - mywloss: 0.5652 - val_loss: 0.7342 - val_mywloss: 0.7342\n",
      "Epoch 242/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5721 - mywloss: 0.5721 - val_loss: 0.7202 - val_mywloss: 0.7202\n",
      "Epoch 243/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5687 - mywloss: 0.5687 - val_loss: 0.7165 - val_mywloss: 0.7165\n",
      "Epoch 244/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5846 - mywloss: 0.5846 - val_loss: 0.7287 - val_mywloss: 0.7287\n",
      "Epoch 245/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5753 - mywloss: 0.5753 - val_loss: 0.7153 - val_mywloss: 0.7153\n",
      "Epoch 246/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5791 - mywloss: 0.5791 - val_loss: 0.7242 - val_mywloss: 0.7242\n",
      "Epoch 247/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5748 - mywloss: 0.5748 - val_loss: 0.7512 - val_mywloss: 0.7512\n",
      "Epoch 248/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5631 - mywloss: 0.5631 - val_loss: 0.7845 - val_mywloss: 0.7845\n",
      "Epoch 249/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5716 - mywloss: 0.5716 - val_loss: 0.7362 - val_mywloss: 0.7362\n",
      "Epoch 250/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5757 - mywloss: 0.5757 - val_loss: 0.7322 - val_mywloss: 0.7322\n",
      "Epoch 251/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5496 - mywloss: 0.5496 - val_loss: 0.7321 - val_mywloss: 0.7321\n",
      "Epoch 252/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5659 - mywloss: 0.5659 - val_loss: 0.7560 - val_mywloss: 0.7560\n",
      "Epoch 253/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5559 - mywloss: 0.5559 - val_loss: 0.7057 - val_mywloss: 0.7057\n",
      "Epoch 254/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5697 - mywloss: 0.5697 - val_loss: 0.7217 - val_mywloss: 0.7217\n",
      "Epoch 255/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5537 - mywloss: 0.5537 - val_loss: 0.7299 - val_mywloss: 0.7299\n",
      "Epoch 256/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5632 - mywloss: 0.5632 - val_loss: 0.7874 - val_mywloss: 0.7874\n",
      "Epoch 257/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5404 - mywloss: 0.5404 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 258/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5420 - mywloss: 0.5420 - val_loss: 0.7616 - val_mywloss: 0.7616\n",
      "Epoch 259/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5525 - mywloss: 0.5525 - val_loss: 0.7951 - val_mywloss: 0.7951\n",
      "Epoch 260/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5587 - mywloss: 0.5587 - val_loss: 0.7859 - val_mywloss: 0.7859\n",
      "Epoch 261/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5407 - mywloss: 0.5407 - val_loss: 0.7770 - val_mywloss: 0.7770\n",
      "Epoch 262/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5601 - mywloss: 0.5601 - val_loss: 0.7534 - val_mywloss: 0.7534\n",
      "Epoch 263/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5722 - mywloss: 0.5722 - val_loss: 0.7309 - val_mywloss: 0.7309\n",
      "Epoch 264/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5505 - mywloss: 0.5505 - val_loss: 0.7489 - val_mywloss: 0.7489\n",
      "Epoch 265/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5376 - mywloss: 0.5376 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 266/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5585 - mywloss: 0.5585 - val_loss: 0.7745 - val_mywloss: 0.7745\n",
      "Epoch 267/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5359 - mywloss: 0.5359 - val_loss: 0.7938 - val_mywloss: 0.7938\n",
      "Epoch 268/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5366 - mywloss: 0.5366 - val_loss: 0.8065 - val_mywloss: 0.8065\n",
      "Epoch 269/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5682 - mywloss: 0.5682 - val_loss: 0.8227 - val_mywloss: 0.8227\n",
      "Epoch 270/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5379 - mywloss: 0.5379 - val_loss: 0.8123 - val_mywloss: 0.8123\n",
      "Epoch 271/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5495 - mywloss: 0.5495 - val_loss: 0.7708 - val_mywloss: 0.7708\n",
      "Epoch 272/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5399 - mywloss: 0.5399 - val_loss: 0.7850 - val_mywloss: 0.7850\n",
      "Epoch 273/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5449 - mywloss: 0.5449 - val_loss: 0.8021 - val_mywloss: 0.8021\n",
      "Epoch 274/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5286 - mywloss: 0.5286 - val_loss: 0.8154 - val_mywloss: 0.8154\n",
      "Epoch 275/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5482 - mywloss: 0.5482 - val_loss: 0.7937 - val_mywloss: 0.7937\n",
      "Epoch 276/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5255 - mywloss: 0.5255 - val_loss: 0.7667 - val_mywloss: 0.7667\n",
      "Epoch 277/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5460 - mywloss: 0.5460 - val_loss: 0.7800 - val_mywloss: 0.7800\n",
      "Epoch 278/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5431 - mywloss: 0.5431 - val_loss: 0.7986 - val_mywloss: 0.7986\n",
      "Epoch 279/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5528 - mywloss: 0.5528 - val_loss: 0.7814 - val_mywloss: 0.7814\n",
      "Epoch 280/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5459 - mywloss: 0.5459 - val_loss: 0.8162 - val_mywloss: 0.8162\n",
      "Epoch 281/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5278 - mywloss: 0.5278 - val_loss: 0.8321 - val_mywloss: 0.8321\n",
      "Epoch 282/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5413 - mywloss: 0.5413 - val_loss: 0.7990 - val_mywloss: 0.7990\n",
      "Epoch 283/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5396 - mywloss: 0.5396 - val_loss: 0.8208 - val_mywloss: 0.8208\n",
      "Epoch 284/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5451 - mywloss: 0.5451 - val_loss: 0.8304 - val_mywloss: 0.8304\n",
      "Epoch 285/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5451 - mywloss: 0.5451 - val_loss: 0.8189 - val_mywloss: 0.8189\n",
      "Epoch 286/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5312 - mywloss: 0.5312 - val_loss: 0.8433 - val_mywloss: 0.8433\n",
      "Epoch 287/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5267 - mywloss: 0.5267 - val_loss: 0.8226 - val_mywloss: 0.8226\n",
      "Epoch 288/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5303 - mywloss: 0.5303 - val_loss: 0.7628 - val_mywloss: 0.7628\n",
      "Epoch 289/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5220 - mywloss: 0.5220 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 290/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5438 - mywloss: 0.5438 - val_loss: 0.8128 - val_mywloss: 0.8128\n",
      "Epoch 291/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5159 - mywloss: 0.5159 - val_loss: 0.8070 - val_mywloss: 0.8070\n",
      "Epoch 292/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5276 - mywloss: 0.5276 - val_loss: 0.8155 - val_mywloss: 0.8155\n",
      "Epoch 293/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5266 - mywloss: 0.5266 - val_loss: 0.8071 - val_mywloss: 0.8071\n",
      "Epoch 294/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5240 - mywloss: 0.5240 - val_loss: 0.8128 - val_mywloss: 0.8128\n",
      "Epoch 295/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5421 - mywloss: 0.5421 - val_loss: 0.7961 - val_mywloss: 0.7961\n",
      "Epoch 296/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5483 - mywloss: 0.5483 - val_loss: 0.8148 - val_mywloss: 0.8148\n",
      "Epoch 297/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5392 - mywloss: 0.5392 - val_loss: 0.7798 - val_mywloss: 0.7798\n",
      "Epoch 298/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5346 - mywloss: 0.5346 - val_loss: 0.7750 - val_mywloss: 0.7750\n",
      "Epoch 299/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5269 - mywloss: 0.5269 - val_loss: 0.7578 - val_mywloss: 0.7578\n",
      "Epoch 300/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5277 - mywloss: 0.5277 - val_loss: 0.7818 - val_mywloss: 0.7818\n",
      "Epoch 301/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5183 - mywloss: 0.5183 - val_loss: 0.8400 - val_mywloss: 0.8400\n",
      "Epoch 302/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5508 - mywloss: 0.5508 - val_loss: 0.8195 - val_mywloss: 0.8195\n",
      "Epoch 303/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5375 - mywloss: 0.5375 - val_loss: 0.8313 - val_mywloss: 0.8313\n",
      "Epoch 304/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5324 - mywloss: 0.5324 - val_loss: 0.7974 - val_mywloss: 0.7974\n",
      "Epoch 305/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5270 - mywloss: 0.5270 - val_loss: 0.7927 - val_mywloss: 0.7927\n",
      "Epoch 306/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5226 - mywloss: 0.5226 - val_loss: 0.8303 - val_mywloss: 0.8303\n",
      "Epoch 307/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5244 - mywloss: 0.5244 - val_loss: 0.8097 - val_mywloss: 0.8097\n",
      "Epoch 308/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5358 - mywloss: 0.5358 - val_loss: 0.7634 - val_mywloss: 0.7634\n",
      "Epoch 309/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5134 - mywloss: 0.5134 - val_loss: 0.8103 - val_mywloss: 0.8103\n",
      "Epoch 310/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5410 - mywloss: 0.5410 - val_loss: 0.7613 - val_mywloss: 0.7613\n",
      "Epoch 311/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5377 - mywloss: 0.5377 - val_loss: 0.7674 - val_mywloss: 0.7674\n",
      "Epoch 312/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5077 - mywloss: 0.5077 - val_loss: 0.8117 - val_mywloss: 0.8117\n",
      "Epoch 313/600\n",
      "6276/6276 [==============================] - 0s 64us/step - loss: 0.5288 - mywloss: 0.5288 - val_loss: 0.7778 - val_mywloss: 0.7778\n",
      "Epoch 314/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5238 - mywloss: 0.5238 - val_loss: 0.7816 - val_mywloss: 0.7816\n",
      "Epoch 315/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5314 - mywloss: 0.5314 - val_loss: 0.7958 - val_mywloss: 0.7958\n",
      "Epoch 316/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5277 - mywloss: 0.5277 - val_loss: 0.8069 - val_mywloss: 0.8069\n",
      "Epoch 317/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5327 - mywloss: 0.5327 - val_loss: 0.7752 - val_mywloss: 0.7752\n",
      "Epoch 318/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5057 - mywloss: 0.5057 - val_loss: 0.7863 - val_mywloss: 0.7863\n",
      "Epoch 319/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5182 - mywloss: 0.5182 - val_loss: 0.7954 - val_mywloss: 0.7954\n",
      "Epoch 320/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5353 - mywloss: 0.5353 - val_loss: 0.8109 - val_mywloss: 0.8109\n",
      "Epoch 321/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5243 - mywloss: 0.5243 - val_loss: 0.8323 - val_mywloss: 0.8323\n",
      "Epoch 322/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5257 - mywloss: 0.5257 - val_loss: 0.7927 - val_mywloss: 0.7927\n",
      "Epoch 323/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5229 - mywloss: 0.5229 - val_loss: 0.8125 - val_mywloss: 0.8125\n",
      "Epoch 324/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5241 - mywloss: 0.5241 - val_loss: 0.8374 - val_mywloss: 0.8374\n",
      "Epoch 325/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4990 - mywloss: 0.4990 - val_loss: 0.8016 - val_mywloss: 0.8016\n",
      "Epoch 326/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5149 - mywloss: 0.5149 - val_loss: 0.7939 - val_mywloss: 0.7939\n",
      "Epoch 327/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5179 - mywloss: 0.5179 - val_loss: 0.8103 - val_mywloss: 0.8103\n",
      "Epoch 328/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5200 - mywloss: 0.5200 - val_loss: 0.8048 - val_mywloss: 0.8048\n",
      "Epoch 329/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5176 - mywloss: 0.5176 - val_loss: 0.7715 - val_mywloss: 0.7715\n",
      "Epoch 330/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5223 - mywloss: 0.5223 - val_loss: 0.8280 - val_mywloss: 0.8280\n",
      "Epoch 331/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5169 - mywloss: 0.5169 - val_loss: 0.8335 - val_mywloss: 0.8335\n",
      "Epoch 332/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5147 - mywloss: 0.5147 - val_loss: 0.7984 - val_mywloss: 0.7984\n",
      "Epoch 333/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5144 - mywloss: 0.5144 - val_loss: 0.8213 - val_mywloss: 0.8213\n",
      "Epoch 334/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5232 - mywloss: 0.5232 - val_loss: 0.8149 - val_mywloss: 0.8149\n",
      "Epoch 335/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5158 - mywloss: 0.5158 - val_loss: 0.8035 - val_mywloss: 0.8035\n",
      "Epoch 336/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5121 - mywloss: 0.5121 - val_loss: 0.7948 - val_mywloss: 0.7948\n",
      "Epoch 337/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5118 - mywloss: 0.5118 - val_loss: 0.7990 - val_mywloss: 0.7990\n",
      "Epoch 338/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5334 - mywloss: 0.5334 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 339/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5128 - mywloss: 0.5128 - val_loss: 0.7656 - val_mywloss: 0.7656\n",
      "Epoch 340/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5108 - mywloss: 0.5108 - val_loss: 0.7654 - val_mywloss: 0.7654\n",
      "Epoch 341/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5230 - mywloss: 0.5230 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 342/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5109 - mywloss: 0.5109 - val_loss: 0.7581 - val_mywloss: 0.7581\n",
      "Epoch 343/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5220 - mywloss: 0.5220 - val_loss: 0.7449 - val_mywloss: 0.7449\n",
      "Epoch 344/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5174 - mywloss: 0.5174 - val_loss: 0.7269 - val_mywloss: 0.7269\n",
      "Epoch 345/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5272 - mywloss: 0.5272 - val_loss: 0.7466 - val_mywloss: 0.7466\n",
      "Epoch 346/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5132 - mywloss: 0.5132 - val_loss: 0.7484 - val_mywloss: 0.7484\n",
      "Epoch 347/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5134 - mywloss: 0.5134 - val_loss: 0.7635 - val_mywloss: 0.7635\n",
      "Epoch 348/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5068 - mywloss: 0.5068 - val_loss: 0.7628 - val_mywloss: 0.7628\n",
      "Epoch 349/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5123 - mywloss: 0.5123 - val_loss: 0.7793 - val_mywloss: 0.7793\n",
      "Epoch 350/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5067 - mywloss: 0.5067 - val_loss: 0.8024 - val_mywloss: 0.8024\n",
      "Epoch 351/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4995 - mywloss: 0.4995 - val_loss: 0.7915 - val_mywloss: 0.7915\n",
      "Epoch 352/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5117 - mywloss: 0.5117 - val_loss: 0.7243 - val_mywloss: 0.7243\n",
      "Epoch 353/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5046 - mywloss: 0.5046 - val_loss: 0.7351 - val_mywloss: 0.7351\n",
      "Epoch 354/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5065 - mywloss: 0.5065 - val_loss: 0.8082 - val_mywloss: 0.8082\n",
      "Epoch 355/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5124 - mywloss: 0.5124 - val_loss: 0.8048 - val_mywloss: 0.8048\n",
      "Epoch 356/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5080 - mywloss: 0.5080 - val_loss: 0.8004 - val_mywloss: 0.8004\n",
      "Epoch 357/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5104 - mywloss: 0.5104 - val_loss: 0.8295 - val_mywloss: 0.8295\n",
      "Epoch 358/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5253 - mywloss: 0.5253 - val_loss: 0.7748 - val_mywloss: 0.7748\n",
      "Epoch 359/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5220 - mywloss: 0.5220 - val_loss: 0.8010 - val_mywloss: 0.8010\n",
      "Epoch 360/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5196 - mywloss: 0.5196 - val_loss: 0.7877 - val_mywloss: 0.7877\n",
      "Epoch 361/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5054 - mywloss: 0.5054 - val_loss: 0.7896 - val_mywloss: 0.7896\n",
      "Epoch 362/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.5072 - mywloss: 0.5072 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 363/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5010 - mywloss: 0.5010 - val_loss: 0.7852 - val_mywloss: 0.7852\n",
      "Epoch 364/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.5281 - mywloss: 0.5281 - val_loss: 0.8615 - val_mywloss: 0.8615\n",
      "Epoch 365/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5011 - mywloss: 0.5011 - val_loss: 0.8374 - val_mywloss: 0.8374\n",
      "Epoch 366/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4931 - mywloss: 0.4931 - val_loss: 0.8465 - val_mywloss: 0.8465\n",
      "Epoch 367/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5007 - mywloss: 0.5007 - val_loss: 0.8151 - val_mywloss: 0.8151\n",
      "Epoch 368/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5161 - mywloss: 0.5161 - val_loss: 0.8365 - val_mywloss: 0.8365\n",
      "Epoch 369/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4821 - mywloss: 0.4821 - val_loss: 0.8549 - val_mywloss: 0.8549\n",
      "Epoch 370/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5140 - mywloss: 0.5140 - val_loss: 0.8702 - val_mywloss: 0.8702\n",
      "Epoch 371/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4959 - mywloss: 0.4959 - val_loss: 0.8683 - val_mywloss: 0.8683\n",
      "Epoch 372/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4991 - mywloss: 0.4991 - val_loss: 0.8787 - val_mywloss: 0.8787\n",
      "Epoch 373/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5062 - mywloss: 0.5062 - val_loss: 0.8722 - val_mywloss: 0.8722\n",
      "Epoch 374/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4807 - mywloss: 0.4807 - val_loss: 0.8696 - val_mywloss: 0.8696\n",
      "Epoch 375/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4974 - mywloss: 0.4974 - val_loss: 0.8858 - val_mywloss: 0.8858\n",
      "Epoch 376/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4832 - mywloss: 0.4832 - val_loss: 0.8454 - val_mywloss: 0.8454\n",
      "Epoch 377/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4872 - mywloss: 0.4872 - val_loss: 0.8825 - val_mywloss: 0.8825\n",
      "Epoch 378/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5024 - mywloss: 0.5024 - val_loss: 0.8298 - val_mywloss: 0.8298\n",
      "Epoch 379/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4857 - mywloss: 0.4857 - val_loss: 0.8224 - val_mywloss: 0.8224\n",
      "Epoch 380/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5007 - mywloss: 0.5007 - val_loss: 0.8415 - val_mywloss: 0.8415\n",
      "Epoch 381/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4983 - mywloss: 0.4983 - val_loss: 0.7817 - val_mywloss: 0.7817\n",
      "Epoch 382/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5026 - mywloss: 0.5026 - val_loss: 0.8028 - val_mywloss: 0.8028\n",
      "Epoch 383/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4853 - mywloss: 0.4853 - val_loss: 0.8363 - val_mywloss: 0.8363\n",
      "Epoch 384/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4922 - mywloss: 0.4922 - val_loss: 0.8214 - val_mywloss: 0.8214\n",
      "Epoch 385/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5032 - mywloss: 0.5032 - val_loss: 0.8466 - val_mywloss: 0.8466\n",
      "Epoch 386/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5239 - mywloss: 0.5239 - val_loss: 0.8359 - val_mywloss: 0.8359\n",
      "Epoch 387/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5068 - mywloss: 0.5068 - val_loss: 0.8351 - val_mywloss: 0.8351\n",
      "Epoch 388/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5026 - mywloss: 0.5026 - val_loss: 0.8195 - val_mywloss: 0.8195\n",
      "Epoch 389/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4889 - mywloss: 0.4889 - val_loss: 0.7829 - val_mywloss: 0.7829\n",
      "Epoch 390/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4945 - mywloss: 0.4945 - val_loss: 0.7899 - val_mywloss: 0.7899\n",
      "Epoch 391/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4973 - mywloss: 0.4973 - val_loss: 0.8174 - val_mywloss: 0.8174\n",
      "Epoch 392/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4956 - mywloss: 0.4956 - val_loss: 0.7899 - val_mywloss: 0.7899\n",
      "Epoch 393/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5055 - mywloss: 0.5055 - val_loss: 0.8300 - val_mywloss: 0.8300\n",
      "Epoch 394/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.5001 - mywloss: 0.5001 - val_loss: 0.7981 - val_mywloss: 0.7981\n",
      "Epoch 395/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4796 - mywloss: 0.4796 - val_loss: 0.7920 - val_mywloss: 0.7920\n",
      "Epoch 396/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.8121 - val_mywloss: 0.8121\n",
      "Epoch 397/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4909 - mywloss: 0.4909 - val_loss: 0.8417 - val_mywloss: 0.8417\n",
      "Epoch 398/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4846 - mywloss: 0.4846 - val_loss: 0.8391 - val_mywloss: 0.8391\n",
      "Epoch 399/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4747 - mywloss: 0.4747 - val_loss: 0.8296 - val_mywloss: 0.8296\n",
      "Epoch 400/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4854 - mywloss: 0.4854 - val_loss: 0.8456 - val_mywloss: 0.8456\n",
      "Epoch 401/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4839 - mywloss: 0.4839 - val_loss: 0.8496 - val_mywloss: 0.8496\n",
      "Epoch 402/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4760 - mywloss: 0.4760 - val_loss: 0.8564 - val_mywloss: 0.8564\n",
      "Epoch 403/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4854 - mywloss: 0.4854 - val_loss: 0.8332 - val_mywloss: 0.8332\n",
      "Epoch 404/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4893 - mywloss: 0.4893 - val_loss: 0.8477 - val_mywloss: 0.8477\n",
      "Epoch 405/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.5114 - mywloss: 0.5114 - val_loss: 0.8320 - val_mywloss: 0.8320\n",
      "Epoch 406/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4778 - mywloss: 0.4778 - val_loss: 0.8300 - val_mywloss: 0.8300\n",
      "Epoch 407/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4923 - mywloss: 0.4923 - val_loss: 0.8150 - val_mywloss: 0.8150\n",
      "Epoch 408/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4897 - mywloss: 0.4897 - val_loss: 0.7955 - val_mywloss: 0.7955\n",
      "Epoch 409/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4763 - mywloss: 0.4763 - val_loss: 0.8299 - val_mywloss: 0.8299\n",
      "Epoch 410/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4707 - mywloss: 0.4707 - val_loss: 0.8215 - val_mywloss: 0.8215\n",
      "Epoch 411/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4881 - mywloss: 0.4881 - val_loss: 0.8164 - val_mywloss: 0.8164\n",
      "Epoch 412/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4762 - mywloss: 0.4762 - val_loss: 0.8010 - val_mywloss: 0.8010\n",
      "Epoch 413/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4552 - mywloss: 0.4552 - val_loss: 0.8392 - val_mywloss: 0.8392\n",
      "Epoch 414/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4764 - mywloss: 0.4764 - val_loss: 0.8446 - val_mywloss: 0.8446\n",
      "Epoch 415/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4876 - mywloss: 0.4876 - val_loss: 0.7951 - val_mywloss: 0.7951\n",
      "Epoch 416/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4913 - mywloss: 0.4913 - val_loss: 0.8118 - val_mywloss: 0.8118\n",
      "Epoch 417/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4807 - mywloss: 0.4807 - val_loss: 0.7943 - val_mywloss: 0.7943\n",
      "Epoch 418/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4952 - mywloss: 0.4952 - val_loss: 0.7961 - val_mywloss: 0.7961\n",
      "Epoch 419/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4713 - mywloss: 0.4713 - val_loss: 0.8048 - val_mywloss: 0.8048\n",
      "Epoch 420/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.5003 - mywloss: 0.5003 - val_loss: 0.8174 - val_mywloss: 0.8174\n",
      "Epoch 421/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4864 - mywloss: 0.4864 - val_loss: 0.8310 - val_mywloss: 0.8310\n",
      "Epoch 422/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4953 - mywloss: 0.4953 - val_loss: 0.8375 - val_mywloss: 0.8375\n",
      "Epoch 423/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4876 - mywloss: 0.4876 - val_loss: 0.8255 - val_mywloss: 0.8255\n",
      "Epoch 424/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4890 - mywloss: 0.4890 - val_loss: 0.8523 - val_mywloss: 0.8523\n",
      "Epoch 425/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4758 - mywloss: 0.4758 - val_loss: 0.8623 - val_mywloss: 0.8623\n",
      "Epoch 426/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4711 - mywloss: 0.4711 - val_loss: 0.8627 - val_mywloss: 0.8627\n",
      "Epoch 427/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4847 - mywloss: 0.4847 - val_loss: 0.8640 - val_mywloss: 0.8640\n",
      "Epoch 428/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4830 - mywloss: 0.4830 - val_loss: 0.8635 - val_mywloss: 0.8635\n",
      "Epoch 429/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4791 - mywloss: 0.4791 - val_loss: 0.8522 - val_mywloss: 0.8522\n",
      "Epoch 430/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4998 - mywloss: 0.4998 - val_loss: 0.8320 - val_mywloss: 0.8320\n",
      "Epoch 431/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4874 - mywloss: 0.4874 - val_loss: 0.7775 - val_mywloss: 0.7775\n",
      "Epoch 432/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4737 - mywloss: 0.4737 - val_loss: 0.7771 - val_mywloss: 0.7771\n",
      "Epoch 433/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4690 - mywloss: 0.4690 - val_loss: 0.8011 - val_mywloss: 0.8011\n",
      "Epoch 434/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4842 - mywloss: 0.4842 - val_loss: 0.8063 - val_mywloss: 0.8063\n",
      "Epoch 435/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4935 - mywloss: 0.4935 - val_loss: 0.8042 - val_mywloss: 0.8042\n",
      "Epoch 436/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4913 - mywloss: 0.4913 - val_loss: 0.7773 - val_mywloss: 0.7773\n",
      "Epoch 437/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4701 - mywloss: 0.4701 - val_loss: 0.7694 - val_mywloss: 0.7694\n",
      "Epoch 438/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4901 - mywloss: 0.4901 - val_loss: 0.7947 - val_mywloss: 0.7947\n",
      "Epoch 439/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4983 - mywloss: 0.4983 - val_loss: 0.8028 - val_mywloss: 0.8028\n",
      "Epoch 440/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4709 - mywloss: 0.4709 - val_loss: 0.8025 - val_mywloss: 0.8025\n",
      "Epoch 441/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4803 - mywloss: 0.4803 - val_loss: 0.7808 - val_mywloss: 0.7808\n",
      "Epoch 442/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4789 - mywloss: 0.4789 - val_loss: 0.8192 - val_mywloss: 0.8192\n",
      "Epoch 443/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4741 - mywloss: 0.4741 - val_loss: 0.8331 - val_mywloss: 0.8331\n",
      "Epoch 444/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4653 - mywloss: 0.4653 - val_loss: 0.7987 - val_mywloss: 0.7987\n",
      "Epoch 445/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4912 - mywloss: 0.4912 - val_loss: 0.8057 - val_mywloss: 0.8057\n",
      "Epoch 446/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4772 - mywloss: 0.4772 - val_loss: 0.8172 - val_mywloss: 0.8172\n",
      "Epoch 447/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4603 - mywloss: 0.4603 - val_loss: 0.8231 - val_mywloss: 0.8231\n",
      "Epoch 448/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4903 - mywloss: 0.4903 - val_loss: 0.8363 - val_mywloss: 0.8363\n",
      "Epoch 449/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4663 - mywloss: 0.4663 - val_loss: 0.8139 - val_mywloss: 0.8139\n",
      "Epoch 450/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4753 - mywloss: 0.4753 - val_loss: 0.8391 - val_mywloss: 0.8391\n",
      "Epoch 451/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4722 - mywloss: 0.4722 - val_loss: 0.8580 - val_mywloss: 0.8580\n",
      "Epoch 452/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4890 - mywloss: 0.4890 - val_loss: 0.8352 - val_mywloss: 0.8352\n",
      "Epoch 453/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4748 - mywloss: 0.4748 - val_loss: 0.7976 - val_mywloss: 0.7976\n",
      "Epoch 454/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4850 - mywloss: 0.4850 - val_loss: 0.8317 - val_mywloss: 0.8317\n",
      "Epoch 455/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4729 - mywloss: 0.4729 - val_loss: 0.8476 - val_mywloss: 0.8476\n",
      "Epoch 456/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4905 - mywloss: 0.4905 - val_loss: 0.8536 - val_mywloss: 0.8536\n",
      "Epoch 457/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4829 - mywloss: 0.4829 - val_loss: 0.8592 - val_mywloss: 0.8592\n",
      "Epoch 458/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4520 - mywloss: 0.4520 - val_loss: 0.8454 - val_mywloss: 0.8454\n",
      "Epoch 459/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4645 - mywloss: 0.4645 - val_loss: 0.8540 - val_mywloss: 0.8540\n",
      "Epoch 460/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4779 - mywloss: 0.4779 - val_loss: 0.8677 - val_mywloss: 0.8677\n",
      "Epoch 461/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4630 - mywloss: 0.4630 - val_loss: 0.8545 - val_mywloss: 0.8545\n",
      "Epoch 462/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4639 - mywloss: 0.4639 - val_loss: 0.8311 - val_mywloss: 0.8311\n",
      "Epoch 463/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4766 - mywloss: 0.4766 - val_loss: 0.8070 - val_mywloss: 0.8070\n",
      "Epoch 464/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4529 - mywloss: 0.4529 - val_loss: 0.8319 - val_mywloss: 0.8319\n",
      "Epoch 465/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4716 - mywloss: 0.4716 - val_loss: 0.8121 - val_mywloss: 0.8121\n",
      "Epoch 466/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4592 - mywloss: 0.4592 - val_loss: 0.8195 - val_mywloss: 0.8195\n",
      "Epoch 467/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4707 - mywloss: 0.4707 - val_loss: 0.8257 - val_mywloss: 0.8257\n",
      "Epoch 468/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4832 - mywloss: 0.4832 - val_loss: 0.8221 - val_mywloss: 0.8221\n",
      "Epoch 469/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4559 - mywloss: 0.4559 - val_loss: 0.8710 - val_mywloss: 0.8710\n",
      "Epoch 470/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4737 - mywloss: 0.4737 - val_loss: 0.8302 - val_mywloss: 0.8302\n",
      "Epoch 471/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4721 - mywloss: 0.4721 - val_loss: 0.8581 - val_mywloss: 0.8581\n",
      "Epoch 472/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4694 - mywloss: 0.4694 - val_loss: 0.8510 - val_mywloss: 0.8510\n",
      "Epoch 473/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4708 - mywloss: 0.4708 - val_loss: 0.8526 - val_mywloss: 0.8526\n",
      "Epoch 474/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4695 - mywloss: 0.4695 - val_loss: 0.8649 - val_mywloss: 0.8649\n",
      "Epoch 475/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4731 - mywloss: 0.4731 - val_loss: 0.8877 - val_mywloss: 0.8877\n",
      "Epoch 476/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4597 - mywloss: 0.4597 - val_loss: 0.8788 - val_mywloss: 0.8788\n",
      "Epoch 477/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4572 - mywloss: 0.4572 - val_loss: 0.9230 - val_mywloss: 0.9230\n",
      "Epoch 478/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4537 - mywloss: 0.4537 - val_loss: 0.8938 - val_mywloss: 0.8938\n",
      "Epoch 479/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4617 - mywloss: 0.4617 - val_loss: 0.9040 - val_mywloss: 0.9040\n",
      "Epoch 480/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4472 - mywloss: 0.4472 - val_loss: 0.8827 - val_mywloss: 0.8827\n",
      "Epoch 481/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4525 - mywloss: 0.4525 - val_loss: 0.8929 - val_mywloss: 0.8929\n",
      "Epoch 482/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4540 - mywloss: 0.4540 - val_loss: 0.8687 - val_mywloss: 0.8687\n",
      "Epoch 483/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4694 - mywloss: 0.4694 - val_loss: 0.8600 - val_mywloss: 0.8600\n",
      "Epoch 484/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4718 - mywloss: 0.4718 - val_loss: 0.8668 - val_mywloss: 0.8668\n",
      "Epoch 485/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4544 - mywloss: 0.4544 - val_loss: 0.8494 - val_mywloss: 0.8494\n",
      "Epoch 486/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4807 - mywloss: 0.4807 - val_loss: 0.8626 - val_mywloss: 0.8626\n",
      "Epoch 487/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4618 - mywloss: 0.4618 - val_loss: 0.8483 - val_mywloss: 0.8483\n",
      "Epoch 488/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4514 - mywloss: 0.4514 - val_loss: 0.8548 - val_mywloss: 0.8548\n",
      "Epoch 489/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4448 - mywloss: 0.4448 - val_loss: 0.8634 - val_mywloss: 0.8634\n",
      "Epoch 490/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4605 - mywloss: 0.4605 - val_loss: 0.8673 - val_mywloss: 0.8673\n",
      "Epoch 491/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4500 - mywloss: 0.4500 - val_loss: 0.8792 - val_mywloss: 0.8792\n",
      "Epoch 492/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4506 - mywloss: 0.4506 - val_loss: 0.8489 - val_mywloss: 0.8489\n",
      "Epoch 493/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4606 - mywloss: 0.4606 - val_loss: 0.8081 - val_mywloss: 0.8081\n",
      "Epoch 494/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4584 - mywloss: 0.4584 - val_loss: 0.8286 - val_mywloss: 0.8286\n",
      "Epoch 495/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4597 - mywloss: 0.4597 - val_loss: 0.8540 - val_mywloss: 0.8540\n",
      "Epoch 496/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4615 - mywloss: 0.4615 - val_loss: 0.8514 - val_mywloss: 0.8514\n",
      "Epoch 497/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4415 - mywloss: 0.4415 - val_loss: 0.8492 - val_mywloss: 0.8492\n",
      "Epoch 498/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4642 - mywloss: 0.4642 - val_loss: 0.8220 - val_mywloss: 0.8220\n",
      "Epoch 499/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4428 - mywloss: 0.4428 - val_loss: 0.8072 - val_mywloss: 0.8072\n",
      "Epoch 500/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4627 - mywloss: 0.4627 - val_loss: 0.8064 - val_mywloss: 0.8064\n",
      "Epoch 501/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4229 - mywloss: 0.4229 - val_loss: 0.8387 - val_mywloss: 0.8387\n",
      "Epoch 502/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4491 - mywloss: 0.4491 - val_loss: 0.8667 - val_mywloss: 0.8667\n",
      "Epoch 503/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4525 - mywloss: 0.4525 - val_loss: 0.8478 - val_mywloss: 0.8478\n",
      "Epoch 504/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4551 - mywloss: 0.4551 - val_loss: 0.8034 - val_mywloss: 0.8034\n",
      "Epoch 505/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4596 - mywloss: 0.4596 - val_loss: 0.8361 - val_mywloss: 0.8361\n",
      "Epoch 506/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4451 - mywloss: 0.4451 - val_loss: 0.8185 - val_mywloss: 0.8185\n",
      "Epoch 507/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.4535 - mywloss: 0.4535 - val_loss: 0.8181 - val_mywloss: 0.8181\n",
      "Epoch 508/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4540 - mywloss: 0.4540 - val_loss: 0.8333 - val_mywloss: 0.8333\n",
      "Epoch 509/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.8201 - val_mywloss: 0.8201\n",
      "Epoch 510/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4593 - mywloss: 0.4593 - val_loss: 0.8210 - val_mywloss: 0.8210\n",
      "Epoch 511/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4542 - mywloss: 0.4542 - val_loss: 0.8428 - val_mywloss: 0.8428\n",
      "Epoch 512/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4593 - mywloss: 0.4593 - val_loss: 0.8570 - val_mywloss: 0.8570\n",
      "Epoch 513/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4478 - mywloss: 0.4478 - val_loss: 0.8606 - val_mywloss: 0.8606\n",
      "Epoch 514/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4518 - mywloss: 0.4518 - val_loss: 0.8729 - val_mywloss: 0.8729\n",
      "Epoch 515/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4449 - mywloss: 0.4449 - val_loss: 0.9121 - val_mywloss: 0.9121\n",
      "Epoch 516/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4613 - mywloss: 0.4613 - val_loss: 0.8668 - val_mywloss: 0.8668\n",
      "Epoch 517/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4635 - mywloss: 0.4635 - val_loss: 0.8679 - val_mywloss: 0.8679\n",
      "Epoch 518/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4584 - mywloss: 0.4584 - val_loss: 0.8541 - val_mywloss: 0.8541\n",
      "Epoch 519/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4311 - mywloss: 0.4311 - val_loss: 0.8540 - val_mywloss: 0.8540\n",
      "Epoch 520/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4562 - mywloss: 0.4562 - val_loss: 0.8431 - val_mywloss: 0.8431\n",
      "Epoch 521/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4340 - mywloss: 0.4340 - val_loss: 0.8534 - val_mywloss: 0.8534\n",
      "Epoch 522/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4404 - mywloss: 0.4404 - val_loss: 0.8141 - val_mywloss: 0.8141\n",
      "Epoch 523/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4229 - mywloss: 0.4229 - val_loss: 0.8266 - val_mywloss: 0.8266\n",
      "Epoch 524/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4536 - mywloss: 0.4536 - val_loss: 0.8155 - val_mywloss: 0.8155\n",
      "Epoch 525/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4603 - mywloss: 0.4603 - val_loss: 0.8610 - val_mywloss: 0.8610\n",
      "Epoch 526/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4287 - mywloss: 0.4287 - val_loss: 0.8486 - val_mywloss: 0.8486\n",
      "Epoch 527/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4452 - mywloss: 0.4452 - val_loss: 0.8485 - val_mywloss: 0.8485\n",
      "Epoch 528/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4457 - mywloss: 0.4457 - val_loss: 0.8206 - val_mywloss: 0.8206\n",
      "Epoch 529/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4426 - mywloss: 0.4426 - val_loss: 0.8400 - val_mywloss: 0.8400\n",
      "Epoch 530/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4443 - mywloss: 0.4443 - val_loss: 0.8371 - val_mywloss: 0.8371\n",
      "Epoch 531/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4411 - mywloss: 0.4411 - val_loss: 0.8149 - val_mywloss: 0.8149\n",
      "Epoch 532/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4282 - mywloss: 0.4282 - val_loss: 0.8370 - val_mywloss: 0.8370\n",
      "Epoch 533/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4494 - mywloss: 0.4494 - val_loss: 0.8387 - val_mywloss: 0.8387\n",
      "Epoch 534/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4667 - mywloss: 0.4667 - val_loss: 0.8758 - val_mywloss: 0.8758\n",
      "Epoch 535/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4553 - mywloss: 0.4553 - val_loss: 0.8234 - val_mywloss: 0.8234\n",
      "Epoch 536/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4335 - mywloss: 0.4335 - val_loss: 0.8723 - val_mywloss: 0.8723\n",
      "Epoch 537/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4547 - mywloss: 0.4547 - val_loss: 0.8403 - val_mywloss: 0.8403\n",
      "Epoch 538/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4216 - mywloss: 0.4216 - val_loss: 0.8688 - val_mywloss: 0.8688\n",
      "Epoch 539/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4747 - mywloss: 0.4747 - val_loss: 0.8336 - val_mywloss: 0.8336\n",
      "Epoch 540/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4530 - mywloss: 0.4530 - val_loss: 0.8384 - val_mywloss: 0.8384\n",
      "Epoch 541/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4088 - mywloss: 0.4088 - val_loss: 0.8349 - val_mywloss: 0.8349\n",
      "Epoch 542/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4370 - mywloss: 0.4370 - val_loss: 0.8273 - val_mywloss: 0.8273\n",
      "Epoch 543/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.8473 - val_mywloss: 0.8473\n",
      "Epoch 544/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4566 - mywloss: 0.4566 - val_loss: 0.8442 - val_mywloss: 0.8442\n",
      "Epoch 545/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4571 - mywloss: 0.4571 - val_loss: 0.8264 - val_mywloss: 0.8264\n",
      "Epoch 546/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4259 - mywloss: 0.4259 - val_loss: 0.8384 - val_mywloss: 0.8384\n",
      "Epoch 547/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4392 - mywloss: 0.4392 - val_loss: 0.8699 - val_mywloss: 0.8699\n",
      "Epoch 548/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4401 - mywloss: 0.4401 - val_loss: 0.8599 - val_mywloss: 0.8599\n",
      "Epoch 549/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4650 - mywloss: 0.4650 - val_loss: 0.8663 - val_mywloss: 0.8663\n",
      "Epoch 550/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4381 - mywloss: 0.4381 - val_loss: 0.8676 - val_mywloss: 0.8676\n",
      "Epoch 551/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4362 - mywloss: 0.4362 - val_loss: 0.8685 - val_mywloss: 0.8685\n",
      "Epoch 552/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4354 - mywloss: 0.4354 - val_loss: 0.8572 - val_mywloss: 0.8572\n",
      "Epoch 553/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4379 - mywloss: 0.4379 - val_loss: 0.8519 - val_mywloss: 0.8519\n",
      "Epoch 554/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4338 - mywloss: 0.4338 - val_loss: 0.8397 - val_mywloss: 0.8397\n",
      "Epoch 555/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4354 - mywloss: 0.4354 - val_loss: 0.8189 - val_mywloss: 0.8189\n",
      "Epoch 556/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4422 - mywloss: 0.4422 - val_loss: 0.8318 - val_mywloss: 0.8318\n",
      "Epoch 557/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4368 - mywloss: 0.4368 - val_loss: 0.8585 - val_mywloss: 0.8585\n",
      "Epoch 558/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4486 - mywloss: 0.4486 - val_loss: 0.8407 - val_mywloss: 0.8407\n",
      "Epoch 559/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4314 - mywloss: 0.4314 - val_loss: 0.8444 - val_mywloss: 0.8444\n",
      "Epoch 560/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4296 - mywloss: 0.4296 - val_loss: 0.8396 - val_mywloss: 0.8396\n",
      "Epoch 561/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4295 - mywloss: 0.4295 - val_loss: 0.8295 - val_mywloss: 0.8295\n",
      "Epoch 562/600\n",
      "6276/6276 [==============================] - 0s 69us/step - loss: 0.4474 - mywloss: 0.4474 - val_loss: 0.7976 - val_mywloss: 0.7976\n",
      "Epoch 563/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4339 - mywloss: 0.4339 - val_loss: 0.8177 - val_mywloss: 0.8177\n",
      "Epoch 564/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4421 - mywloss: 0.4421 - val_loss: 0.8228 - val_mywloss: 0.8228\n",
      "Epoch 565/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4112 - mywloss: 0.4112 - val_loss: 0.8232 - val_mywloss: 0.8232\n",
      "Epoch 566/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4251 - mywloss: 0.4251 - val_loss: 0.8086 - val_mywloss: 0.8086\n",
      "Epoch 567/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4351 - mywloss: 0.4351 - val_loss: 0.8179 - val_mywloss: 0.8179\n",
      "Epoch 568/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4224 - mywloss: 0.4224 - val_loss: 0.8317 - val_mywloss: 0.8317\n",
      "Epoch 569/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4471 - mywloss: 0.4471 - val_loss: 0.8237 - val_mywloss: 0.8237\n",
      "Epoch 570/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4441 - mywloss: 0.4441 - val_loss: 0.8118 - val_mywloss: 0.8118\n",
      "Epoch 571/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4322 - mywloss: 0.4322 - val_loss: 0.8508 - val_mywloss: 0.8508\n",
      "Epoch 572/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4550 - mywloss: 0.4550 - val_loss: 0.8457 - val_mywloss: 0.8457\n",
      "Epoch 573/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4182 - mywloss: 0.4182 - val_loss: 0.8649 - val_mywloss: 0.8649\n",
      "Epoch 574/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4235 - mywloss: 0.4235 - val_loss: 0.8650 - val_mywloss: 0.8650\n",
      "Epoch 575/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4425 - mywloss: 0.4425 - val_loss: 0.8082 - val_mywloss: 0.8082\n",
      "Epoch 576/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4167 - mywloss: 0.4167 - val_loss: 0.8178 - val_mywloss: 0.8178\n",
      "Epoch 577/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4293 - mywloss: 0.4293 - val_loss: 0.8160 - val_mywloss: 0.8160\n",
      "Epoch 578/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4377 - mywloss: 0.4377 - val_loss: 0.8126 - val_mywloss: 0.8126\n",
      "Epoch 579/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4336 - mywloss: 0.4336 - val_loss: 0.8238 - val_mywloss: 0.8238\n",
      "Epoch 580/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4064 - mywloss: 0.4064 - val_loss: 0.8149 - val_mywloss: 0.8149\n",
      "Epoch 581/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4254 - mywloss: 0.4254 - val_loss: 0.8636 - val_mywloss: 0.8636\n",
      "Epoch 582/600\n",
      "6276/6276 [==============================] - 0s 65us/step - loss: 0.4186 - mywloss: 0.4186 - val_loss: 0.7657 - val_mywloss: 0.7657\n",
      "Epoch 583/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4378 - mywloss: 0.4378 - val_loss: 0.7595 - val_mywloss: 0.7595\n",
      "Epoch 584/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4333 - mywloss: 0.4333 - val_loss: 0.8008 - val_mywloss: 0.8008\n",
      "Epoch 585/600\n",
      "6276/6276 [==============================] - 0s 70us/step - loss: 0.4353 - mywloss: 0.4353 - val_loss: 0.8001 - val_mywloss: 0.8001\n",
      "Epoch 586/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4440 - mywloss: 0.4440 - val_loss: 0.8047 - val_mywloss: 0.8047\n",
      "Epoch 587/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4361 - mywloss: 0.4361 - val_loss: 0.7820 - val_mywloss: 0.7820\n",
      "Epoch 588/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4205 - mywloss: 0.4205 - val_loss: 0.7752 - val_mywloss: 0.7752\n",
      "Epoch 589/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4456 - mywloss: 0.4456 - val_loss: 0.8175 - val_mywloss: 0.8175\n",
      "Epoch 590/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4439 - mywloss: 0.4439 - val_loss: 0.8280 - val_mywloss: 0.8280\n",
      "Epoch 591/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4277 - mywloss: 0.4277 - val_loss: 0.8045 - val_mywloss: 0.8045\n",
      "Epoch 592/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4293 - mywloss: 0.4293 - val_loss: 0.8055 - val_mywloss: 0.8055\n",
      "Epoch 593/600\n",
      "6276/6276 [==============================] - 0s 68us/step - loss: 0.4324 - mywloss: 0.4324 - val_loss: 0.8122 - val_mywloss: 0.8122\n",
      "Epoch 594/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4232 - mywloss: 0.4232 - val_loss: 0.8126 - val_mywloss: 0.8126\n",
      "Epoch 595/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4494 - mywloss: 0.4494 - val_loss: 0.7938 - val_mywloss: 0.7938\n",
      "Epoch 596/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4243 - mywloss: 0.4243 - val_loss: 0.8230 - val_mywloss: 0.8230\n",
      "Epoch 597/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4340 - mywloss: 0.4340 - val_loss: 0.8510 - val_mywloss: 0.8510\n",
      "Epoch 598/600\n",
      "6276/6276 [==============================] - 0s 67us/step - loss: 0.4321 - mywloss: 0.4321 - val_loss: 0.8212 - val_mywloss: 0.8212\n",
      "Epoch 599/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4287 - mywloss: 0.4287 - val_loss: 0.7845 - val_mywloss: 0.7845\n",
      "Epoch 600/600\n",
      "6276/6276 [==============================] - 0s 66us/step - loss: 0.4415 - mywloss: 0.4415 - val_loss: 0.8192 - val_mywloss: 0.8192\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM6twYTAzswoXBjMzq3BhMDOziv8C/si268IYJ1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbacff90b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model\n",
      "0.6945516055203915\n",
      "Train on 6277 samples, validate on 1571 samples\n",
      "Epoch 1/600\n",
      "6277/6277 [==============================] - 2s 245us/step - loss: 2.5551 - mywloss: 2.5551 - val_loss: 1.4214 - val_mywloss: 1.4214\n",
      "Epoch 2/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.9372 - mywloss: 1.9372 - val_loss: 1.2304 - val_mywloss: 1.2304\n",
      "Epoch 3/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.6488 - mywloss: 1.6488 - val_loss: 1.1531 - val_mywloss: 1.1531\n",
      "Epoch 4/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.5321 - mywloss: 1.5321 - val_loss: 1.0877 - val_mywloss: 1.0877\n",
      "Epoch 5/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.4424 - mywloss: 1.4424 - val_loss: 1.0284 - val_mywloss: 1.0284\n",
      "Epoch 6/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.3580 - mywloss: 1.3580 - val_loss: 1.0173 - val_mywloss: 1.0173\n",
      "Epoch 7/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 1.2867 - mywloss: 1.2867 - val_loss: 0.9994 - val_mywloss: 0.9994\n",
      "Epoch 8/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.2776 - mywloss: 1.2776 - val_loss: 0.9532 - val_mywloss: 0.9532\n",
      "Epoch 9/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 1.2344 - mywloss: 1.2344 - val_loss: 0.9880 - val_mywloss: 0.9880\n",
      "Epoch 10/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.1565 - mywloss: 1.1565 - val_loss: 0.9509 - val_mywloss: 0.9509\n",
      "Epoch 11/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.1425 - mywloss: 1.1425 - val_loss: 0.9325 - val_mywloss: 0.9325\n",
      "Epoch 12/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.1106 - mywloss: 1.1106 - val_loss: 0.9555 - val_mywloss: 0.9555\n",
      "Epoch 13/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.0950 - mywloss: 1.0950 - val_loss: 0.9203 - val_mywloss: 0.9203\n",
      "Epoch 14/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 1.0818 - mywloss: 1.0818 - val_loss: 0.9385 - val_mywloss: 0.9385\n",
      "Epoch 15/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.0446 - mywloss: 1.0446 - val_loss: 0.9398 - val_mywloss: 0.9398\n",
      "Epoch 16/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.0280 - mywloss: 1.0280 - val_loss: 0.9127 - val_mywloss: 0.9127\n",
      "Epoch 17/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.0233 - mywloss: 1.0233 - val_loss: 0.9117 - val_mywloss: 0.9117\n",
      "Epoch 18/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 1.0088 - mywloss: 1.0088 - val_loss: 0.8957 - val_mywloss: 0.8957\n",
      "Epoch 19/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.9761 - mywloss: 0.9761 - val_loss: 0.8974 - val_mywloss: 0.8974\n",
      "Epoch 20/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.9824 - mywloss: 0.9824 - val_loss: 0.8775 - val_mywloss: 0.8775\n",
      "Epoch 21/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.9591 - mywloss: 0.9591 - val_loss: 0.8799 - val_mywloss: 0.8799\n",
      "Epoch 22/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.9535 - mywloss: 0.9535 - val_loss: 0.8904 - val_mywloss: 0.8904\n",
      "Epoch 23/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.9218 - mywloss: 0.9218 - val_loss: 0.8679 - val_mywloss: 0.8679\n",
      "Epoch 24/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.9270 - mywloss: 0.9270 - val_loss: 0.8608 - val_mywloss: 0.8608\n",
      "Epoch 25/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.9297 - mywloss: 0.9297 - val_loss: 0.8633 - val_mywloss: 0.8633\n",
      "Epoch 26/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.9279 - mywloss: 0.9279 - val_loss: 0.8402 - val_mywloss: 0.8402\n",
      "Epoch 27/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.9275 - mywloss: 0.9275 - val_loss: 0.8582 - val_mywloss: 0.8582\n",
      "Epoch 28/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8948 - mywloss: 0.8948 - val_loss: 0.8473 - val_mywloss: 0.8473\n",
      "Epoch 29/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8835 - mywloss: 0.8835 - val_loss: 0.8360 - val_mywloss: 0.8360\n",
      "Epoch 30/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8917 - mywloss: 0.8917 - val_loss: 0.8306 - val_mywloss: 0.8306\n",
      "Epoch 31/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8634 - mywloss: 0.8634 - val_loss: 0.8356 - val_mywloss: 0.8356\n",
      "Epoch 32/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8679 - mywloss: 0.8679 - val_loss: 0.8205 - val_mywloss: 0.8205\n",
      "Epoch 33/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8674 - mywloss: 0.8674 - val_loss: 0.8391 - val_mywloss: 0.8391\n",
      "Epoch 34/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8440 - mywloss: 0.8440 - val_loss: 0.8370 - val_mywloss: 0.8370\n",
      "Epoch 35/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8465 - mywloss: 0.8465 - val_loss: 0.8371 - val_mywloss: 0.8371\n",
      "Epoch 36/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8539 - mywloss: 0.8539 - val_loss: 0.8194 - val_mywloss: 0.8194\n",
      "Epoch 37/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8508 - mywloss: 0.8508 - val_loss: 0.8000 - val_mywloss: 0.8000\n",
      "Epoch 38/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8449 - mywloss: 0.8449 - val_loss: 0.8438 - val_mywloss: 0.8438\n",
      "Epoch 39/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8393 - mywloss: 0.8393 - val_loss: 0.8258 - val_mywloss: 0.8258\n",
      "Epoch 40/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8478 - mywloss: 0.8478 - val_loss: 0.8245 - val_mywloss: 0.8245\n",
      "Epoch 41/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8195 - mywloss: 0.8195 - val_loss: 0.8378 - val_mywloss: 0.8378\n",
      "Epoch 42/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8231 - mywloss: 0.8231 - val_loss: 0.8384 - val_mywloss: 0.8384\n",
      "Epoch 43/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8216 - mywloss: 0.8216 - val_loss: 0.8082 - val_mywloss: 0.8082\n",
      "Epoch 44/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8154 - mywloss: 0.8154 - val_loss: 0.8342 - val_mywloss: 0.8342\n",
      "Epoch 45/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8266 - mywloss: 0.8266 - val_loss: 0.8220 - val_mywloss: 0.8220\n",
      "Epoch 46/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.8179 - mywloss: 0.8179 - val_loss: 0.8556 - val_mywloss: 0.8556\n",
      "Epoch 47/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8155 - mywloss: 0.8155 - val_loss: 0.7997 - val_mywloss: 0.7997\n",
      "Epoch 48/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8197 - mywloss: 0.8197 - val_loss: 0.8575 - val_mywloss: 0.8575\n",
      "Epoch 49/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8026 - mywloss: 0.8026 - val_loss: 0.8242 - val_mywloss: 0.8242\n",
      "Epoch 50/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8025 - mywloss: 0.8025 - val_loss: 0.8118 - val_mywloss: 0.8118\n",
      "Epoch 51/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7921 - mywloss: 0.7921 - val_loss: 0.8240 - val_mywloss: 0.8240\n",
      "Epoch 52/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7972 - mywloss: 0.7972 - val_loss: 0.8415 - val_mywloss: 0.8415\n",
      "Epoch 53/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7971 - mywloss: 0.7971 - val_loss: 0.8291 - val_mywloss: 0.8291\n",
      "Epoch 54/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7763 - mywloss: 0.7763 - val_loss: 0.8006 - val_mywloss: 0.8006\n",
      "Epoch 55/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7667 - mywloss: 0.7667 - val_loss: 0.8287 - val_mywloss: 0.8287\n",
      "Epoch 56/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7766 - mywloss: 0.7766 - val_loss: 0.8302 - val_mywloss: 0.8302\n",
      "Epoch 57/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7775 - mywloss: 0.7775 - val_loss: 0.8402 - val_mywloss: 0.8402\n",
      "Epoch 58/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7898 - mywloss: 0.7898 - val_loss: 0.8444 - val_mywloss: 0.8444\n",
      "Epoch 59/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.8032 - mywloss: 0.8032 - val_loss: 0.8708 - val_mywloss: 0.8708\n",
      "Epoch 60/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.7764 - mywloss: 0.7764 - val_loss: 0.8246 - val_mywloss: 0.8246\n",
      "Epoch 61/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7636 - mywloss: 0.7636 - val_loss: 0.8342 - val_mywloss: 0.8342\n",
      "Epoch 62/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7586 - mywloss: 0.7586 - val_loss: 0.8265 - val_mywloss: 0.8265\n",
      "Epoch 63/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7583 - mywloss: 0.7583 - val_loss: 0.7971 - val_mywloss: 0.7971\n",
      "Epoch 64/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7738 - mywloss: 0.7738 - val_loss: 0.8303 - val_mywloss: 0.8303\n",
      "Epoch 65/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7380 - mywloss: 0.7380 - val_loss: 0.8157 - val_mywloss: 0.8157\n",
      "Epoch 66/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7662 - mywloss: 0.7662 - val_loss: 0.8221 - val_mywloss: 0.8221\n",
      "Epoch 67/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7373 - mywloss: 0.7373 - val_loss: 0.7991 - val_mywloss: 0.7991\n",
      "Epoch 68/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.7373 - mywloss: 0.7373 - val_loss: 0.8342 - val_mywloss: 0.8342\n",
      "Epoch 69/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7362 - mywloss: 0.7362 - val_loss: 0.8254 - val_mywloss: 0.8254\n",
      "Epoch 70/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7277 - mywloss: 0.7277 - val_loss: 0.8304 - val_mywloss: 0.8304\n",
      "Epoch 71/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7285 - mywloss: 0.7285 - val_loss: 0.8240 - val_mywloss: 0.8240\n",
      "Epoch 72/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7352 - mywloss: 0.7352 - val_loss: 0.8609 - val_mywloss: 0.8609\n",
      "Epoch 73/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.7367 - mywloss: 0.7367 - val_loss: 0.8224 - val_mywloss: 0.8224\n",
      "Epoch 74/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7656 - mywloss: 0.7656 - val_loss: 0.8580 - val_mywloss: 0.8580\n",
      "Epoch 75/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7471 - mywloss: 0.7471 - val_loss: 0.8178 - val_mywloss: 0.8178\n",
      "Epoch 76/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7475 - mywloss: 0.7475 - val_loss: 0.8297 - val_mywloss: 0.8297\n",
      "Epoch 77/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7221 - mywloss: 0.7221 - val_loss: 0.8212 - val_mywloss: 0.8212\n",
      "Epoch 78/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.7108 - mywloss: 0.7108 - val_loss: 0.8455 - val_mywloss: 0.8455\n",
      "Epoch 79/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7249 - mywloss: 0.7249 - val_loss: 0.8320 - val_mywloss: 0.8320\n",
      "Epoch 80/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7092 - mywloss: 0.7092 - val_loss: 0.8132 - val_mywloss: 0.8132\n",
      "Epoch 81/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.7131 - mywloss: 0.7131 - val_loss: 0.8088 - val_mywloss: 0.8088\n",
      "Epoch 82/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7317 - mywloss: 0.7317 - val_loss: 0.8179 - val_mywloss: 0.8179\n",
      "Epoch 83/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.7099 - mywloss: 0.7099 - val_loss: 0.7999 - val_mywloss: 0.7999\n",
      "Epoch 84/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7035 - mywloss: 0.7035 - val_loss: 0.8093 - val_mywloss: 0.8093\n",
      "Epoch 85/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6967 - mywloss: 0.6967 - val_loss: 0.7898 - val_mywloss: 0.7898\n",
      "Epoch 86/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6985 - mywloss: 0.6985 - val_loss: 0.8350 - val_mywloss: 0.8350\n",
      "Epoch 87/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7107 - mywloss: 0.7107 - val_loss: 0.8353 - val_mywloss: 0.8353\n",
      "Epoch 88/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6897 - mywloss: 0.6897 - val_loss: 0.8153 - val_mywloss: 0.8153\n",
      "Epoch 89/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6888 - mywloss: 0.6888 - val_loss: 0.8480 - val_mywloss: 0.8480\n",
      "Epoch 90/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6925 - mywloss: 0.6925 - val_loss: 0.8405 - val_mywloss: 0.8405\n",
      "Epoch 91/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7002 - mywloss: 0.7002 - val_loss: 0.8266 - val_mywloss: 0.8266\n",
      "Epoch 92/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6875 - mywloss: 0.6875 - val_loss: 0.8211 - val_mywloss: 0.8211\n",
      "Epoch 93/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6887 - mywloss: 0.6887 - val_loss: 0.8416 - val_mywloss: 0.8416\n",
      "Epoch 94/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7045 - mywloss: 0.7045 - val_loss: 0.8236 - val_mywloss: 0.8236\n",
      "Epoch 95/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6877 - mywloss: 0.6877 - val_loss: 0.8295 - val_mywloss: 0.8295\n",
      "Epoch 96/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.7101 - mywloss: 0.7101 - val_loss: 0.8541 - val_mywloss: 0.8541\n",
      "Epoch 97/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6973 - mywloss: 0.6973 - val_loss: 0.8425 - val_mywloss: 0.8425\n",
      "Epoch 98/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6774 - mywloss: 0.6774 - val_loss: 0.8290 - val_mywloss: 0.8290\n",
      "Epoch 99/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6822 - mywloss: 0.6822 - val_loss: 0.8420 - val_mywloss: 0.8420\n",
      "Epoch 100/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6848 - mywloss: 0.6848 - val_loss: 0.8504 - val_mywloss: 0.8504\n",
      "Epoch 101/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.7028 - mywloss: 0.7028 - val_loss: 0.8501 - val_mywloss: 0.8501\n",
      "Epoch 102/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6971 - mywloss: 0.6971 - val_loss: 0.8433 - val_mywloss: 0.8433\n",
      "Epoch 103/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6827 - mywloss: 0.6827 - val_loss: 0.8361 - val_mywloss: 0.8361\n",
      "Epoch 104/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6828 - mywloss: 0.6828 - val_loss: 0.8602 - val_mywloss: 0.8602\n",
      "Epoch 105/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6684 - mywloss: 0.6684 - val_loss: 0.8293 - val_mywloss: 0.8293\n",
      "Epoch 106/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6786 - mywloss: 0.6786 - val_loss: 0.8597 - val_mywloss: 0.8597\n",
      "Epoch 107/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6840 - mywloss: 0.6840 - val_loss: 0.8350 - val_mywloss: 0.8350\n",
      "Epoch 108/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6873 - mywloss: 0.6873 - val_loss: 0.8222 - val_mywloss: 0.8222\n",
      "Epoch 109/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6667 - mywloss: 0.6667 - val_loss: 0.8421 - val_mywloss: 0.8421\n",
      "Epoch 110/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6791 - mywloss: 0.6791 - val_loss: 0.8097 - val_mywloss: 0.8097\n",
      "Epoch 111/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6604 - mywloss: 0.6604 - val_loss: 0.8077 - val_mywloss: 0.8077\n",
      "Epoch 112/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6752 - mywloss: 0.6752 - val_loss: 0.8249 - val_mywloss: 0.8249\n",
      "Epoch 113/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6453 - mywloss: 0.6453 - val_loss: 0.8264 - val_mywloss: 0.8264\n",
      "Epoch 114/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6730 - mywloss: 0.6730 - val_loss: 0.8606 - val_mywloss: 0.8606\n",
      "Epoch 115/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6500 - mywloss: 0.6500 - val_loss: 0.8383 - val_mywloss: 0.8383\n",
      "Epoch 116/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6770 - mywloss: 0.6770 - val_loss: 0.8468 - val_mywloss: 0.8468\n",
      "Epoch 117/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6576 - mywloss: 0.6576 - val_loss: 0.8175 - val_mywloss: 0.8175\n",
      "Epoch 118/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6724 - mywloss: 0.6724 - val_loss: 0.8521 - val_mywloss: 0.8521\n",
      "Epoch 119/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6601 - mywloss: 0.6601 - val_loss: 0.8049 - val_mywloss: 0.8049\n",
      "Epoch 120/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6669 - mywloss: 0.6669 - val_loss: 0.8104 - val_mywloss: 0.8104\n",
      "Epoch 121/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6609 - mywloss: 0.6609 - val_loss: 0.8126 - val_mywloss: 0.8126\n",
      "Epoch 122/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6529 - mywloss: 0.6529 - val_loss: 0.8065 - val_mywloss: 0.8065\n",
      "Epoch 123/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6528 - mywloss: 0.6528 - val_loss: 0.8387 - val_mywloss: 0.8387\n",
      "Epoch 124/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6411 - mywloss: 0.6411 - val_loss: 0.8544 - val_mywloss: 0.8544\n",
      "Epoch 125/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6516 - mywloss: 0.6516 - val_loss: 0.8329 - val_mywloss: 0.8329\n",
      "Epoch 126/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6484 - mywloss: 0.6484 - val_loss: 0.8518 - val_mywloss: 0.8518\n",
      "Epoch 127/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6395 - mywloss: 0.6395 - val_loss: 0.8288 - val_mywloss: 0.8288\n",
      "Epoch 128/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6569 - mywloss: 0.6569 - val_loss: 0.8851 - val_mywloss: 0.8851\n",
      "Epoch 129/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6675 - mywloss: 0.6675 - val_loss: 0.8416 - val_mywloss: 0.8416\n",
      "Epoch 130/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6522 - mywloss: 0.6522 - val_loss: 0.8318 - val_mywloss: 0.8318\n",
      "Epoch 131/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6357 - mywloss: 0.6357 - val_loss: 0.8059 - val_mywloss: 0.8059\n",
      "Epoch 132/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6925 - mywloss: 0.6925 - val_loss: 0.8551 - val_mywloss: 0.8551\n",
      "Epoch 133/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6314 - mywloss: 0.6314 - val_loss: 0.8349 - val_mywloss: 0.8349\n",
      "Epoch 134/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6512 - mywloss: 0.6512 - val_loss: 0.8126 - val_mywloss: 0.8126\n",
      "Epoch 135/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6449 - mywloss: 0.6449 - val_loss: 0.8037 - val_mywloss: 0.8037\n",
      "Epoch 136/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6219 - mywloss: 0.6219 - val_loss: 0.8177 - val_mywloss: 0.8177\n",
      "Epoch 137/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6370 - mywloss: 0.6370 - val_loss: 0.8266 - val_mywloss: 0.8266\n",
      "Epoch 138/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6376 - mywloss: 0.6376 - val_loss: 0.8191 - val_mywloss: 0.8191\n",
      "Epoch 139/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6308 - mywloss: 0.6308 - val_loss: 0.8337 - val_mywloss: 0.8337\n",
      "Epoch 140/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6381 - mywloss: 0.6381 - val_loss: 0.8158 - val_mywloss: 0.8158\n",
      "Epoch 141/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6473 - mywloss: 0.6473 - val_loss: 0.8315 - val_mywloss: 0.8315\n",
      "Epoch 142/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6252 - mywloss: 0.6252 - val_loss: 0.8207 - val_mywloss: 0.8207\n",
      "Epoch 143/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6310 - mywloss: 0.6310 - val_loss: 0.8152 - val_mywloss: 0.8152\n",
      "Epoch 144/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6119 - mywloss: 0.6119 - val_loss: 0.8256 - val_mywloss: 0.8256\n",
      "Epoch 145/600\n",
      "6277/6277 [==============================] - 0s 64us/step - loss: 0.6332 - mywloss: 0.6332 - val_loss: 0.8214 - val_mywloss: 0.8214\n",
      "Epoch 146/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6267 - mywloss: 0.6267 - val_loss: 0.8362 - val_mywloss: 0.8362\n",
      "Epoch 147/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6154 - mywloss: 0.6154 - val_loss: 0.8637 - val_mywloss: 0.8637\n",
      "Epoch 148/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6192 - mywloss: 0.6192 - val_loss: 0.8518 - val_mywloss: 0.8518\n",
      "Epoch 149/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6326 - mywloss: 0.6326 - val_loss: 0.8525 - val_mywloss: 0.8525\n",
      "Epoch 150/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6262 - mywloss: 0.6262 - val_loss: 0.8604 - val_mywloss: 0.8604\n",
      "Epoch 151/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6284 - mywloss: 0.6284 - val_loss: 0.8635 - val_mywloss: 0.8635\n",
      "Epoch 152/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6119 - mywloss: 0.6119 - val_loss: 0.8405 - val_mywloss: 0.8405\n",
      "Epoch 153/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6147 - mywloss: 0.6147 - val_loss: 0.8349 - val_mywloss: 0.8349\n",
      "Epoch 154/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6117 - mywloss: 0.6117 - val_loss: 0.8158 - val_mywloss: 0.8158\n",
      "Epoch 155/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.6126 - mywloss: 0.6126 - val_loss: 0.8273 - val_mywloss: 0.8273\n",
      "Epoch 156/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6224 - mywloss: 0.6224 - val_loss: 0.8420 - val_mywloss: 0.8420\n",
      "Epoch 157/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6162 - mywloss: 0.6162 - val_loss: 0.8392 - val_mywloss: 0.8392\n",
      "Epoch 158/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6181 - mywloss: 0.6181 - val_loss: 0.8347 - val_mywloss: 0.8347\n",
      "Epoch 159/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5939 - mywloss: 0.5939 - val_loss: 0.8543 - val_mywloss: 0.8543\n",
      "Epoch 160/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5972 - mywloss: 0.5972 - val_loss: 0.8469 - val_mywloss: 0.8469\n",
      "Epoch 161/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.6135 - mywloss: 0.6135 - val_loss: 0.8326 - val_mywloss: 0.8326\n",
      "Epoch 162/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6196 - mywloss: 0.6196 - val_loss: 0.8054 - val_mywloss: 0.8054\n",
      "Epoch 163/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6018 - mywloss: 0.6018 - val_loss: 0.8391 - val_mywloss: 0.8391\n",
      "Epoch 164/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6134 - mywloss: 0.6134 - val_loss: 0.8157 - val_mywloss: 0.8157\n",
      "Epoch 165/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6166 - mywloss: 0.6166 - val_loss: 0.8706 - val_mywloss: 0.8706\n",
      "Epoch 166/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5969 - mywloss: 0.5969 - val_loss: 0.8358 - val_mywloss: 0.8358\n",
      "Epoch 167/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5990 - mywloss: 0.5990 - val_loss: 0.8221 - val_mywloss: 0.8221\n",
      "Epoch 168/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6129 - mywloss: 0.6129 - val_loss: 0.8364 - val_mywloss: 0.8364\n",
      "Epoch 169/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6149 - mywloss: 0.6149 - val_loss: 0.8431 - val_mywloss: 0.8431\n",
      "Epoch 170/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5987 - mywloss: 0.5987 - val_loss: 0.8331 - val_mywloss: 0.8331\n",
      "Epoch 171/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5831 - mywloss: 0.5831 - val_loss: 0.8486 - val_mywloss: 0.8486\n",
      "Epoch 172/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6049 - mywloss: 0.6049 - val_loss: 0.8447 - val_mywloss: 0.8447\n",
      "Epoch 173/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6047 - mywloss: 0.6047 - val_loss: 0.8436 - val_mywloss: 0.8436\n",
      "Epoch 174/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5905 - mywloss: 0.5905 - val_loss: 0.8624 - val_mywloss: 0.8624\n",
      "Epoch 175/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6202 - mywloss: 0.6202 - val_loss: 0.8461 - val_mywloss: 0.8461\n",
      "Epoch 176/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6113 - mywloss: 0.6113 - val_loss: 0.8657 - val_mywloss: 0.8657\n",
      "Epoch 177/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5901 - mywloss: 0.5901 - val_loss: 0.8680 - val_mywloss: 0.8680\n",
      "Epoch 178/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5944 - mywloss: 0.5944 - val_loss: 0.8514 - val_mywloss: 0.8514\n",
      "Epoch 179/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5815 - mywloss: 0.5815 - val_loss: 0.8567 - val_mywloss: 0.8567\n",
      "Epoch 180/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5922 - mywloss: 0.5922 - val_loss: 0.8357 - val_mywloss: 0.8357\n",
      "Epoch 181/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6165 - mywloss: 0.6165 - val_loss: 0.8727 - val_mywloss: 0.8727\n",
      "Epoch 182/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5997 - mywloss: 0.5997 - val_loss: 0.8308 - val_mywloss: 0.8308\n",
      "Epoch 183/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5836 - mywloss: 0.5836 - val_loss: 0.8519 - val_mywloss: 0.8519\n",
      "Epoch 184/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5981 - mywloss: 0.5981 - val_loss: 0.8392 - val_mywloss: 0.8392\n",
      "Epoch 185/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5760 - mywloss: 0.5760 - val_loss: 0.8316 - val_mywloss: 0.8316\n",
      "Epoch 186/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5957 - mywloss: 0.5957 - val_loss: 0.8777 - val_mywloss: 0.8777\n",
      "Epoch 187/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5968 - mywloss: 0.5968 - val_loss: 0.8286 - val_mywloss: 0.8286\n",
      "Epoch 188/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5982 - mywloss: 0.5982 - val_loss: 0.8398 - val_mywloss: 0.8398\n",
      "Epoch 189/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5931 - mywloss: 0.5931 - val_loss: 0.8192 - val_mywloss: 0.8192\n",
      "Epoch 190/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5964 - mywloss: 0.5964 - val_loss: 0.8196 - val_mywloss: 0.8196\n",
      "Epoch 191/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.6001 - mywloss: 0.6001 - val_loss: 0.8428 - val_mywloss: 0.8428\n",
      "Epoch 192/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5760 - mywloss: 0.5760 - val_loss: 0.8541 - val_mywloss: 0.8541\n",
      "Epoch 193/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5732 - mywloss: 0.5732 - val_loss: 0.8583 - val_mywloss: 0.8583\n",
      "Epoch 194/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5872 - mywloss: 0.5872 - val_loss: 0.8440 - val_mywloss: 0.8440\n",
      "Epoch 195/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.6009 - mywloss: 0.6009 - val_loss: 0.8203 - val_mywloss: 0.8203\n",
      "Epoch 196/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5848 - mywloss: 0.5848 - val_loss: 0.8486 - val_mywloss: 0.8486\n",
      "Epoch 197/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5753 - mywloss: 0.5753 - val_loss: 0.8504 - val_mywloss: 0.8504\n",
      "Epoch 198/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5835 - mywloss: 0.5835 - val_loss: 0.8234 - val_mywloss: 0.8234\n",
      "Epoch 199/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5921 - mywloss: 0.5921 - val_loss: 0.8427 - val_mywloss: 0.8427\n",
      "Epoch 200/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5671 - mywloss: 0.5671 - val_loss: 0.8464 - val_mywloss: 0.8464\n",
      "Epoch 201/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5707 - mywloss: 0.5707 - val_loss: 0.8445 - val_mywloss: 0.8445\n",
      "Epoch 202/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5938 - mywloss: 0.5938 - val_loss: 0.8635 - val_mywloss: 0.8635\n",
      "Epoch 203/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5769 - mywloss: 0.5769 - val_loss: 0.8606 - val_mywloss: 0.8606\n",
      "Epoch 204/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5727 - mywloss: 0.5727 - val_loss: 0.8509 - val_mywloss: 0.8509\n",
      "Epoch 205/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5949 - mywloss: 0.5949 - val_loss: 0.8455 - val_mywloss: 0.8455\n",
      "Epoch 206/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5880 - mywloss: 0.5880 - val_loss: 0.8849 - val_mywloss: 0.8849\n",
      "Epoch 207/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5744 - mywloss: 0.5744 - val_loss: 0.8637 - val_mywloss: 0.8637\n",
      "Epoch 208/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5972 - mywloss: 0.5972 - val_loss: 0.8421 - val_mywloss: 0.8421\n",
      "Epoch 209/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5967 - mywloss: 0.5967 - val_loss: 0.8327 - val_mywloss: 0.8327\n",
      "Epoch 210/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5823 - mywloss: 0.5823 - val_loss: 0.8760 - val_mywloss: 0.8760\n",
      "Epoch 211/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5583 - mywloss: 0.5583 - val_loss: 0.8619 - val_mywloss: 0.8619\n",
      "Epoch 212/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5867 - mywloss: 0.5867 - val_loss: 0.8509 - val_mywloss: 0.8509\n",
      "Epoch 213/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5633 - mywloss: 0.5633 - val_loss: 0.8451 - val_mywloss: 0.8451\n",
      "Epoch 214/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5656 - mywloss: 0.5656 - val_loss: 0.8629 - val_mywloss: 0.8629\n",
      "Epoch 215/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5812 - mywloss: 0.5812 - val_loss: 0.8484 - val_mywloss: 0.8484\n",
      "Epoch 216/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5604 - mywloss: 0.5604 - val_loss: 0.8627 - val_mywloss: 0.8627\n",
      "Epoch 217/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5748 - mywloss: 0.5748 - val_loss: 0.8831 - val_mywloss: 0.8831\n",
      "Epoch 218/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5451 - mywloss: 0.5451 - val_loss: 0.8437 - val_mywloss: 0.8437\n",
      "Epoch 219/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5840 - mywloss: 0.5840 - val_loss: 0.8494 - val_mywloss: 0.8494\n",
      "Epoch 220/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5793 - mywloss: 0.5793 - val_loss: 0.8467 - val_mywloss: 0.8467\n",
      "Epoch 221/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5606 - mywloss: 0.5606 - val_loss: 0.8346 - val_mywloss: 0.8346\n",
      "Epoch 222/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5697 - mywloss: 0.5697 - val_loss: 0.8453 - val_mywloss: 0.8453\n",
      "Epoch 223/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5681 - mywloss: 0.5681 - val_loss: 0.8318 - val_mywloss: 0.8318\n",
      "Epoch 224/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5772 - mywloss: 0.5772 - val_loss: 0.8514 - val_mywloss: 0.8514\n",
      "Epoch 225/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5462 - mywloss: 0.5462 - val_loss: 0.8491 - val_mywloss: 0.8491\n",
      "Epoch 226/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5838 - mywloss: 0.5838 - val_loss: 0.8254 - val_mywloss: 0.8254\n",
      "Epoch 227/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5321 - mywloss: 0.5321 - val_loss: 0.8542 - val_mywloss: 0.8542\n",
      "Epoch 228/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5583 - mywloss: 0.5583 - val_loss: 0.8405 - val_mywloss: 0.8405\n",
      "Epoch 229/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5543 - mywloss: 0.5543 - val_loss: 0.8413 - val_mywloss: 0.8413\n",
      "Epoch 230/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5679 - mywloss: 0.5679 - val_loss: 0.8369 - val_mywloss: 0.8369\n",
      "Epoch 231/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5421 - mywloss: 0.5421 - val_loss: 0.8322 - val_mywloss: 0.8322\n",
      "Epoch 232/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5667 - mywloss: 0.5667 - val_loss: 0.8435 - val_mywloss: 0.8435\n",
      "Epoch 233/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5631 - mywloss: 0.5631 - val_loss: 0.8655 - val_mywloss: 0.8655\n",
      "Epoch 234/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5630 - mywloss: 0.5630 - val_loss: 0.8479 - val_mywloss: 0.8479\n",
      "Epoch 235/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5554 - mywloss: 0.5554 - val_loss: 0.8394 - val_mywloss: 0.8394\n",
      "Epoch 236/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5410 - mywloss: 0.5410 - val_loss: 0.8350 - val_mywloss: 0.8350\n",
      "Epoch 237/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5646 - mywloss: 0.5646 - val_loss: 0.8474 - val_mywloss: 0.8474\n",
      "Epoch 238/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5427 - mywloss: 0.5427 - val_loss: 0.8283 - val_mywloss: 0.8283\n",
      "Epoch 239/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5547 - mywloss: 0.5547 - val_loss: 0.8538 - val_mywloss: 0.8538\n",
      "Epoch 240/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5680 - mywloss: 0.5680 - val_loss: 0.8712 - val_mywloss: 0.8712\n",
      "Epoch 241/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5450 - mywloss: 0.5450 - val_loss: 0.8219 - val_mywloss: 0.8219\n",
      "Epoch 242/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5407 - mywloss: 0.5407 - val_loss: 0.8532 - val_mywloss: 0.8532\n",
      "Epoch 243/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5478 - mywloss: 0.5478 - val_loss: 0.8375 - val_mywloss: 0.8375\n",
      "Epoch 244/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5390 - mywloss: 0.5390 - val_loss: 0.8410 - val_mywloss: 0.8410\n",
      "Epoch 245/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5508 - mywloss: 0.5508 - val_loss: 0.8172 - val_mywloss: 0.8172\n",
      "Epoch 246/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5513 - mywloss: 0.5513 - val_loss: 0.8544 - val_mywloss: 0.8544\n",
      "Epoch 247/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5548 - mywloss: 0.5548 - val_loss: 0.8492 - val_mywloss: 0.8492\n",
      "Epoch 248/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5764 - mywloss: 0.5764 - val_loss: 0.8659 - val_mywloss: 0.8659\n",
      "Epoch 249/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5545 - mywloss: 0.5545 - val_loss: 0.8759 - val_mywloss: 0.8759\n",
      "Epoch 250/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5474 - mywloss: 0.5474 - val_loss: 0.8964 - val_mywloss: 0.8964\n",
      "Epoch 251/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5314 - mywloss: 0.5314 - val_loss: 0.8844 - val_mywloss: 0.8844\n",
      "Epoch 252/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5454 - mywloss: 0.5454 - val_loss: 0.8566 - val_mywloss: 0.8566\n",
      "Epoch 253/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5756 - mywloss: 0.5756 - val_loss: 0.8759 - val_mywloss: 0.8759\n",
      "Epoch 254/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5404 - mywloss: 0.5404 - val_loss: 0.8243 - val_mywloss: 0.8243\n",
      "Epoch 255/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5742 - mywloss: 0.5742 - val_loss: 0.8363 - val_mywloss: 0.8363\n",
      "Epoch 256/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5492 - mywloss: 0.5492 - val_loss: 0.8656 - val_mywloss: 0.8656\n",
      "Epoch 257/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5475 - mywloss: 0.5475 - val_loss: 0.8404 - val_mywloss: 0.8404\n",
      "Epoch 258/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5536 - mywloss: 0.5536 - val_loss: 0.8351 - val_mywloss: 0.8351\n",
      "Epoch 259/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5459 - mywloss: 0.5459 - val_loss: 0.8765 - val_mywloss: 0.8765\n",
      "Epoch 260/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5484 - mywloss: 0.5484 - val_loss: 0.8367 - val_mywloss: 0.8367\n",
      "Epoch 261/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5482 - mywloss: 0.5482 - val_loss: 0.8165 - val_mywloss: 0.8165\n",
      "Epoch 262/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5545 - mywloss: 0.5545 - val_loss: 0.8289 - val_mywloss: 0.8289\n",
      "Epoch 263/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5595 - mywloss: 0.5595 - val_loss: 0.8183 - val_mywloss: 0.8183\n",
      "Epoch 264/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5556 - mywloss: 0.5556 - val_loss: 0.8273 - val_mywloss: 0.8273\n",
      "Epoch 265/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5504 - mywloss: 0.5504 - val_loss: 0.8429 - val_mywloss: 0.8429\n",
      "Epoch 266/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5373 - mywloss: 0.5373 - val_loss: 0.8312 - val_mywloss: 0.8312\n",
      "Epoch 267/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5502 - mywloss: 0.5502 - val_loss: 0.8375 - val_mywloss: 0.8375\n",
      "Epoch 268/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5494 - mywloss: 0.5494 - val_loss: 0.8473 - val_mywloss: 0.8473\n",
      "Epoch 269/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5221 - mywloss: 0.5221 - val_loss: 0.8370 - val_mywloss: 0.8370\n",
      "Epoch 270/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5386 - mywloss: 0.5386 - val_loss: 0.8769 - val_mywloss: 0.8769\n",
      "Epoch 271/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5388 - mywloss: 0.5388 - val_loss: 0.8442 - val_mywloss: 0.8442\n",
      "Epoch 272/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5322 - mywloss: 0.5322 - val_loss: 0.8775 - val_mywloss: 0.8775\n",
      "Epoch 273/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5178 - mywloss: 0.5178 - val_loss: 0.8535 - val_mywloss: 0.8535\n",
      "Epoch 274/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5468 - mywloss: 0.5468 - val_loss: 0.8825 - val_mywloss: 0.8825\n",
      "Epoch 275/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5354 - mywloss: 0.5354 - val_loss: 0.8848 - val_mywloss: 0.8848\n",
      "Epoch 276/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5450 - mywloss: 0.5450 - val_loss: 0.8616 - val_mywloss: 0.8616\n",
      "Epoch 277/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5481 - mywloss: 0.5481 - val_loss: 0.8599 - val_mywloss: 0.8599\n",
      "Epoch 278/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5471 - mywloss: 0.5471 - val_loss: 0.8307 - val_mywloss: 0.8307\n",
      "Epoch 279/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5536 - mywloss: 0.5536 - val_loss: 0.8595 - val_mywloss: 0.8595\n",
      "Epoch 280/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5415 - mywloss: 0.5415 - val_loss: 0.8560 - val_mywloss: 0.8560\n",
      "Epoch 281/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5471 - mywloss: 0.5471 - val_loss: 0.8620 - val_mywloss: 0.8620\n",
      "Epoch 282/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5236 - mywloss: 0.5236 - val_loss: 0.8999 - val_mywloss: 0.8999\n",
      "Epoch 283/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5450 - mywloss: 0.5450 - val_loss: 0.8830 - val_mywloss: 0.8830\n",
      "Epoch 284/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5204 - mywloss: 0.5204 - val_loss: 0.8648 - val_mywloss: 0.8648\n",
      "Epoch 285/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5475 - mywloss: 0.5475 - val_loss: 0.8827 - val_mywloss: 0.8827\n",
      "Epoch 286/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5280 - mywloss: 0.5280 - val_loss: 0.8604 - val_mywloss: 0.8604\n",
      "Epoch 287/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5499 - mywloss: 0.5499 - val_loss: 0.9495 - val_mywloss: 0.9495\n",
      "Epoch 288/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5329 - mywloss: 0.5329 - val_loss: 0.8241 - val_mywloss: 0.8241\n",
      "Epoch 289/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5284 - mywloss: 0.5284 - val_loss: 0.8567 - val_mywloss: 0.8567\n",
      "Epoch 290/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5428 - mywloss: 0.5428 - val_loss: 0.8728 - val_mywloss: 0.8728\n",
      "Epoch 291/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5331 - mywloss: 0.5331 - val_loss: 0.8153 - val_mywloss: 0.8153\n",
      "Epoch 292/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5474 - mywloss: 0.5474 - val_loss: 0.8820 - val_mywloss: 0.8820\n",
      "Epoch 293/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5302 - mywloss: 0.5302 - val_loss: 0.8684 - val_mywloss: 0.8684\n",
      "Epoch 294/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5352 - mywloss: 0.5352 - val_loss: 0.8838 - val_mywloss: 0.8838\n",
      "Epoch 295/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5270 - mywloss: 0.5270 - val_loss: 0.8850 - val_mywloss: 0.8850\n",
      "Epoch 296/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5317 - mywloss: 0.5317 - val_loss: 0.8892 - val_mywloss: 0.8892\n",
      "Epoch 297/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5319 - mywloss: 0.5319 - val_loss: 0.8790 - val_mywloss: 0.8790\n",
      "Epoch 298/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5152 - mywloss: 0.5152 - val_loss: 0.8959 - val_mywloss: 0.8959\n",
      "Epoch 299/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5194 - mywloss: 0.5194 - val_loss: 0.8738 - val_mywloss: 0.8738\n",
      "Epoch 300/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5199 - mywloss: 0.5199 - val_loss: 0.8726 - val_mywloss: 0.8726\n",
      "Epoch 301/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5256 - mywloss: 0.5256 - val_loss: 0.9031 - val_mywloss: 0.9031\n",
      "Epoch 302/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5332 - mywloss: 0.5332 - val_loss: 0.8867 - val_mywloss: 0.8867\n",
      "Epoch 303/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5563 - mywloss: 0.5563 - val_loss: 0.8848 - val_mywloss: 0.8848\n",
      "Epoch 304/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5402 - mywloss: 0.5402 - val_loss: 0.8772 - val_mywloss: 0.8772\n",
      "Epoch 305/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5348 - mywloss: 0.5348 - val_loss: 0.8406 - val_mywloss: 0.8406\n",
      "Epoch 306/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5158 - mywloss: 0.5158 - val_loss: 0.8587 - val_mywloss: 0.8587\n",
      "Epoch 307/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5250 - mywloss: 0.5250 - val_loss: 0.8445 - val_mywloss: 0.8445\n",
      "Epoch 308/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5362 - mywloss: 0.5362 - val_loss: 0.8310 - val_mywloss: 0.8310\n",
      "Epoch 309/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5345 - mywloss: 0.5345 - val_loss: 0.8517 - val_mywloss: 0.8517\n",
      "Epoch 310/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5258 - mywloss: 0.5258 - val_loss: 0.8559 - val_mywloss: 0.8559\n",
      "Epoch 311/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5361 - mywloss: 0.5361 - val_loss: 0.8619 - val_mywloss: 0.8619\n",
      "Epoch 312/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5294 - mywloss: 0.5294 - val_loss: 0.8695 - val_mywloss: 0.8695\n",
      "Epoch 313/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5362 - mywloss: 0.5362 - val_loss: 0.8372 - val_mywloss: 0.8372\n",
      "Epoch 314/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5337 - mywloss: 0.5337 - val_loss: 0.8247 - val_mywloss: 0.8247\n",
      "Epoch 315/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5328 - mywloss: 0.5328 - val_loss: 0.8542 - val_mywloss: 0.8542\n",
      "Epoch 316/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5313 - mywloss: 0.5313 - val_loss: 0.8583 - val_mywloss: 0.8583\n",
      "Epoch 317/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5329 - mywloss: 0.5329 - val_loss: 0.8847 - val_mywloss: 0.8847\n",
      "Epoch 318/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5124 - mywloss: 0.5124 - val_loss: 0.8426 - val_mywloss: 0.8426\n",
      "Epoch 319/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5111 - mywloss: 0.5111 - val_loss: 0.8626 - val_mywloss: 0.8626\n",
      "Epoch 320/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5062 - mywloss: 0.5062 - val_loss: 0.8521 - val_mywloss: 0.8521\n",
      "Epoch 321/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5133 - mywloss: 0.5133 - val_loss: 0.8445 - val_mywloss: 0.8445\n",
      "Epoch 322/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5034 - mywloss: 0.5034 - val_loss: 0.8518 - val_mywloss: 0.8518\n",
      "Epoch 323/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5121 - mywloss: 0.5121 - val_loss: 0.8298 - val_mywloss: 0.8298\n",
      "Epoch 324/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5091 - mywloss: 0.5091 - val_loss: 0.8471 - val_mywloss: 0.8471\n",
      "Epoch 325/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5272 - mywloss: 0.5272 - val_loss: 0.8237 - val_mywloss: 0.8237\n",
      "Epoch 326/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5111 - mywloss: 0.5111 - val_loss: 0.8212 - val_mywloss: 0.8212\n",
      "Epoch 327/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5044 - mywloss: 0.5044 - val_loss: 0.8607 - val_mywloss: 0.8607\n",
      "Epoch 328/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5069 - mywloss: 0.5069 - val_loss: 0.8570 - val_mywloss: 0.8570\n",
      "Epoch 329/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5117 - mywloss: 0.5117 - val_loss: 0.8926 - val_mywloss: 0.8926\n",
      "Epoch 330/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4996 - mywloss: 0.4996 - val_loss: 0.8547 - val_mywloss: 0.8547\n",
      "Epoch 331/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5120 - mywloss: 0.5120 - val_loss: 0.8524 - val_mywloss: 0.8524\n",
      "Epoch 332/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5154 - mywloss: 0.5154 - val_loss: 0.8443 - val_mywloss: 0.8443\n",
      "Epoch 333/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5055 - mywloss: 0.5055 - val_loss: 0.8542 - val_mywloss: 0.8542\n",
      "Epoch 334/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5089 - mywloss: 0.5089 - val_loss: 0.8651 - val_mywloss: 0.8651\n",
      "Epoch 335/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4914 - mywloss: 0.4914 - val_loss: 0.8643 - val_mywloss: 0.8643\n",
      "Epoch 336/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4944 - mywloss: 0.4944 - val_loss: 0.8759 - val_mywloss: 0.8759\n",
      "Epoch 337/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4937 - mywloss: 0.4937 - val_loss: 0.8643 - val_mywloss: 0.8643\n",
      "Epoch 338/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4978 - mywloss: 0.4978 - val_loss: 0.8614 - val_mywloss: 0.8614\n",
      "Epoch 339/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4831 - mywloss: 0.4831 - val_loss: 0.8906 - val_mywloss: 0.8906\n",
      "Epoch 340/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5015 - mywloss: 0.5015 - val_loss: 0.9137 - val_mywloss: 0.9137\n",
      "Epoch 341/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5028 - mywloss: 0.5028 - val_loss: 0.9044 - val_mywloss: 0.9044\n",
      "Epoch 342/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5098 - mywloss: 0.5098 - val_loss: 0.9198 - val_mywloss: 0.9198\n",
      "Epoch 343/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5054 - mywloss: 0.5054 - val_loss: 0.8837 - val_mywloss: 0.8837\n",
      "Epoch 344/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4790 - mywloss: 0.4790 - val_loss: 0.8810 - val_mywloss: 0.8810\n",
      "Epoch 345/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4987 - mywloss: 0.4987 - val_loss: 0.8831 - val_mywloss: 0.8831\n",
      "Epoch 346/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5007 - mywloss: 0.5007 - val_loss: 0.9193 - val_mywloss: 0.9193\n",
      "Epoch 347/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5027 - mywloss: 0.5027 - val_loss: 0.9193 - val_mywloss: 0.9193\n",
      "Epoch 348/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5026 - mywloss: 0.5026 - val_loss: 0.9065 - val_mywloss: 0.9065\n",
      "Epoch 349/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5098 - mywloss: 0.5098 - val_loss: 0.9184 - val_mywloss: 0.9184\n",
      "Epoch 350/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4891 - mywloss: 0.4891 - val_loss: 0.9062 - val_mywloss: 0.9062\n",
      "Epoch 351/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4882 - mywloss: 0.4882 - val_loss: 0.8772 - val_mywloss: 0.8772\n",
      "Epoch 352/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4946 - mywloss: 0.4946 - val_loss: 0.8751 - val_mywloss: 0.8751\n",
      "Epoch 353/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4970 - mywloss: 0.4970 - val_loss: 0.8594 - val_mywloss: 0.8594\n",
      "Epoch 354/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4943 - mywloss: 0.4943 - val_loss: 0.8590 - val_mywloss: 0.8590\n",
      "Epoch 355/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4847 - mywloss: 0.4847 - val_loss: 0.8740 - val_mywloss: 0.8740\n",
      "Epoch 356/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5079 - mywloss: 0.5079 - val_loss: 0.8607 - val_mywloss: 0.8607\n",
      "Epoch 357/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.5183 - mywloss: 0.5183 - val_loss: 0.8992 - val_mywloss: 0.8992\n",
      "Epoch 358/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5042 - mywloss: 0.5042 - val_loss: 0.8691 - val_mywloss: 0.8691\n",
      "Epoch 359/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5046 - mywloss: 0.5046 - val_loss: 0.8815 - val_mywloss: 0.8815\n",
      "Epoch 360/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5122 - mywloss: 0.5122 - val_loss: 0.8563 - val_mywloss: 0.8563\n",
      "Epoch 361/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4944 - mywloss: 0.4944 - val_loss: 0.8599 - val_mywloss: 0.8599\n",
      "Epoch 362/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5043 - mywloss: 0.5043 - val_loss: 0.8558 - val_mywloss: 0.8558\n",
      "Epoch 363/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4980 - mywloss: 0.4980 - val_loss: 0.8476 - val_mywloss: 0.8476\n",
      "Epoch 364/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4892 - mywloss: 0.4892 - val_loss: 0.8551 - val_mywloss: 0.8551\n",
      "Epoch 365/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4888 - mywloss: 0.4888 - val_loss: 0.8512 - val_mywloss: 0.8512\n",
      "Epoch 366/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4887 - mywloss: 0.4887 - val_loss: 0.8528 - val_mywloss: 0.8528\n",
      "Epoch 367/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5026 - mywloss: 0.5026 - val_loss: 0.8611 - val_mywloss: 0.8611\n",
      "Epoch 368/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4746 - mywloss: 0.4746 - val_loss: 0.8705 - val_mywloss: 0.8705\n",
      "Epoch 369/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.5012 - mywloss: 0.5012 - val_loss: 0.8722 - val_mywloss: 0.8722\n",
      "Epoch 370/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4998 - mywloss: 0.4998 - val_loss: 0.8919 - val_mywloss: 0.8919\n",
      "Epoch 371/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4927 - mywloss: 0.4927 - val_loss: 0.8801 - val_mywloss: 0.8801\n",
      "Epoch 372/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4980 - mywloss: 0.4980 - val_loss: 0.8614 - val_mywloss: 0.8614\n",
      "Epoch 373/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4979 - mywloss: 0.4979 - val_loss: 0.8478 - val_mywloss: 0.8478\n",
      "Epoch 374/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.5047 - mywloss: 0.5047 - val_loss: 0.8567 - val_mywloss: 0.8567\n",
      "Epoch 375/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5029 - mywloss: 0.5029 - val_loss: 0.8608 - val_mywloss: 0.8608\n",
      "Epoch 376/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4994 - mywloss: 0.4994 - val_loss: 0.8660 - val_mywloss: 0.8660\n",
      "Epoch 377/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4908 - mywloss: 0.4908 - val_loss: 0.8710 - val_mywloss: 0.8710\n",
      "Epoch 378/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4984 - mywloss: 0.4984 - val_loss: 0.8428 - val_mywloss: 0.8428\n",
      "Epoch 379/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5177 - mywloss: 0.5177 - val_loss: 0.8426 - val_mywloss: 0.8426\n",
      "Epoch 380/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.5139 - mywloss: 0.5139 - val_loss: 0.8656 - val_mywloss: 0.8656\n",
      "Epoch 381/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.5041 - mywloss: 0.5041 - val_loss: 0.8688 - val_mywloss: 0.8688\n",
      "Epoch 382/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4875 - mywloss: 0.4875 - val_loss: 0.8753 - val_mywloss: 0.8753\n",
      "Epoch 383/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5020 - mywloss: 0.5020 - val_loss: 0.8998 - val_mywloss: 0.8998\n",
      "Epoch 384/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4945 - mywloss: 0.4945 - val_loss: 0.9219 - val_mywloss: 0.9219\n",
      "Epoch 385/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4902 - mywloss: 0.4902 - val_loss: 0.8741 - val_mywloss: 0.8741\n",
      "Epoch 386/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4831 - mywloss: 0.4831 - val_loss: 0.8630 - val_mywloss: 0.8630\n",
      "Epoch 387/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4852 - mywloss: 0.4852 - val_loss: 0.8759 - val_mywloss: 0.8759\n",
      "Epoch 388/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4947 - mywloss: 0.4947 - val_loss: 0.8744 - val_mywloss: 0.8744\n",
      "Epoch 389/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4743 - mywloss: 0.4743 - val_loss: 0.9018 - val_mywloss: 0.9018\n",
      "Epoch 390/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4978 - mywloss: 0.4978 - val_loss: 0.8703 - val_mywloss: 0.8703\n",
      "Epoch 391/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4825 - mywloss: 0.4825 - val_loss: 0.9048 - val_mywloss: 0.9048\n",
      "Epoch 392/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4929 - mywloss: 0.4929 - val_loss: 0.8776 - val_mywloss: 0.8776\n",
      "Epoch 393/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4962 - mywloss: 0.4962 - val_loss: 0.8562 - val_mywloss: 0.8562\n",
      "Epoch 394/600\n",
      "6277/6277 [==============================] - 0s 65us/step - loss: 0.4868 - mywloss: 0.4868 - val_loss: 0.8659 - val_mywloss: 0.8659\n",
      "Epoch 395/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4958 - mywloss: 0.4958 - val_loss: 0.8402 - val_mywloss: 0.8402\n",
      "Epoch 396/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4955 - mywloss: 0.4955 - val_loss: 0.8705 - val_mywloss: 0.8705\n",
      "Epoch 397/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4810 - mywloss: 0.4810 - val_loss: 0.8798 - val_mywloss: 0.8798\n",
      "Epoch 398/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4912 - mywloss: 0.4912 - val_loss: 0.8877 - val_mywloss: 0.8877\n",
      "Epoch 399/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.5134 - mywloss: 0.5134 - val_loss: 0.8769 - val_mywloss: 0.8769\n",
      "Epoch 400/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4799 - mywloss: 0.4799 - val_loss: 0.8699 - val_mywloss: 0.8699\n",
      "Epoch 401/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4791 - mywloss: 0.4791 - val_loss: 0.8664 - val_mywloss: 0.8664\n",
      "Epoch 402/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4839 - mywloss: 0.4839 - val_loss: 0.9073 - val_mywloss: 0.9073\n",
      "Epoch 403/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4789 - mywloss: 0.4789 - val_loss: 0.8844 - val_mywloss: 0.8844\n",
      "Epoch 404/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4931 - mywloss: 0.4931 - val_loss: 0.8595 - val_mywloss: 0.8595\n",
      "Epoch 405/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4727 - mywloss: 0.4727 - val_loss: 0.8534 - val_mywloss: 0.8534\n",
      "Epoch 406/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4610 - mywloss: 0.4610 - val_loss: 0.8621 - val_mywloss: 0.8621\n",
      "Epoch 407/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.5027 - mywloss: 0.5027 - val_loss: 0.8739 - val_mywloss: 0.8739\n",
      "Epoch 408/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4957 - mywloss: 0.4957 - val_loss: 0.9244 - val_mywloss: 0.9244\n",
      "Epoch 409/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4708 - mywloss: 0.4708 - val_loss: 0.8978 - val_mywloss: 0.8978\n",
      "Epoch 410/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4842 - mywloss: 0.4842 - val_loss: 0.8928 - val_mywloss: 0.8928\n",
      "Epoch 411/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4849 - mywloss: 0.4849 - val_loss: 0.8613 - val_mywloss: 0.8613\n",
      "Epoch 412/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4789 - mywloss: 0.4789 - val_loss: 0.8597 - val_mywloss: 0.8597\n",
      "Epoch 413/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4694 - mywloss: 0.4694 - val_loss: 0.8871 - val_mywloss: 0.8871\n",
      "Epoch 414/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4898 - mywloss: 0.4898 - val_loss: 0.8829 - val_mywloss: 0.8829\n",
      "Epoch 415/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4757 - mywloss: 0.4757 - val_loss: 0.9302 - val_mywloss: 0.9302\n",
      "Epoch 416/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4897 - mywloss: 0.4897 - val_loss: 0.9199 - val_mywloss: 0.9199\n",
      "Epoch 417/600\n",
      "6277/6277 [==============================] - 0s 72us/step - loss: 0.4954 - mywloss: 0.4954 - val_loss: 0.8712 - val_mywloss: 0.8712\n",
      "Epoch 418/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4739 - mywloss: 0.4739 - val_loss: 0.8956 - val_mywloss: 0.8956\n",
      "Epoch 419/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4852 - mywloss: 0.4852 - val_loss: 0.9045 - val_mywloss: 0.9045\n",
      "Epoch 420/600\n",
      "6277/6277 [==============================] - 0s 72us/step - loss: 0.4700 - mywloss: 0.4700 - val_loss: 0.9077 - val_mywloss: 0.9077\n",
      "Epoch 421/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4805 - mywloss: 0.4805 - val_loss: 0.9071 - val_mywloss: 0.9071\n",
      "Epoch 422/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4784 - mywloss: 0.4784 - val_loss: 0.8900 - val_mywloss: 0.8900\n",
      "Epoch 423/600\n",
      "6277/6277 [==============================] - 0s 72us/step - loss: 0.4910 - mywloss: 0.4910 - val_loss: 0.8665 - val_mywloss: 0.8665\n",
      "Epoch 424/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4788 - mywloss: 0.4788 - val_loss: 0.8665 - val_mywloss: 0.8665\n",
      "Epoch 425/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4911 - mywloss: 0.4911 - val_loss: 0.8614 - val_mywloss: 0.8614\n",
      "Epoch 426/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4751 - mywloss: 0.4751 - val_loss: 0.8841 - val_mywloss: 0.8841\n",
      "Epoch 427/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4639 - mywloss: 0.4639 - val_loss: 0.9033 - val_mywloss: 0.9033\n",
      "Epoch 428/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4866 - mywloss: 0.4866 - val_loss: 0.9124 - val_mywloss: 0.9124\n",
      "Epoch 429/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4775 - mywloss: 0.4775 - val_loss: 0.8689 - val_mywloss: 0.8689\n",
      "Epoch 430/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4927 - mywloss: 0.4927 - val_loss: 0.8882 - val_mywloss: 0.8882\n",
      "Epoch 431/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4679 - mywloss: 0.4679 - val_loss: 0.8642 - val_mywloss: 0.8642\n",
      "Epoch 432/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4857 - mywloss: 0.4857 - val_loss: 0.8703 - val_mywloss: 0.8703\n",
      "Epoch 433/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4949 - mywloss: 0.4949 - val_loss: 0.8659 - val_mywloss: 0.8659\n",
      "Epoch 434/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4607 - mywloss: 0.4607 - val_loss: 0.8653 - val_mywloss: 0.8653\n",
      "Epoch 435/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4905 - mywloss: 0.4905 - val_loss: 0.8783 - val_mywloss: 0.8783\n",
      "Epoch 436/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4770 - mywloss: 0.4770 - val_loss: 0.8908 - val_mywloss: 0.8908\n",
      "Epoch 437/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4641 - mywloss: 0.4641 - val_loss: 0.8887 - val_mywloss: 0.8887\n",
      "Epoch 438/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.8621 - val_mywloss: 0.8621\n",
      "Epoch 439/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4531 - mywloss: 0.4531 - val_loss: 0.8732 - val_mywloss: 0.8732\n",
      "Epoch 440/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4668 - mywloss: 0.4668 - val_loss: 0.8825 - val_mywloss: 0.8825\n",
      "Epoch 441/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4775 - mywloss: 0.4775 - val_loss: 0.8953 - val_mywloss: 0.8953\n",
      "Epoch 442/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4659 - mywloss: 0.4659 - val_loss: 0.8685 - val_mywloss: 0.8685\n",
      "Epoch 443/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4801 - mywloss: 0.4801 - val_loss: 0.9023 - val_mywloss: 0.9023\n",
      "Epoch 444/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.9156 - val_mywloss: 0.9156\n",
      "Epoch 445/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4699 - mywloss: 0.4699 - val_loss: 0.8601 - val_mywloss: 0.8601\n",
      "Epoch 446/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4610 - mywloss: 0.4610 - val_loss: 0.8565 - val_mywloss: 0.8565\n",
      "Epoch 447/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4577 - mywloss: 0.4577 - val_loss: 0.8539 - val_mywloss: 0.8539\n",
      "Epoch 448/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4695 - mywloss: 0.4695 - val_loss: 0.8783 - val_mywloss: 0.8783\n",
      "Epoch 449/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4556 - mywloss: 0.4556 - val_loss: 0.8631 - val_mywloss: 0.8631\n",
      "Epoch 450/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4648 - mywloss: 0.4648 - val_loss: 0.8561 - val_mywloss: 0.8561\n",
      "Epoch 451/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4757 - mywloss: 0.4757 - val_loss: 0.8803 - val_mywloss: 0.8803\n",
      "Epoch 452/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4556 - mywloss: 0.4556 - val_loss: 0.8911 - val_mywloss: 0.8911\n",
      "Epoch 453/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4627 - mywloss: 0.4627 - val_loss: 0.8800 - val_mywloss: 0.8800\n",
      "Epoch 454/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4541 - mywloss: 0.4541 - val_loss: 0.8828 - val_mywloss: 0.8828\n",
      "Epoch 455/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4714 - mywloss: 0.4714 - val_loss: 0.9260 - val_mywloss: 0.9260\n",
      "Epoch 456/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4569 - mywloss: 0.4569 - val_loss: 0.9052 - val_mywloss: 0.9052\n",
      "Epoch 457/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4733 - mywloss: 0.4733 - val_loss: 0.9097 - val_mywloss: 0.9097\n",
      "Epoch 458/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4572 - mywloss: 0.4572 - val_loss: 0.9668 - val_mywloss: 0.9668\n",
      "Epoch 459/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4348 - mywloss: 0.4348 - val_loss: 0.9374 - val_mywloss: 0.9374\n",
      "Epoch 460/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4541 - mywloss: 0.4541 - val_loss: 0.9455 - val_mywloss: 0.9455\n",
      "Epoch 461/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4682 - mywloss: 0.4682 - val_loss: 0.9248 - val_mywloss: 0.9248\n",
      "Epoch 462/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4628 - mywloss: 0.4628 - val_loss: 0.9237 - val_mywloss: 0.9237\n",
      "Epoch 463/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4683 - mywloss: 0.4683 - val_loss: 0.8769 - val_mywloss: 0.8769\n",
      "Epoch 464/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4615 - mywloss: 0.4615 - val_loss: 0.9022 - val_mywloss: 0.9022\n",
      "Epoch 465/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4584 - mywloss: 0.4584 - val_loss: 0.9312 - val_mywloss: 0.9312\n",
      "Epoch 466/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4573 - mywloss: 0.4573 - val_loss: 0.9307 - val_mywloss: 0.9307\n",
      "Epoch 467/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4647 - mywloss: 0.4647 - val_loss: 0.8891 - val_mywloss: 0.8891\n",
      "Epoch 468/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4670 - mywloss: 0.4670 - val_loss: 0.9442 - val_mywloss: 0.9442\n",
      "Epoch 469/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4566 - mywloss: 0.4566 - val_loss: 0.9316 - val_mywloss: 0.9316\n",
      "Epoch 470/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4498 - mywloss: 0.4498 - val_loss: 0.9442 - val_mywloss: 0.9442\n",
      "Epoch 471/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4683 - mywloss: 0.4683 - val_loss: 0.9192 - val_mywloss: 0.9192\n",
      "Epoch 472/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4576 - mywloss: 0.4576 - val_loss: 0.9104 - val_mywloss: 0.9104\n",
      "Epoch 473/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4599 - mywloss: 0.4599 - val_loss: 0.9074 - val_mywloss: 0.9074\n",
      "Epoch 474/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4627 - mywloss: 0.4627 - val_loss: 0.9367 - val_mywloss: 0.9367\n",
      "Epoch 475/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4627 - mywloss: 0.4627 - val_loss: 0.9102 - val_mywloss: 0.9102\n",
      "Epoch 476/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4667 - mywloss: 0.4667 - val_loss: 0.9351 - val_mywloss: 0.9351\n",
      "Epoch 477/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4597 - mywloss: 0.4597 - val_loss: 0.9114 - val_mywloss: 0.9114\n",
      "Epoch 478/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4611 - mywloss: 0.4611 - val_loss: 0.9336 - val_mywloss: 0.9336\n",
      "Epoch 479/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4793 - mywloss: 0.4793 - val_loss: 0.8877 - val_mywloss: 0.8877\n",
      "Epoch 480/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4596 - mywloss: 0.4596 - val_loss: 0.9527 - val_mywloss: 0.9527\n",
      "Epoch 481/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4525 - mywloss: 0.4525 - val_loss: 0.9194 - val_mywloss: 0.9194\n",
      "Epoch 482/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4671 - mywloss: 0.4671 - val_loss: 0.9280 - val_mywloss: 0.9280\n",
      "Epoch 483/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4758 - mywloss: 0.4758 - val_loss: 0.9333 - val_mywloss: 0.9333\n",
      "Epoch 484/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4722 - mywloss: 0.4722 - val_loss: 0.9328 - val_mywloss: 0.9328\n",
      "Epoch 485/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4744 - mywloss: 0.4744 - val_loss: 0.9217 - val_mywloss: 0.9217\n",
      "Epoch 486/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4604 - mywloss: 0.4604 - val_loss: 0.9443 - val_mywloss: 0.9443\n",
      "Epoch 487/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4684 - mywloss: 0.4684 - val_loss: 0.9097 - val_mywloss: 0.9097\n",
      "Epoch 488/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4475 - mywloss: 0.4475 - val_loss: 0.8983 - val_mywloss: 0.8983\n",
      "Epoch 489/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4456 - mywloss: 0.4456 - val_loss: 0.9120 - val_mywloss: 0.9120\n",
      "Epoch 490/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4375 - mywloss: 0.4375 - val_loss: 0.8996 - val_mywloss: 0.8996\n",
      "Epoch 491/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4510 - mywloss: 0.4510 - val_loss: 0.9023 - val_mywloss: 0.9023\n",
      "Epoch 492/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4597 - mywloss: 0.4597 - val_loss: 0.9119 - val_mywloss: 0.9119\n",
      "Epoch 493/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4565 - mywloss: 0.4565 - val_loss: 0.8947 - val_mywloss: 0.8947\n",
      "Epoch 494/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4498 - mywloss: 0.4498 - val_loss: 0.9004 - val_mywloss: 0.9004\n",
      "Epoch 495/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4377 - mywloss: 0.4377 - val_loss: 0.9372 - val_mywloss: 0.9372\n",
      "Epoch 496/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4516 - mywloss: 0.4516 - val_loss: 0.9042 - val_mywloss: 0.9042\n",
      "Epoch 497/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4536 - mywloss: 0.4536 - val_loss: 0.9103 - val_mywloss: 0.9103\n",
      "Epoch 498/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4387 - mywloss: 0.4387 - val_loss: 0.9296 - val_mywloss: 0.9296\n",
      "Epoch 499/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4587 - mywloss: 0.4587 - val_loss: 0.9355 - val_mywloss: 0.9355\n",
      "Epoch 500/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4524 - mywloss: 0.4524 - val_loss: 0.9683 - val_mywloss: 0.9683\n",
      "Epoch 501/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4520 - mywloss: 0.4520 - val_loss: 0.9425 - val_mywloss: 0.9425\n",
      "Epoch 502/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4320 - mywloss: 0.4320 - val_loss: 0.9414 - val_mywloss: 0.9414\n",
      "Epoch 503/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4462 - mywloss: 0.4462 - val_loss: 0.9430 - val_mywloss: 0.9430\n",
      "Epoch 504/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4474 - mywloss: 0.4474 - val_loss: 0.9265 - val_mywloss: 0.9265\n",
      "Epoch 505/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4505 - mywloss: 0.4505 - val_loss: 0.9491 - val_mywloss: 0.9491\n",
      "Epoch 506/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4223 - mywloss: 0.4223 - val_loss: 0.9654 - val_mywloss: 0.9654\n",
      "Epoch 507/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4527 - mywloss: 0.4527 - val_loss: 0.9329 - val_mywloss: 0.9329\n",
      "Epoch 508/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4595 - mywloss: 0.4595 - val_loss: 0.9500 - val_mywloss: 0.9500\n",
      "Epoch 509/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4370 - mywloss: 0.4370 - val_loss: 0.9613 - val_mywloss: 0.9613\n",
      "Epoch 510/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4528 - mywloss: 0.4528 - val_loss: 0.9239 - val_mywloss: 0.9239\n",
      "Epoch 511/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4565 - mywloss: 0.4565 - val_loss: 0.9215 - val_mywloss: 0.9215\n",
      "Epoch 512/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4530 - mywloss: 0.4530 - val_loss: 0.9381 - val_mywloss: 0.9381\n",
      "Epoch 513/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4476 - mywloss: 0.4476 - val_loss: 0.9319 - val_mywloss: 0.9319\n",
      "Epoch 514/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4476 - mywloss: 0.4476 - val_loss: 0.9105 - val_mywloss: 0.9105\n",
      "Epoch 515/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4713 - mywloss: 0.4713 - val_loss: 0.9116 - val_mywloss: 0.9116\n",
      "Epoch 516/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4480 - mywloss: 0.4480 - val_loss: 0.9739 - val_mywloss: 0.9739\n",
      "Epoch 517/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4399 - mywloss: 0.4399 - val_loss: 0.9336 - val_mywloss: 0.9336\n",
      "Epoch 518/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4521 - mywloss: 0.4521 - val_loss: 0.9110 - val_mywloss: 0.9110\n",
      "Epoch 519/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4281 - mywloss: 0.4281 - val_loss: 0.9448 - val_mywloss: 0.9448\n",
      "Epoch 520/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4213 - mywloss: 0.4213 - val_loss: 0.9530 - val_mywloss: 0.9530\n",
      "Epoch 521/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4451 - mywloss: 0.4451 - val_loss: 0.9140 - val_mywloss: 0.9140\n",
      "Epoch 522/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4657 - mywloss: 0.4657 - val_loss: 0.9175 - val_mywloss: 0.9175\n",
      "Epoch 523/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4560 - mywloss: 0.4560 - val_loss: 0.9047 - val_mywloss: 0.9047\n",
      "Epoch 524/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4343 - mywloss: 0.4343 - val_loss: 0.8886 - val_mywloss: 0.8886\n",
      "Epoch 525/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4293 - mywloss: 0.4293 - val_loss: 0.9043 - val_mywloss: 0.9043\n",
      "Epoch 526/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4245 - mywloss: 0.4245 - val_loss: 0.9085 - val_mywloss: 0.9085\n",
      "Epoch 527/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4502 - mywloss: 0.4502 - val_loss: 0.9667 - val_mywloss: 0.9667\n",
      "Epoch 528/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4517 - mywloss: 0.4517 - val_loss: 0.9486 - val_mywloss: 0.9486\n",
      "Epoch 529/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4659 - mywloss: 0.4659 - val_loss: 0.9109 - val_mywloss: 0.9109\n",
      "Epoch 530/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4473 - mywloss: 0.4473 - val_loss: 0.8781 - val_mywloss: 0.8781\n",
      "Epoch 531/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4603 - mywloss: 0.4603 - val_loss: 0.9076 - val_mywloss: 0.9076\n",
      "Epoch 532/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4205 - mywloss: 0.4205 - val_loss: 0.8977 - val_mywloss: 0.8977\n",
      "Epoch 533/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4563 - mywloss: 0.4563 - val_loss: 0.8832 - val_mywloss: 0.8832\n",
      "Epoch 534/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4477 - mywloss: 0.4477 - val_loss: 0.9292 - val_mywloss: 0.9292\n",
      "Epoch 535/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4324 - mywloss: 0.4324 - val_loss: 0.9032 - val_mywloss: 0.9032\n",
      "Epoch 536/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4405 - mywloss: 0.4405 - val_loss: 0.8910 - val_mywloss: 0.8910\n",
      "Epoch 537/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4515 - mywloss: 0.4515 - val_loss: 0.9120 - val_mywloss: 0.9120\n",
      "Epoch 538/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4427 - mywloss: 0.4427 - val_loss: 0.8807 - val_mywloss: 0.8807\n",
      "Epoch 539/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4485 - mywloss: 0.4485 - val_loss: 0.9201 - val_mywloss: 0.9201\n",
      "Epoch 540/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4455 - mywloss: 0.4455 - val_loss: 0.9321 - val_mywloss: 0.9321\n",
      "Epoch 541/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4486 - mywloss: 0.4486 - val_loss: 0.9356 - val_mywloss: 0.9356\n",
      "Epoch 542/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4447 - mywloss: 0.4447 - val_loss: 0.9125 - val_mywloss: 0.9125\n",
      "Epoch 543/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4456 - mywloss: 0.4456 - val_loss: 0.9073 - val_mywloss: 0.9073\n",
      "Epoch 544/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4477 - mywloss: 0.4477 - val_loss: 0.9336 - val_mywloss: 0.9336\n",
      "Epoch 545/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4600 - mywloss: 0.4600 - val_loss: 0.9026 - val_mywloss: 0.9026\n",
      "Epoch 546/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4442 - mywloss: 0.4442 - val_loss: 0.9562 - val_mywloss: 0.9562\n",
      "Epoch 547/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4400 - mywloss: 0.4400 - val_loss: 0.9392 - val_mywloss: 0.9392\n",
      "Epoch 548/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4431 - mywloss: 0.4431 - val_loss: 0.9081 - val_mywloss: 0.9081\n",
      "Epoch 549/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4477 - mywloss: 0.4477 - val_loss: 0.9281 - val_mywloss: 0.9281\n",
      "Epoch 550/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4552 - mywloss: 0.4552 - val_loss: 0.9126 - val_mywloss: 0.9126\n",
      "Epoch 551/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4305 - mywloss: 0.4305 - val_loss: 0.8939 - val_mywloss: 0.8939\n",
      "Epoch 552/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4542 - mywloss: 0.4542 - val_loss: 0.8963 - val_mywloss: 0.8963\n",
      "Epoch 553/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4395 - mywloss: 0.4395 - val_loss: 0.9205 - val_mywloss: 0.9205\n",
      "Epoch 554/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4517 - mywloss: 0.4517 - val_loss: 0.9381 - val_mywloss: 0.9381\n",
      "Epoch 555/600\n",
      "6277/6277 [==============================] - 0s 66us/step - loss: 0.4421 - mywloss: 0.4421 - val_loss: 0.9131 - val_mywloss: 0.9131\n",
      "Epoch 556/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4368 - mywloss: 0.4368 - val_loss: 0.9075 - val_mywloss: 0.9075\n",
      "Epoch 557/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4191 - mywloss: 0.4191 - val_loss: 0.9586 - val_mywloss: 0.9586\n",
      "Epoch 558/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4401 - mywloss: 0.4401 - val_loss: 0.9437 - val_mywloss: 0.9437\n",
      "Epoch 559/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4346 - mywloss: 0.4346 - val_loss: 0.9429 - val_mywloss: 0.9429\n",
      "Epoch 560/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4494 - mywloss: 0.4494 - val_loss: 0.9389 - val_mywloss: 0.9389\n",
      "Epoch 561/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4437 - mywloss: 0.4437 - val_loss: 0.9569 - val_mywloss: 0.9569\n",
      "Epoch 562/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4255 - mywloss: 0.4255 - val_loss: 0.9426 - val_mywloss: 0.9426\n",
      "Epoch 563/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4293 - mywloss: 0.4293 - val_loss: 0.9574 - val_mywloss: 0.9574\n",
      "Epoch 564/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4365 - mywloss: 0.4365 - val_loss: 0.9574 - val_mywloss: 0.9574\n",
      "Epoch 565/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4224 - mywloss: 0.4224 - val_loss: 0.9439 - val_mywloss: 0.9439\n",
      "Epoch 566/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4492 - mywloss: 0.4492 - val_loss: 0.9641 - val_mywloss: 0.9641\n",
      "Epoch 567/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4394 - mywloss: 0.4394 - val_loss: 0.9605 - val_mywloss: 0.9605\n",
      "Epoch 568/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4221 - mywloss: 0.4221 - val_loss: 0.9578 - val_mywloss: 0.9578\n",
      "Epoch 569/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4384 - mywloss: 0.4384 - val_loss: 0.9464 - val_mywloss: 0.9464\n",
      "Epoch 570/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4370 - mywloss: 0.4370 - val_loss: 0.9428 - val_mywloss: 0.9428\n",
      "Epoch 571/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4122 - mywloss: 0.4122 - val_loss: 0.9689 - val_mywloss: 0.9689\n",
      "Epoch 572/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4313 - mywloss: 0.4313 - val_loss: 0.9409 - val_mywloss: 0.9409\n",
      "Epoch 573/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4317 - mywloss: 0.4317 - val_loss: 0.9207 - val_mywloss: 0.9207\n",
      "Epoch 574/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4170 - mywloss: 0.4170 - val_loss: 0.9078 - val_mywloss: 0.9078\n",
      "Epoch 575/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4468 - mywloss: 0.4468 - val_loss: 0.9133 - val_mywloss: 0.9133\n",
      "Epoch 576/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4240 - mywloss: 0.4240 - val_loss: 0.9310 - val_mywloss: 0.9310\n",
      "Epoch 577/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4371 - mywloss: 0.4371 - val_loss: 0.9667 - val_mywloss: 0.9667\n",
      "Epoch 578/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4277 - mywloss: 0.4277 - val_loss: 0.9477 - val_mywloss: 0.9477\n",
      "Epoch 579/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4298 - mywloss: 0.4298 - val_loss: 0.9738 - val_mywloss: 0.9738\n",
      "Epoch 580/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4241 - mywloss: 0.4241 - val_loss: 0.9830 - val_mywloss: 0.9830\n",
      "Epoch 581/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4344 - mywloss: 0.4344 - val_loss: 0.9942 - val_mywloss: 0.9942\n",
      "Epoch 582/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4393 - mywloss: 0.4393 - val_loss: 0.9772 - val_mywloss: 0.9772\n",
      "Epoch 583/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4335 - mywloss: 0.4335 - val_loss: 0.9737 - val_mywloss: 0.9737\n",
      "Epoch 584/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4404 - mywloss: 0.4404 - val_loss: 0.9627 - val_mywloss: 0.9627\n",
      "Epoch 585/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4250 - mywloss: 0.4250 - val_loss: 0.9597 - val_mywloss: 0.9597\n",
      "Epoch 586/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4094 - mywloss: 0.4094 - val_loss: 0.9827 - val_mywloss: 0.9827\n",
      "Epoch 587/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4267 - mywloss: 0.4267 - val_loss: 0.9590 - val_mywloss: 0.9590\n",
      "Epoch 588/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4247 - mywloss: 0.4247 - val_loss: 0.9528 - val_mywloss: 0.9528\n",
      "Epoch 589/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4276 - mywloss: 0.4276 - val_loss: 0.9722 - val_mywloss: 0.9722\n",
      "Epoch 590/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4275 - mywloss: 0.4275 - val_loss: 0.9497 - val_mywloss: 0.9497\n",
      "Epoch 591/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4393 - mywloss: 0.4393 - val_loss: 0.9649 - val_mywloss: 0.9649\n",
      "Epoch 592/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4319 - mywloss: 0.4319 - val_loss: 0.9307 - val_mywloss: 0.9307\n",
      "Epoch 593/600\n",
      "6277/6277 [==============================] - 0s 67us/step - loss: 0.4397 - mywloss: 0.4397 - val_loss: 0.9838 - val_mywloss: 0.9838\n",
      "Epoch 594/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4292 - mywloss: 0.4292 - val_loss: 0.9883 - val_mywloss: 0.9883\n",
      "Epoch 595/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4478 - mywloss: 0.4478 - val_loss: 0.9653 - val_mywloss: 0.9653\n",
      "Epoch 596/600\n",
      "6277/6277 [==============================] - 0s 71us/step - loss: 0.4197 - mywloss: 0.4197 - val_loss: 0.9431 - val_mywloss: 0.9431\n",
      "Epoch 597/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4121 - mywloss: 0.4121 - val_loss: 0.9516 - val_mywloss: 0.9516\n",
      "Epoch 598/600\n",
      "6277/6277 [==============================] - 0s 70us/step - loss: 0.4458 - mywloss: 0.4458 - val_loss: 0.9612 - val_mywloss: 0.9612\n",
      "Epoch 599/600\n",
      "6277/6277 [==============================] - 0s 68us/step - loss: 0.4325 - mywloss: 0.4325 - val_loss: 0.9540 - val_mywloss: 0.9540\n",
      "Epoch 600/600\n",
      "6277/6277 [==============================] - 0s 69us/step - loss: 0.4141 - mywloss: 0.4141 - val_loss: 0.9420 - val_mywloss: 0.9420\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb7004b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model\n",
      "0.7712385277179443\n",
      "Train on 6281 samples, validate on 1567 samples\n",
      "Epoch 1/600\n",
      "6281/6281 [==============================] - 2s 258us/step - loss: 2.5296 - mywloss: 2.5296 - val_loss: 1.4091 - val_mywloss: 1.4091\n",
      "Epoch 2/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.9035 - mywloss: 1.9035 - val_loss: 1.2450 - val_mywloss: 1.2450\n",
      "Epoch 3/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.6887 - mywloss: 1.6887 - val_loss: 1.1231 - val_mywloss: 1.1231\n",
      "Epoch 4/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.5318 - mywloss: 1.5318 - val_loss: 1.0368 - val_mywloss: 1.0368\n",
      "Epoch 5/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.4189 - mywloss: 1.4189 - val_loss: 1.0377 - val_mywloss: 1.0377\n",
      "Epoch 6/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.3835 - mywloss: 1.3835 - val_loss: 0.9594 - val_mywloss: 0.9594\n",
      "Epoch 7/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.3010 - mywloss: 1.3010 - val_loss: 0.9451 - val_mywloss: 0.9451\n",
      "Epoch 8/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.2569 - mywloss: 1.2569 - val_loss: 0.9295 - val_mywloss: 0.9295\n",
      "Epoch 9/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.2125 - mywloss: 1.2125 - val_loss: 0.8993 - val_mywloss: 0.8993\n",
      "Epoch 10/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.1966 - mywloss: 1.1966 - val_loss: 0.8820 - val_mywloss: 0.8820\n",
      "Epoch 11/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.1639 - mywloss: 1.1639 - val_loss: 0.8592 - val_mywloss: 0.8592\n",
      "Epoch 12/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.1289 - mywloss: 1.1289 - val_loss: 0.8568 - val_mywloss: 0.8568\n",
      "Epoch 13/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.0908 - mywloss: 1.0908 - val_loss: 0.8337 - val_mywloss: 0.8337\n",
      "Epoch 14/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 1.0877 - mywloss: 1.0877 - val_loss: 0.8271 - val_mywloss: 0.8271\n",
      "Epoch 15/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.0561 - mywloss: 1.0561 - val_loss: 0.8108 - val_mywloss: 0.8108\n",
      "Epoch 16/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 1.0402 - mywloss: 1.0402 - val_loss: 0.8145 - val_mywloss: 0.8145\n",
      "Epoch 17/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 1.0194 - mywloss: 1.0194 - val_loss: 0.8143 - val_mywloss: 0.8143\n",
      "Epoch 18/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 1.0115 - mywloss: 1.0115 - val_loss: 0.8253 - val_mywloss: 0.8253\n",
      "Epoch 19/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.9979 - mywloss: 0.9979 - val_loss: 0.8102 - val_mywloss: 0.8102\n",
      "Epoch 20/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9935 - mywloss: 0.9935 - val_loss: 0.8194 - val_mywloss: 0.8194\n",
      "Epoch 21/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9885 - mywloss: 0.9885 - val_loss: 0.8108 - val_mywloss: 0.8108\n",
      "Epoch 22/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9722 - mywloss: 0.9722 - val_loss: 0.8237 - val_mywloss: 0.8237\n",
      "Epoch 23/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9394 - mywloss: 0.9394 - val_loss: 0.7976 - val_mywloss: 0.7976\n",
      "Epoch 24/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.9257 - mywloss: 0.9257 - val_loss: 0.8040 - val_mywloss: 0.8040\n",
      "Epoch 25/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9126 - mywloss: 0.9126 - val_loss: 0.8045 - val_mywloss: 0.8045\n",
      "Epoch 26/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9290 - mywloss: 0.9290 - val_loss: 0.7893 - val_mywloss: 0.7893\n",
      "Epoch 27/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.9248 - mywloss: 0.9248 - val_loss: 0.7920 - val_mywloss: 0.7920\n",
      "Epoch 28/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8936 - mywloss: 0.8936 - val_loss: 0.7964 - val_mywloss: 0.7964\n",
      "Epoch 29/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.9159 - mywloss: 0.9159 - val_loss: 0.7961 - val_mywloss: 0.7961\n",
      "Epoch 30/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.9208 - mywloss: 0.9208 - val_loss: 0.7673 - val_mywloss: 0.7673\n",
      "Epoch 31/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8767 - mywloss: 0.8767 - val_loss: 0.7889 - val_mywloss: 0.7889\n",
      "Epoch 32/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8663 - mywloss: 0.8663 - val_loss: 0.7605 - val_mywloss: 0.7605\n",
      "Epoch 33/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8674 - mywloss: 0.8674 - val_loss: 0.7729 - val_mywloss: 0.7729\n",
      "Epoch 34/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8612 - mywloss: 0.8612 - val_loss: 0.7667 - val_mywloss: 0.7667\n",
      "Epoch 35/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8700 - mywloss: 0.8700 - val_loss: 0.7884 - val_mywloss: 0.7884\n",
      "Epoch 36/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8574 - mywloss: 0.8574 - val_loss: 0.7804 - val_mywloss: 0.7804\n",
      "Epoch 37/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8548 - mywloss: 0.8548 - val_loss: 0.7911 - val_mywloss: 0.7911\n",
      "Epoch 38/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8529 - mywloss: 0.8529 - val_loss: 0.7763 - val_mywloss: 0.7763\n",
      "Epoch 39/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8519 - mywloss: 0.8519 - val_loss: 0.7762 - val_mywloss: 0.7762\n",
      "Epoch 40/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8275 - mywloss: 0.8275 - val_loss: 0.7484 - val_mywloss: 0.7484\n",
      "Epoch 41/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8462 - mywloss: 0.8462 - val_loss: 0.7556 - val_mywloss: 0.7556\n",
      "Epoch 42/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8498 - mywloss: 0.8498 - val_loss: 0.7422 - val_mywloss: 0.7422\n",
      "Epoch 43/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8298 - mywloss: 0.8298 - val_loss: 0.7456 - val_mywloss: 0.7456\n",
      "Epoch 44/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8474 - mywloss: 0.8474 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 45/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8108 - mywloss: 0.8108 - val_loss: 0.7505 - val_mywloss: 0.7505\n",
      "Epoch 46/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8185 - mywloss: 0.8185 - val_loss: 0.7430 - val_mywloss: 0.7430\n",
      "Epoch 47/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8365 - mywloss: 0.8365 - val_loss: 0.7573 - val_mywloss: 0.7573\n",
      "Epoch 48/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.8087 - mywloss: 0.8087 - val_loss: 0.7619 - val_mywloss: 0.7619\n",
      "Epoch 49/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8253 - mywloss: 0.8253 - val_loss: 0.7543 - val_mywloss: 0.7543\n",
      "Epoch 50/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7949 - mywloss: 0.7949 - val_loss: 0.7699 - val_mywloss: 0.7699\n",
      "Epoch 51/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.8187 - mywloss: 0.8187 - val_loss: 0.7571 - val_mywloss: 0.7571\n",
      "Epoch 52/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7985 - mywloss: 0.7985 - val_loss: 0.7428 - val_mywloss: 0.7428\n",
      "Epoch 53/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7993 - mywloss: 0.7993 - val_loss: 0.7584 - val_mywloss: 0.7584\n",
      "Epoch 54/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.8016 - mywloss: 0.8016 - val_loss: 0.7625 - val_mywloss: 0.7625\n",
      "Epoch 55/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7796 - mywloss: 0.7796 - val_loss: 0.7291 - val_mywloss: 0.7291\n",
      "Epoch 56/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7811 - mywloss: 0.7811 - val_loss: 0.7324 - val_mywloss: 0.7324\n",
      "Epoch 57/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7584 - mywloss: 0.7584 - val_loss: 0.7435 - val_mywloss: 0.7435\n",
      "Epoch 58/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7692 - mywloss: 0.7692 - val_loss: 0.7468 - val_mywloss: 0.7468\n",
      "Epoch 59/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7769 - mywloss: 0.7769 - val_loss: 0.7492 - val_mywloss: 0.7492\n",
      "Epoch 60/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7818 - mywloss: 0.7818 - val_loss: 0.7503 - val_mywloss: 0.7503\n",
      "Epoch 61/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7694 - mywloss: 0.7694 - val_loss: 0.7454 - val_mywloss: 0.7454\n",
      "Epoch 62/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7644 - mywloss: 0.7644 - val_loss: 0.7490 - val_mywloss: 0.7490\n",
      "Epoch 63/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7743 - mywloss: 0.7743 - val_loss: 0.7578 - val_mywloss: 0.7578\n",
      "Epoch 64/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7710 - mywloss: 0.7710 - val_loss: 0.7551 - val_mywloss: 0.7551\n",
      "Epoch 65/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7692 - mywloss: 0.7692 - val_loss: 0.7411 - val_mywloss: 0.7411\n",
      "Epoch 66/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7893 - mywloss: 0.7893 - val_loss: 0.7914 - val_mywloss: 0.7914\n",
      "Epoch 67/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7545 - mywloss: 0.7545 - val_loss: 0.7528 - val_mywloss: 0.7528\n",
      "Epoch 68/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7442 - mywloss: 0.7442 - val_loss: 0.7399 - val_mywloss: 0.7399\n",
      "Epoch 69/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7487 - mywloss: 0.7487 - val_loss: 0.7414 - val_mywloss: 0.7414\n",
      "Epoch 70/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7280 - mywloss: 0.7280 - val_loss: 0.7558 - val_mywloss: 0.7558\n",
      "Epoch 71/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.7450 - mywloss: 0.7450 - val_loss: 0.7462 - val_mywloss: 0.7462\n",
      "Epoch 72/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7265 - mywloss: 0.7265 - val_loss: 0.7461 - val_mywloss: 0.7461\n",
      "Epoch 73/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.7358 - mywloss: 0.7358 - val_loss: 0.7632 - val_mywloss: 0.7632\n",
      "Epoch 74/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7345 - mywloss: 0.7345 - val_loss: 0.7491 - val_mywloss: 0.7491\n",
      "Epoch 75/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7597 - mywloss: 0.7597 - val_loss: 0.7497 - val_mywloss: 0.7497\n",
      "Epoch 76/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7366 - mywloss: 0.7366 - val_loss: 0.7388 - val_mywloss: 0.7388\n",
      "Epoch 77/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7330 - mywloss: 0.7330 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 78/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7314 - mywloss: 0.7314 - val_loss: 0.7550 - val_mywloss: 0.7550\n",
      "Epoch 79/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7243 - mywloss: 0.7243 - val_loss: 0.7491 - val_mywloss: 0.7491\n",
      "Epoch 80/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7365 - mywloss: 0.7365 - val_loss: 0.7698 - val_mywloss: 0.7698\n",
      "Epoch 81/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7374 - mywloss: 0.7374 - val_loss: 0.7582 - val_mywloss: 0.7582\n",
      "Epoch 82/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7179 - mywloss: 0.7179 - val_loss: 0.7671 - val_mywloss: 0.7671\n",
      "Epoch 83/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7313 - mywloss: 0.7313 - val_loss: 0.7690 - val_mywloss: 0.7690\n",
      "Epoch 84/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7311 - mywloss: 0.7311 - val_loss: 0.8059 - val_mywloss: 0.8059\n",
      "Epoch 85/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7088 - mywloss: 0.7088 - val_loss: 0.7777 - val_mywloss: 0.7777\n",
      "Epoch 86/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7125 - mywloss: 0.7125 - val_loss: 0.7625 - val_mywloss: 0.7625\n",
      "Epoch 87/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7067 - mywloss: 0.7067 - val_loss: 0.7528 - val_mywloss: 0.7528\n",
      "Epoch 88/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6934 - mywloss: 0.6934 - val_loss: 0.7603 - val_mywloss: 0.7603\n",
      "Epoch 89/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.7018 - mywloss: 0.7018 - val_loss: 0.7310 - val_mywloss: 0.7310\n",
      "Epoch 90/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7072 - mywloss: 0.7072 - val_loss: 0.7424 - val_mywloss: 0.7424\n",
      "Epoch 91/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7111 - mywloss: 0.7111 - val_loss: 0.7254 - val_mywloss: 0.7254\n",
      "Epoch 92/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6991 - mywloss: 0.6991 - val_loss: 0.7340 - val_mywloss: 0.7340\n",
      "Epoch 93/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7262 - mywloss: 0.7262 - val_loss: 0.7353 - val_mywloss: 0.7353\n",
      "Epoch 94/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7048 - mywloss: 0.7048 - val_loss: 0.7112 - val_mywloss: 0.7112\n",
      "Epoch 95/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7182 - mywloss: 0.7182 - val_loss: 0.7415 - val_mywloss: 0.7415\n",
      "Epoch 96/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7048 - mywloss: 0.7048 - val_loss: 0.7456 - val_mywloss: 0.7456\n",
      "Epoch 97/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7023 - mywloss: 0.7023 - val_loss: 0.7220 - val_mywloss: 0.7220\n",
      "Epoch 98/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.7008 - mywloss: 0.7008 - val_loss: 0.7180 - val_mywloss: 0.7180\n",
      "Epoch 99/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6764 - mywloss: 0.6764 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 100/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.6966 - mywloss: 0.6966 - val_loss: 0.7419 - val_mywloss: 0.7419\n",
      "Epoch 101/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6744 - mywloss: 0.6744 - val_loss: 0.7204 - val_mywloss: 0.7204\n",
      "Epoch 102/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6877 - mywloss: 0.6877 - val_loss: 0.7275 - val_mywloss: 0.7275\n",
      "Epoch 103/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6742 - mywloss: 0.6742 - val_loss: 0.7256 - val_mywloss: 0.7256\n",
      "Epoch 104/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.6704 - mywloss: 0.6704 - val_loss: 0.7536 - val_mywloss: 0.7536\n",
      "Epoch 105/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.6822 - mywloss: 0.6822 - val_loss: 0.7558 - val_mywloss: 0.7558\n",
      "Epoch 106/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6854 - mywloss: 0.6854 - val_loss: 0.7359 - val_mywloss: 0.7359\n",
      "Epoch 107/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6610 - mywloss: 0.6610 - val_loss: 0.7449 - val_mywloss: 0.7449\n",
      "Epoch 108/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6693 - mywloss: 0.6693 - val_loss: 0.7581 - val_mywloss: 0.7581\n",
      "Epoch 109/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.6667 - mywloss: 0.6667 - val_loss: 0.7752 - val_mywloss: 0.7752\n",
      "Epoch 110/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6637 - mywloss: 0.6637 - val_loss: 0.7517 - val_mywloss: 0.7517\n",
      "Epoch 111/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6861 - mywloss: 0.6861 - val_loss: 0.7599 - val_mywloss: 0.7599\n",
      "Epoch 112/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.6642 - mywloss: 0.6642 - val_loss: 0.7378 - val_mywloss: 0.7378\n",
      "Epoch 113/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6726 - mywloss: 0.6726 - val_loss: 0.7508 - val_mywloss: 0.7508\n",
      "Epoch 114/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6730 - mywloss: 0.6730 - val_loss: 0.7572 - val_mywloss: 0.7572\n",
      "Epoch 115/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6656 - mywloss: 0.6656 - val_loss: 0.7526 - val_mywloss: 0.7526\n",
      "Epoch 116/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6663 - mywloss: 0.6663 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 117/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.6594 - mywloss: 0.6594 - val_loss: 0.7349 - val_mywloss: 0.7349\n",
      "Epoch 118/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6582 - mywloss: 0.6582 - val_loss: 0.7693 - val_mywloss: 0.7693\n",
      "Epoch 119/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6545 - mywloss: 0.6545 - val_loss: 0.7347 - val_mywloss: 0.7347\n",
      "Epoch 120/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6474 - mywloss: 0.6474 - val_loss: 0.7382 - val_mywloss: 0.7382\n",
      "Epoch 121/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6729 - mywloss: 0.6729 - val_loss: 0.7641 - val_mywloss: 0.7641\n",
      "Epoch 122/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6585 - mywloss: 0.6585 - val_loss: 0.7267 - val_mywloss: 0.7267\n",
      "Epoch 123/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6943 - mywloss: 0.6943 - val_loss: 0.7358 - val_mywloss: 0.7358\n",
      "Epoch 124/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6613 - mywloss: 0.6613 - val_loss: 0.7495 - val_mywloss: 0.7495\n",
      "Epoch 125/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6766 - mywloss: 0.6766 - val_loss: 0.7532 - val_mywloss: 0.7532\n",
      "Epoch 126/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6699 - mywloss: 0.6699 - val_loss: 0.7451 - val_mywloss: 0.7451\n",
      "Epoch 127/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6414 - mywloss: 0.6414 - val_loss: 0.7542 - val_mywloss: 0.7542\n",
      "Epoch 128/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6458 - mywloss: 0.6458 - val_loss: 0.7326 - val_mywloss: 0.7326\n",
      "Epoch 129/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6777 - mywloss: 0.6777 - val_loss: 0.7227 - val_mywloss: 0.7227\n",
      "Epoch 130/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6438 - mywloss: 0.6438 - val_loss: 0.7426 - val_mywloss: 0.7426\n",
      "Epoch 131/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6466 - mywloss: 0.6466 - val_loss: 0.7531 - val_mywloss: 0.7531\n",
      "Epoch 132/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.6533 - mywloss: 0.6533 - val_loss: 0.7384 - val_mywloss: 0.7384\n",
      "Epoch 133/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6452 - mywloss: 0.6452 - val_loss: 0.7624 - val_mywloss: 0.7624\n",
      "Epoch 134/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.6521 - mywloss: 0.6521 - val_loss: 0.7896 - val_mywloss: 0.7896\n",
      "Epoch 135/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6655 - mywloss: 0.6655 - val_loss: 0.7419 - val_mywloss: 0.7419\n",
      "Epoch 136/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6591 - mywloss: 0.6591 - val_loss: 0.7304 - val_mywloss: 0.7304\n",
      "Epoch 137/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6697 - mywloss: 0.6697 - val_loss: 0.7386 - val_mywloss: 0.7386\n",
      "Epoch 138/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6288 - mywloss: 0.6288 - val_loss: 0.7489 - val_mywloss: 0.7489\n",
      "Epoch 139/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6502 - mywloss: 0.6502 - val_loss: 0.7232 - val_mywloss: 0.7232\n",
      "Epoch 140/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6185 - mywloss: 0.6185 - val_loss: 0.7477 - val_mywloss: 0.7477\n",
      "Epoch 141/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6402 - mywloss: 0.6402 - val_loss: 0.7342 - val_mywloss: 0.7342\n",
      "Epoch 142/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6465 - mywloss: 0.6465 - val_loss: 0.7235 - val_mywloss: 0.7235\n",
      "Epoch 143/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6152 - mywloss: 0.6152 - val_loss: 0.7505 - val_mywloss: 0.7505\n",
      "Epoch 144/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6290 - mywloss: 0.6290 - val_loss: 0.7450 - val_mywloss: 0.7450\n",
      "Epoch 145/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6445 - mywloss: 0.6445 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 146/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6545 - mywloss: 0.6545 - val_loss: 0.7263 - val_mywloss: 0.7263\n",
      "Epoch 147/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6378 - mywloss: 0.6378 - val_loss: 0.7407 - val_mywloss: 0.7407\n",
      "Epoch 148/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.6447 - mywloss: 0.6447 - val_loss: 0.7478 - val_mywloss: 0.7478\n",
      "Epoch 149/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6553 - mywloss: 0.6553 - val_loss: 0.7540 - val_mywloss: 0.7540\n",
      "Epoch 150/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6364 - mywloss: 0.6364 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 151/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6323 - mywloss: 0.6323 - val_loss: 0.7446 - val_mywloss: 0.7446\n",
      "Epoch 152/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6288 - mywloss: 0.6288 - val_loss: 0.7413 - val_mywloss: 0.7413\n",
      "Epoch 153/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6324 - mywloss: 0.6324 - val_loss: 0.7514 - val_mywloss: 0.7514\n",
      "Epoch 154/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6158 - mywloss: 0.6158 - val_loss: 0.7513 - val_mywloss: 0.7513\n",
      "Epoch 155/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6459 - mywloss: 0.6459 - val_loss: 0.7356 - val_mywloss: 0.7356\n",
      "Epoch 156/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6363 - mywloss: 0.6363 - val_loss: 0.7481 - val_mywloss: 0.7481\n",
      "Epoch 157/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6256 - mywloss: 0.6256 - val_loss: 0.7523 - val_mywloss: 0.7523\n",
      "Epoch 158/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6202 - mywloss: 0.6202 - val_loss: 0.7610 - val_mywloss: 0.7610\n",
      "Epoch 159/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6153 - mywloss: 0.6153 - val_loss: 0.7813 - val_mywloss: 0.7813\n",
      "Epoch 160/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6121 - mywloss: 0.6121 - val_loss: 0.7783 - val_mywloss: 0.7783\n",
      "Epoch 161/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.6414 - mywloss: 0.6414 - val_loss: 0.7199 - val_mywloss: 0.7199\n",
      "Epoch 162/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6371 - mywloss: 0.6371 - val_loss: 0.7273 - val_mywloss: 0.7273\n",
      "Epoch 163/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6243 - mywloss: 0.6243 - val_loss: 0.7329 - val_mywloss: 0.7329\n",
      "Epoch 164/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6121 - mywloss: 0.6121 - val_loss: 0.7626 - val_mywloss: 0.7626\n",
      "Epoch 165/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6168 - mywloss: 0.6168 - val_loss: 0.7578 - val_mywloss: 0.7578\n",
      "Epoch 166/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6250 - mywloss: 0.6250 - val_loss: 0.7761 - val_mywloss: 0.7761\n",
      "Epoch 167/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6289 - mywloss: 0.6289 - val_loss: 0.7525 - val_mywloss: 0.7525\n",
      "Epoch 168/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6127 - mywloss: 0.6127 - val_loss: 0.7335 - val_mywloss: 0.7335\n",
      "Epoch 169/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6103 - mywloss: 0.6103 - val_loss: 0.7648 - val_mywloss: 0.7648\n",
      "Epoch 170/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6168 - mywloss: 0.6168 - val_loss: 0.7400 - val_mywloss: 0.7400\n",
      "Epoch 171/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6116 - mywloss: 0.6116 - val_loss: 0.7795 - val_mywloss: 0.7795\n",
      "Epoch 172/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6083 - mywloss: 0.6083 - val_loss: 0.7506 - val_mywloss: 0.7506\n",
      "Epoch 173/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6122 - mywloss: 0.6122 - val_loss: 0.7495 - val_mywloss: 0.7495\n",
      "Epoch 174/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5882 - mywloss: 0.5882 - val_loss: 0.7854 - val_mywloss: 0.7854\n",
      "Epoch 175/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6170 - mywloss: 0.6170 - val_loss: 0.7492 - val_mywloss: 0.7492\n",
      "Epoch 176/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6247 - mywloss: 0.6247 - val_loss: 0.7567 - val_mywloss: 0.7567\n",
      "Epoch 177/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6206 - mywloss: 0.6206 - val_loss: 0.7226 - val_mywloss: 0.7226\n",
      "Epoch 178/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5953 - mywloss: 0.5953 - val_loss: 0.7198 - val_mywloss: 0.7198\n",
      "Epoch 179/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6038 - mywloss: 0.6038 - val_loss: 0.7327 - val_mywloss: 0.7327\n",
      "Epoch 180/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5936 - mywloss: 0.5936 - val_loss: 0.7510 - val_mywloss: 0.7510\n",
      "Epoch 181/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6177 - mywloss: 0.6177 - val_loss: 0.7586 - val_mywloss: 0.7586\n",
      "Epoch 182/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6094 - mywloss: 0.6094 - val_loss: 0.7370 - val_mywloss: 0.7370\n",
      "Epoch 183/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6024 - mywloss: 0.6024 - val_loss: 0.7418 - val_mywloss: 0.7418\n",
      "Epoch 184/600\n",
      "6281/6281 [==============================] - 0s 65us/step - loss: 0.5924 - mywloss: 0.5924 - val_loss: 0.7729 - val_mywloss: 0.7729\n",
      "Epoch 185/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5867 - mywloss: 0.5867 - val_loss: 0.7392 - val_mywloss: 0.7392\n",
      "Epoch 186/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6103 - mywloss: 0.6103 - val_loss: 0.7294 - val_mywloss: 0.7294\n",
      "Epoch 187/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5951 - mywloss: 0.5951 - val_loss: 0.7542 - val_mywloss: 0.7542\n",
      "Epoch 188/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6133 - mywloss: 0.6133 - val_loss: 0.7529 - val_mywloss: 0.7529\n",
      "Epoch 189/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5872 - mywloss: 0.5872 - val_loss: 0.7445 - val_mywloss: 0.7445\n",
      "Epoch 190/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5992 - mywloss: 0.5992 - val_loss: 0.7491 - val_mywloss: 0.7491\n",
      "Epoch 191/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5708 - mywloss: 0.5708 - val_loss: 0.7545 - val_mywloss: 0.7545\n",
      "Epoch 192/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5937 - mywloss: 0.5937 - val_loss: 0.7462 - val_mywloss: 0.7462\n",
      "Epoch 193/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5939 - mywloss: 0.5939 - val_loss: 0.7369 - val_mywloss: 0.7369\n",
      "Epoch 194/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5887 - mywloss: 0.5887 - val_loss: 0.7503 - val_mywloss: 0.7503\n",
      "Epoch 195/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5773 - mywloss: 0.5773 - val_loss: 0.7432 - val_mywloss: 0.7432\n",
      "Epoch 196/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6166 - mywloss: 0.6166 - val_loss: 0.7426 - val_mywloss: 0.7426\n",
      "Epoch 197/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5931 - mywloss: 0.5931 - val_loss: 0.7590 - val_mywloss: 0.7590\n",
      "Epoch 198/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5941 - mywloss: 0.5941 - val_loss: 0.7602 - val_mywloss: 0.7602\n",
      "Epoch 199/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5989 - mywloss: 0.5989 - val_loss: 0.7127 - val_mywloss: 0.7127\n",
      "Epoch 200/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6148 - mywloss: 0.6148 - val_loss: 0.7367 - val_mywloss: 0.7367\n",
      "Epoch 201/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6080 - mywloss: 0.6080 - val_loss: 0.7523 - val_mywloss: 0.7523\n",
      "Epoch 202/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5978 - mywloss: 0.5978 - val_loss: 0.7527 - val_mywloss: 0.7527\n",
      "Epoch 203/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.6210 - mywloss: 0.6210 - val_loss: 0.7517 - val_mywloss: 0.7517\n",
      "Epoch 204/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5934 - mywloss: 0.5934 - val_loss: 0.7688 - val_mywloss: 0.7688\n",
      "Epoch 205/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5959 - mywloss: 0.5959 - val_loss: 0.7842 - val_mywloss: 0.7842\n",
      "Epoch 206/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.6035 - mywloss: 0.6035 - val_loss: 0.7644 - val_mywloss: 0.7644\n",
      "Epoch 207/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5759 - mywloss: 0.5759 - val_loss: 0.7833 - val_mywloss: 0.7833\n",
      "Epoch 208/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5782 - mywloss: 0.5782 - val_loss: 0.7625 - val_mywloss: 0.7625\n",
      "Epoch 209/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5948 - mywloss: 0.5948 - val_loss: 0.7506 - val_mywloss: 0.7506\n",
      "Epoch 210/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5888 - mywloss: 0.5888 - val_loss: 0.7519 - val_mywloss: 0.7519\n",
      "Epoch 211/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5884 - mywloss: 0.5884 - val_loss: 0.7572 - val_mywloss: 0.7572\n",
      "Epoch 212/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5756 - mywloss: 0.5756 - val_loss: 0.7599 - val_mywloss: 0.7599\n",
      "Epoch 213/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5792 - mywloss: 0.5792 - val_loss: 0.7875 - val_mywloss: 0.7875\n",
      "Epoch 214/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5561 - mywloss: 0.5561 - val_loss: 0.7689 - val_mywloss: 0.7689\n",
      "Epoch 215/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.6007 - mywloss: 0.6007 - val_loss: 0.7303 - val_mywloss: 0.7303\n",
      "Epoch 216/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5900 - mywloss: 0.5900 - val_loss: 0.7222 - val_mywloss: 0.7222\n",
      "Epoch 217/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.6028 - mywloss: 0.6028 - val_loss: 0.7107 - val_mywloss: 0.7107\n",
      "Epoch 218/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5642 - mywloss: 0.5642 - val_loss: 0.7587 - val_mywloss: 0.7587\n",
      "Epoch 219/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5740 - mywloss: 0.5740 - val_loss: 0.7526 - val_mywloss: 0.7526\n",
      "Epoch 220/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5736 - mywloss: 0.5736 - val_loss: 0.7622 - val_mywloss: 0.7622\n",
      "Epoch 221/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5839 - mywloss: 0.5839 - val_loss: 0.7703 - val_mywloss: 0.7703\n",
      "Epoch 222/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5820 - mywloss: 0.5820 - val_loss: 0.7646 - val_mywloss: 0.7646\n",
      "Epoch 223/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5989 - mywloss: 0.5989 - val_loss: 0.7728 - val_mywloss: 0.7728\n",
      "Epoch 224/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5707 - mywloss: 0.5707 - val_loss: 0.7792 - val_mywloss: 0.7792\n",
      "Epoch 225/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5755 - mywloss: 0.5755 - val_loss: 0.7505 - val_mywloss: 0.7505\n",
      "Epoch 226/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5843 - mywloss: 0.5843 - val_loss: 0.7683 - val_mywloss: 0.7683\n",
      "Epoch 227/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5871 - mywloss: 0.5871 - val_loss: 0.7781 - val_mywloss: 0.7781\n",
      "Epoch 228/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5763 - mywloss: 0.5763 - val_loss: 0.7771 - val_mywloss: 0.7771\n",
      "Epoch 229/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5694 - mywloss: 0.5694 - val_loss: 0.7642 - val_mywloss: 0.7642\n",
      "Epoch 230/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.5808 - mywloss: 0.5808 - val_loss: 0.7565 - val_mywloss: 0.7565\n",
      "Epoch 231/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5454 - mywloss: 0.5454 - val_loss: 0.7642 - val_mywloss: 0.7642\n",
      "Epoch 232/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5567 - mywloss: 0.5567 - val_loss: 0.7586 - val_mywloss: 0.7586\n",
      "Epoch 233/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5387 - mywloss: 0.5387 - val_loss: 0.7671 - val_mywloss: 0.7671\n",
      "Epoch 234/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5709 - mywloss: 0.5709 - val_loss: 0.7715 - val_mywloss: 0.7715\n",
      "Epoch 235/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5691 - mywloss: 0.5691 - val_loss: 0.7869 - val_mywloss: 0.7869\n",
      "Epoch 236/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5634 - mywloss: 0.5634 - val_loss: 0.7693 - val_mywloss: 0.7693\n",
      "Epoch 237/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5427 - mywloss: 0.5427 - val_loss: 0.7554 - val_mywloss: 0.7554\n",
      "Epoch 238/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5858 - mywloss: 0.5858 - val_loss: 0.7626 - val_mywloss: 0.7626\n",
      "Epoch 239/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5759 - mywloss: 0.5759 - val_loss: 0.7678 - val_mywloss: 0.7678\n",
      "Epoch 240/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5495 - mywloss: 0.5495 - val_loss: 0.7654 - val_mywloss: 0.7654\n",
      "Epoch 241/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5551 - mywloss: 0.5551 - val_loss: 0.7679 - val_mywloss: 0.7679\n",
      "Epoch 242/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5566 - mywloss: 0.5566 - val_loss: 0.7687 - val_mywloss: 0.7687\n",
      "Epoch 243/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5657 - mywloss: 0.5657 - val_loss: 0.7689 - val_mywloss: 0.7689\n",
      "Epoch 244/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5737 - mywloss: 0.5737 - val_loss: 0.7801 - val_mywloss: 0.7801\n",
      "Epoch 245/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5459 - mywloss: 0.5459 - val_loss: 0.7866 - val_mywloss: 0.7866\n",
      "Epoch 246/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5541 - mywloss: 0.5541 - val_loss: 0.8040 - val_mywloss: 0.8040\n",
      "Epoch 247/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5637 - mywloss: 0.5637 - val_loss: 0.8252 - val_mywloss: 0.8252\n",
      "Epoch 248/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5506 - mywloss: 0.5506 - val_loss: 0.7814 - val_mywloss: 0.7814\n",
      "Epoch 249/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5780 - mywloss: 0.5780 - val_loss: 0.7589 - val_mywloss: 0.7589\n",
      "Epoch 250/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5493 - mywloss: 0.5493 - val_loss: 0.7767 - val_mywloss: 0.7767\n",
      "Epoch 251/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5523 - mywloss: 0.5523 - val_loss: 0.7715 - val_mywloss: 0.7715\n",
      "Epoch 252/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5292 - mywloss: 0.5292 - val_loss: 0.7825 - val_mywloss: 0.7825\n",
      "Epoch 253/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5484 - mywloss: 0.5484 - val_loss: 0.7786 - val_mywloss: 0.7786\n",
      "Epoch 254/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5568 - mywloss: 0.5568 - val_loss: 0.7758 - val_mywloss: 0.7758\n",
      "Epoch 255/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5587 - mywloss: 0.5587 - val_loss: 0.8067 - val_mywloss: 0.8067\n",
      "Epoch 256/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5533 - mywloss: 0.5533 - val_loss: 0.7936 - val_mywloss: 0.7936\n",
      "Epoch 257/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5385 - mywloss: 0.5385 - val_loss: 0.7972 - val_mywloss: 0.7972\n",
      "Epoch 258/600\n",
      "6281/6281 [==============================] - 0s 72us/step - loss: 0.5506 - mywloss: 0.5506 - val_loss: 0.7521 - val_mywloss: 0.7521\n",
      "Epoch 259/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5555 - mywloss: 0.5555 - val_loss: 0.7643 - val_mywloss: 0.7643\n",
      "Epoch 260/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5679 - mywloss: 0.5679 - val_loss: 0.7660 - val_mywloss: 0.7660\n",
      "Epoch 261/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5634 - mywloss: 0.5634 - val_loss: 0.7666 - val_mywloss: 0.7666\n",
      "Epoch 262/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5399 - mywloss: 0.5399 - val_loss: 0.7760 - val_mywloss: 0.7760\n",
      "Epoch 263/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5320 - mywloss: 0.5320 - val_loss: 0.7592 - val_mywloss: 0.7592\n",
      "Epoch 264/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5708 - mywloss: 0.5708 - val_loss: 0.7504 - val_mywloss: 0.7504\n",
      "Epoch 265/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5413 - mywloss: 0.5413 - val_loss: 0.7204 - val_mywloss: 0.7204\n",
      "Epoch 266/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5644 - mywloss: 0.5644 - val_loss: 0.7840 - val_mywloss: 0.7840\n",
      "Epoch 267/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5582 - mywloss: 0.5582 - val_loss: 0.7529 - val_mywloss: 0.7529\n",
      "Epoch 268/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5658 - mywloss: 0.5658 - val_loss: 0.7575 - val_mywloss: 0.7575\n",
      "Epoch 269/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5528 - mywloss: 0.5528 - val_loss: 0.7386 - val_mywloss: 0.7386\n",
      "Epoch 270/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5393 - mywloss: 0.5393 - val_loss: 0.7397 - val_mywloss: 0.7397\n",
      "Epoch 271/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5454 - mywloss: 0.5454 - val_loss: 0.7376 - val_mywloss: 0.7376\n",
      "Epoch 272/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5331 - mywloss: 0.5331 - val_loss: 0.7243 - val_mywloss: 0.7243\n",
      "Epoch 273/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5421 - mywloss: 0.5421 - val_loss: 0.7431 - val_mywloss: 0.7431\n",
      "Epoch 274/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5375 - mywloss: 0.5375 - val_loss: 0.7777 - val_mywloss: 0.7777\n",
      "Epoch 275/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5449 - mywloss: 0.5449 - val_loss: 0.7799 - val_mywloss: 0.7799\n",
      "Epoch 276/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5320 - mywloss: 0.5320 - val_loss: 0.7508 - val_mywloss: 0.7508\n",
      "Epoch 277/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5401 - mywloss: 0.5401 - val_loss: 0.7932 - val_mywloss: 0.7932\n",
      "Epoch 278/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5331 - mywloss: 0.5331 - val_loss: 0.7705 - val_mywloss: 0.7705\n",
      "Epoch 279/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5639 - mywloss: 0.5639 - val_loss: 0.7978 - val_mywloss: 0.7978\n",
      "Epoch 280/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5601 - mywloss: 0.5601 - val_loss: 0.7675 - val_mywloss: 0.7675\n",
      "Epoch 281/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5370 - mywloss: 0.5370 - val_loss: 0.7471 - val_mywloss: 0.7471\n",
      "Epoch 282/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5466 - mywloss: 0.5466 - val_loss: 0.7727 - val_mywloss: 0.7727\n",
      "Epoch 283/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.5321 - mywloss: 0.5321 - val_loss: 0.7314 - val_mywloss: 0.7314\n",
      "Epoch 284/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5363 - mywloss: 0.5363 - val_loss: 0.7388 - val_mywloss: 0.7388\n",
      "Epoch 285/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5529 - mywloss: 0.5529 - val_loss: 0.7209 - val_mywloss: 0.7209\n",
      "Epoch 286/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5389 - mywloss: 0.5389 - val_loss: 0.7331 - val_mywloss: 0.7331\n",
      "Epoch 287/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5266 - mywloss: 0.5266 - val_loss: 0.7564 - val_mywloss: 0.7564\n",
      "Epoch 288/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5428 - mywloss: 0.5428 - val_loss: 0.7626 - val_mywloss: 0.7626\n",
      "Epoch 289/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5361 - mywloss: 0.5361 - val_loss: 0.7427 - val_mywloss: 0.7427\n",
      "Epoch 290/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5383 - mywloss: 0.5383 - val_loss: 0.7453 - val_mywloss: 0.7453\n",
      "Epoch 291/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5372 - mywloss: 0.5372 - val_loss: 0.7557 - val_mywloss: 0.7557\n",
      "Epoch 292/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5324 - mywloss: 0.5324 - val_loss: 0.7544 - val_mywloss: 0.7544\n",
      "Epoch 293/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5453 - mywloss: 0.5453 - val_loss: 0.7744 - val_mywloss: 0.7744\n",
      "Epoch 294/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5192 - mywloss: 0.5192 - val_loss: 0.7499 - val_mywloss: 0.7499\n",
      "Epoch 295/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5235 - mywloss: 0.5235 - val_loss: 0.7743 - val_mywloss: 0.7743\n",
      "Epoch 296/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5186 - mywloss: 0.5186 - val_loss: 0.7736 - val_mywloss: 0.7736\n",
      "Epoch 297/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5280 - mywloss: 0.5280 - val_loss: 0.7666 - val_mywloss: 0.7666\n",
      "Epoch 298/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5468 - mywloss: 0.5468 - val_loss: 0.7452 - val_mywloss: 0.7452\n",
      "Epoch 299/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5326 - mywloss: 0.5326 - val_loss: 0.7759 - val_mywloss: 0.7759\n",
      "Epoch 300/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5408 - mywloss: 0.5408 - val_loss: 0.7939 - val_mywloss: 0.7939\n",
      "Epoch 301/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5457 - mywloss: 0.5457 - val_loss: 0.7654 - val_mywloss: 0.7654\n",
      "Epoch 302/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5280 - mywloss: 0.5280 - val_loss: 0.7661 - val_mywloss: 0.7661\n",
      "Epoch 303/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5491 - mywloss: 0.5491 - val_loss: 0.7390 - val_mywloss: 0.7390\n",
      "Epoch 304/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5375 - mywloss: 0.5375 - val_loss: 0.7504 - val_mywloss: 0.7504\n",
      "Epoch 305/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5294 - mywloss: 0.5294 - val_loss: 0.7672 - val_mywloss: 0.7672\n",
      "Epoch 306/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.5293 - mywloss: 0.5293 - val_loss: 0.7735 - val_mywloss: 0.7735\n",
      "Epoch 307/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5326 - mywloss: 0.5326 - val_loss: 0.7572 - val_mywloss: 0.7572\n",
      "Epoch 308/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5206 - mywloss: 0.5206 - val_loss: 0.7876 - val_mywloss: 0.7876\n",
      "Epoch 309/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5104 - mywloss: 0.5104 - val_loss: 0.7896 - val_mywloss: 0.7896\n",
      "Epoch 310/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5454 - mywloss: 0.5454 - val_loss: 0.7769 - val_mywloss: 0.7769\n",
      "Epoch 311/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5262 - mywloss: 0.5262 - val_loss: 0.8097 - val_mywloss: 0.8097\n",
      "Epoch 312/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5277 - mywloss: 0.5277 - val_loss: 0.7896 - val_mywloss: 0.7896\n",
      "Epoch 313/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5065 - mywloss: 0.5065 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 314/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5201 - mywloss: 0.5201 - val_loss: 0.7885 - val_mywloss: 0.7885\n",
      "Epoch 315/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5342 - mywloss: 0.5342 - val_loss: 0.7590 - val_mywloss: 0.7590\n",
      "Epoch 316/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5387 - mywloss: 0.5387 - val_loss: 0.7605 - val_mywloss: 0.7605\n",
      "Epoch 317/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5136 - mywloss: 0.5136 - val_loss: 0.7531 - val_mywloss: 0.7531\n",
      "Epoch 318/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5292 - mywloss: 0.5292 - val_loss: 0.7621 - val_mywloss: 0.7621\n",
      "Epoch 319/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5242 - mywloss: 0.5242 - val_loss: 0.7502 - val_mywloss: 0.7502\n",
      "Epoch 320/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5177 - mywloss: 0.5177 - val_loss: 0.7903 - val_mywloss: 0.7903\n",
      "Epoch 321/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4953 - mywloss: 0.4953 - val_loss: 0.8121 - val_mywloss: 0.8121\n",
      "Epoch 322/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5094 - mywloss: 0.5094 - val_loss: 0.7746 - val_mywloss: 0.7746\n",
      "Epoch 323/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5005 - mywloss: 0.5005 - val_loss: 0.7892 - val_mywloss: 0.7892\n",
      "Epoch 324/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5198 - mywloss: 0.5198 - val_loss: 0.7397 - val_mywloss: 0.7397\n",
      "Epoch 325/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5292 - mywloss: 0.5292 - val_loss: 0.7773 - val_mywloss: 0.7773\n",
      "Epoch 326/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5360 - mywloss: 0.5360 - val_loss: 0.7623 - val_mywloss: 0.7623\n",
      "Epoch 327/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.5269 - mywloss: 0.5269 - val_loss: 0.7443 - val_mywloss: 0.7443\n",
      "Epoch 328/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5289 - mywloss: 0.5289 - val_loss: 0.7765 - val_mywloss: 0.7765\n",
      "Epoch 329/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5302 - mywloss: 0.5302 - val_loss: 0.7419 - val_mywloss: 0.7419\n",
      "Epoch 330/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5159 - mywloss: 0.5159 - val_loss: 0.7366 - val_mywloss: 0.7366\n",
      "Epoch 331/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4982 - mywloss: 0.4982 - val_loss: 0.7191 - val_mywloss: 0.7191\n",
      "Epoch 332/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5239 - mywloss: 0.5239 - val_loss: 0.7341 - val_mywloss: 0.7341\n",
      "Epoch 333/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5060 - mywloss: 0.5060 - val_loss: 0.7492 - val_mywloss: 0.7492\n",
      "Epoch 334/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5419 - mywloss: 0.5419 - val_loss: 0.7679 - val_mywloss: 0.7679\n",
      "Epoch 335/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5285 - mywloss: 0.5285 - val_loss: 0.7427 - val_mywloss: 0.7427\n",
      "Epoch 336/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5162 - mywloss: 0.5162 - val_loss: 0.7555 - val_mywloss: 0.7555\n",
      "Epoch 337/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5235 - mywloss: 0.5235 - val_loss: 0.7407 - val_mywloss: 0.7407\n",
      "Epoch 338/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5367 - mywloss: 0.5367 - val_loss: 0.7447 - val_mywloss: 0.7447\n",
      "Epoch 339/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4992 - mywloss: 0.4992 - val_loss: 0.7598 - val_mywloss: 0.7598\n",
      "Epoch 340/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5200 - mywloss: 0.5200 - val_loss: 0.7670 - val_mywloss: 0.7670\n",
      "Epoch 341/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5117 - mywloss: 0.5117 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 342/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5239 - mywloss: 0.5239 - val_loss: 0.7582 - val_mywloss: 0.7582\n",
      "Epoch 343/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5097 - mywloss: 0.5097 - val_loss: 0.7364 - val_mywloss: 0.7364\n",
      "Epoch 344/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5049 - mywloss: 0.5049 - val_loss: 0.7614 - val_mywloss: 0.7614\n",
      "Epoch 345/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4956 - mywloss: 0.4956 - val_loss: 0.7699 - val_mywloss: 0.7699\n",
      "Epoch 346/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4985 - mywloss: 0.4985 - val_loss: 0.7424 - val_mywloss: 0.7424\n",
      "Epoch 347/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.5231 - mywloss: 0.5231 - val_loss: 0.7235 - val_mywloss: 0.7235\n",
      "Epoch 348/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5170 - mywloss: 0.5170 - val_loss: 0.7381 - val_mywloss: 0.7381\n",
      "Epoch 349/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5194 - mywloss: 0.5194 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 350/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5091 - mywloss: 0.5091 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 351/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4941 - mywloss: 0.4941 - val_loss: 0.7806 - val_mywloss: 0.7806\n",
      "Epoch 352/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5110 - mywloss: 0.5110 - val_loss: 0.7736 - val_mywloss: 0.7736\n",
      "Epoch 353/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5077 - mywloss: 0.5077 - val_loss: 0.7994 - val_mywloss: 0.7994\n",
      "Epoch 354/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4946 - mywloss: 0.4946 - val_loss: 0.7395 - val_mywloss: 0.7395\n",
      "Epoch 355/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5195 - mywloss: 0.5195 - val_loss: 0.7744 - val_mywloss: 0.7744\n",
      "Epoch 356/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5246 - mywloss: 0.5246 - val_loss: 0.7414 - val_mywloss: 0.7414\n",
      "Epoch 357/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4964 - mywloss: 0.4964 - val_loss: 0.7591 - val_mywloss: 0.7591\n",
      "Epoch 358/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4996 - mywloss: 0.4996 - val_loss: 0.8053 - val_mywloss: 0.8053\n",
      "Epoch 359/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5172 - mywloss: 0.5172 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 360/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5046 - mywloss: 0.5046 - val_loss: 0.8017 - val_mywloss: 0.8017\n",
      "Epoch 361/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4921 - mywloss: 0.4921 - val_loss: 0.7985 - val_mywloss: 0.7985\n",
      "Epoch 362/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5012 - mywloss: 0.5012 - val_loss: 0.7926 - val_mywloss: 0.7926\n",
      "Epoch 363/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4814 - mywloss: 0.4814 - val_loss: 0.7985 - val_mywloss: 0.7985\n",
      "Epoch 364/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5101 - mywloss: 0.5101 - val_loss: 0.8023 - val_mywloss: 0.8023\n",
      "Epoch 365/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4978 - mywloss: 0.4978 - val_loss: 0.8090 - val_mywloss: 0.8090\n",
      "Epoch 366/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5061 - mywloss: 0.5061 - val_loss: 0.7753 - val_mywloss: 0.7753\n",
      "Epoch 367/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.5063 - mywloss: 0.5063 - val_loss: 0.7571 - val_mywloss: 0.7571\n",
      "Epoch 368/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4865 - mywloss: 0.4865 - val_loss: 0.7812 - val_mywloss: 0.7812\n",
      "Epoch 369/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5032 - mywloss: 0.5032 - val_loss: 0.7791 - val_mywloss: 0.7791\n",
      "Epoch 370/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4879 - mywloss: 0.4879 - val_loss: 0.7749 - val_mywloss: 0.7749\n",
      "Epoch 371/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4996 - mywloss: 0.4996 - val_loss: 0.7752 - val_mywloss: 0.7752\n",
      "Epoch 372/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4802 - mywloss: 0.4802 - val_loss: 0.8078 - val_mywloss: 0.8078\n",
      "Epoch 373/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4934 - mywloss: 0.4934 - val_loss: 0.8092 - val_mywloss: 0.8092\n",
      "Epoch 374/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4832 - mywloss: 0.4832 - val_loss: 0.7975 - val_mywloss: 0.7975\n",
      "Epoch 375/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.5142 - mywloss: 0.5142 - val_loss: 0.8176 - val_mywloss: 0.8176\n",
      "Epoch 376/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4810 - mywloss: 0.4810 - val_loss: 0.7926 - val_mywloss: 0.7926\n",
      "Epoch 377/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.5086 - mywloss: 0.5086 - val_loss: 0.7726 - val_mywloss: 0.7726\n",
      "Epoch 378/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4919 - mywloss: 0.4919 - val_loss: 0.7759 - val_mywloss: 0.7759\n",
      "Epoch 379/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.5072 - mywloss: 0.5072 - val_loss: 0.7863 - val_mywloss: 0.7863\n",
      "Epoch 380/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4966 - mywloss: 0.4966 - val_loss: 0.7766 - val_mywloss: 0.7766\n",
      "Epoch 381/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4914 - mywloss: 0.4914 - val_loss: 0.7679 - val_mywloss: 0.7679\n",
      "Epoch 382/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4788 - mywloss: 0.4788 - val_loss: 0.8040 - val_mywloss: 0.8040\n",
      "Epoch 383/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4874 - mywloss: 0.4874 - val_loss: 0.8091 - val_mywloss: 0.8091\n",
      "Epoch 384/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4971 - mywloss: 0.4971 - val_loss: 0.8275 - val_mywloss: 0.8275\n",
      "Epoch 385/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4826 - mywloss: 0.4826 - val_loss: 0.8149 - val_mywloss: 0.8149\n",
      "Epoch 386/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4760 - mywloss: 0.4760 - val_loss: 0.8519 - val_mywloss: 0.8519\n",
      "Epoch 387/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4743 - mywloss: 0.4743 - val_loss: 0.8393 - val_mywloss: 0.8393\n",
      "Epoch 388/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.5042 - mywloss: 0.5042 - val_loss: 0.8093 - val_mywloss: 0.8093\n",
      "Epoch 389/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4986 - mywloss: 0.4986 - val_loss: 0.7860 - val_mywloss: 0.7860\n",
      "Epoch 390/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4995 - mywloss: 0.4995 - val_loss: 0.7876 - val_mywloss: 0.7876\n",
      "Epoch 391/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4893 - mywloss: 0.4893 - val_loss: 0.7633 - val_mywloss: 0.7633\n",
      "Epoch 392/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.5082 - mywloss: 0.5082 - val_loss: 0.7786 - val_mywloss: 0.7786\n",
      "Epoch 393/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5157 - mywloss: 0.5157 - val_loss: 0.7997 - val_mywloss: 0.7997\n",
      "Epoch 394/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4719 - mywloss: 0.4719 - val_loss: 0.7973 - val_mywloss: 0.7973\n",
      "Epoch 395/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4942 - mywloss: 0.4942 - val_loss: 0.7999 - val_mywloss: 0.7999\n",
      "Epoch 396/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4934 - mywloss: 0.4934 - val_loss: 0.8692 - val_mywloss: 0.8692\n",
      "Epoch 397/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4945 - mywloss: 0.4945 - val_loss: 0.7966 - val_mywloss: 0.7966\n",
      "Epoch 398/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4923 - mywloss: 0.4923 - val_loss: 0.7877 - val_mywloss: 0.7877\n",
      "Epoch 399/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4926 - mywloss: 0.4926 - val_loss: 0.8056 - val_mywloss: 0.8056\n",
      "Epoch 400/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4956 - mywloss: 0.4956 - val_loss: 0.7856 - val_mywloss: 0.7856\n",
      "Epoch 401/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4828 - mywloss: 0.4828 - val_loss: 0.7782 - val_mywloss: 0.7782\n",
      "Epoch 402/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4860 - mywloss: 0.4860 - val_loss: 0.7917 - val_mywloss: 0.7917\n",
      "Epoch 403/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4808 - mywloss: 0.4808 - val_loss: 0.8194 - val_mywloss: 0.8194\n",
      "Epoch 404/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4875 - mywloss: 0.4875 - val_loss: 0.8014 - val_mywloss: 0.8014\n",
      "Epoch 405/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4829 - mywloss: 0.4829 - val_loss: 0.8012 - val_mywloss: 0.8012\n",
      "Epoch 406/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4886 - mywloss: 0.4886 - val_loss: 0.8449 - val_mywloss: 0.8449\n",
      "Epoch 407/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.5048 - mywloss: 0.5048 - val_loss: 0.8285 - val_mywloss: 0.8285\n",
      "Epoch 408/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4938 - mywloss: 0.4938 - val_loss: 0.8135 - val_mywloss: 0.8135\n",
      "Epoch 409/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4815 - mywloss: 0.4815 - val_loss: 0.7979 - val_mywloss: 0.7979\n",
      "Epoch 410/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4880 - mywloss: 0.4880 - val_loss: 0.8040 - val_mywloss: 0.8040\n",
      "Epoch 411/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4997 - mywloss: 0.4997 - val_loss: 0.7912 - val_mywloss: 0.7912\n",
      "Epoch 412/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4866 - mywloss: 0.4866 - val_loss: 0.8060 - val_mywloss: 0.8060\n",
      "Epoch 413/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4631 - mywloss: 0.4631 - val_loss: 0.7995 - val_mywloss: 0.7995\n",
      "Epoch 414/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4753 - mywloss: 0.4753 - val_loss: 0.8159 - val_mywloss: 0.8159\n",
      "Epoch 415/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4733 - mywloss: 0.4733 - val_loss: 0.7671 - val_mywloss: 0.7671\n",
      "Epoch 416/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4779 - mywloss: 0.4779 - val_loss: 0.8327 - val_mywloss: 0.8327\n",
      "Epoch 417/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4893 - mywloss: 0.4893 - val_loss: 0.8224 - val_mywloss: 0.8224\n",
      "Epoch 418/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4915 - mywloss: 0.4915 - val_loss: 0.8280 - val_mywloss: 0.8280\n",
      "Epoch 419/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4868 - mywloss: 0.4868 - val_loss: 0.8138 - val_mywloss: 0.8138\n",
      "Epoch 420/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4855 - mywloss: 0.4855 - val_loss: 0.8110 - val_mywloss: 0.8110\n",
      "Epoch 421/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.4598 - mywloss: 0.4598 - val_loss: 0.8423 - val_mywloss: 0.8423\n",
      "Epoch 422/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4817 - mywloss: 0.4817 - val_loss: 0.8150 - val_mywloss: 0.8150\n",
      "Epoch 423/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.4880 - mywloss: 0.4880 - val_loss: 0.8336 - val_mywloss: 0.8336\n",
      "Epoch 424/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4888 - mywloss: 0.4888 - val_loss: 0.8386 - val_mywloss: 0.8386\n",
      "Epoch 425/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4740 - mywloss: 0.4740 - val_loss: 0.8784 - val_mywloss: 0.8784\n",
      "Epoch 426/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4813 - mywloss: 0.4813 - val_loss: 0.8493 - val_mywloss: 0.8493\n",
      "Epoch 427/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4942 - mywloss: 0.4942 - val_loss: 0.8539 - val_mywloss: 0.8539\n",
      "Epoch 428/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4801 - mywloss: 0.4801 - val_loss: 0.8305 - val_mywloss: 0.8305\n",
      "Epoch 429/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4705 - mywloss: 0.4705 - val_loss: 0.8199 - val_mywloss: 0.8199\n",
      "Epoch 430/600\n",
      "6281/6281 [==============================] - 0s 72us/step - loss: 0.4650 - mywloss: 0.4650 - val_loss: 0.7955 - val_mywloss: 0.7955\n",
      "Epoch 431/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4726 - mywloss: 0.4726 - val_loss: 0.7836 - val_mywloss: 0.7836\n",
      "Epoch 432/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4818 - mywloss: 0.4818 - val_loss: 0.7868 - val_mywloss: 0.7868\n",
      "Epoch 433/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.4738 - mywloss: 0.4738 - val_loss: 0.8057 - val_mywloss: 0.8057\n",
      "Epoch 434/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4710 - mywloss: 0.4710 - val_loss: 0.7909 - val_mywloss: 0.7909\n",
      "Epoch 435/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4669 - mywloss: 0.4669 - val_loss: 0.7982 - val_mywloss: 0.7982\n",
      "Epoch 436/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4567 - mywloss: 0.4567 - val_loss: 0.8001 - val_mywloss: 0.8001\n",
      "Epoch 437/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4662 - mywloss: 0.4662 - val_loss: 0.7972 - val_mywloss: 0.7972\n",
      "Epoch 438/600\n",
      "6281/6281 [==============================] - 0s 74us/step - loss: 0.4862 - mywloss: 0.4862 - val_loss: 0.7907 - val_mywloss: 0.7907\n",
      "Epoch 439/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4686 - mywloss: 0.4686 - val_loss: 0.7865 - val_mywloss: 0.7865\n",
      "Epoch 440/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4905 - mywloss: 0.4905 - val_loss: 0.7701 - val_mywloss: 0.7701\n",
      "Epoch 441/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4918 - mywloss: 0.4918 - val_loss: 0.7751 - val_mywloss: 0.7751\n",
      "Epoch 442/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4645 - mywloss: 0.4645 - val_loss: 0.7893 - val_mywloss: 0.7893\n",
      "Epoch 443/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4746 - mywloss: 0.4746 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 444/600\n",
      "6281/6281 [==============================] - 0s 72us/step - loss: 0.4836 - mywloss: 0.4836 - val_loss: 0.7659 - val_mywloss: 0.7659\n",
      "Epoch 445/600\n",
      "6281/6281 [==============================] - 0s 75us/step - loss: 0.4831 - mywloss: 0.4831 - val_loss: 0.7407 - val_mywloss: 0.7407\n",
      "Epoch 446/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4796 - mywloss: 0.4796 - val_loss: 0.7512 - val_mywloss: 0.7512\n",
      "Epoch 447/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4631 - mywloss: 0.4631 - val_loss: 0.7701 - val_mywloss: 0.7701\n",
      "Epoch 448/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4740 - mywloss: 0.4740 - val_loss: 0.8384 - val_mywloss: 0.8384\n",
      "Epoch 449/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4776 - mywloss: 0.4776 - val_loss: 0.7727 - val_mywloss: 0.7727\n",
      "Epoch 450/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4879 - mywloss: 0.4879 - val_loss: 0.7857 - val_mywloss: 0.7857\n",
      "Epoch 451/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.7892 - val_mywloss: 0.7892\n",
      "Epoch 452/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4834 - mywloss: 0.4834 - val_loss: 0.7771 - val_mywloss: 0.7771\n",
      "Epoch 453/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4746 - mywloss: 0.4746 - val_loss: 0.7505 - val_mywloss: 0.7505\n",
      "Epoch 454/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 455/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4557 - mywloss: 0.4557 - val_loss: 0.7935 - val_mywloss: 0.7935\n",
      "Epoch 456/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4625 - mywloss: 0.4625 - val_loss: 0.8057 - val_mywloss: 0.8057\n",
      "Epoch 457/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4753 - mywloss: 0.4753 - val_loss: 0.8271 - val_mywloss: 0.8271\n",
      "Epoch 458/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4580 - mywloss: 0.4580 - val_loss: 0.8144 - val_mywloss: 0.8144\n",
      "Epoch 459/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4640 - mywloss: 0.4640 - val_loss: 0.8537 - val_mywloss: 0.8537\n",
      "Epoch 460/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4748 - mywloss: 0.4748 - val_loss: 0.8553 - val_mywloss: 0.8553\n",
      "Epoch 461/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4806 - mywloss: 0.4806 - val_loss: 0.8352 - val_mywloss: 0.8352\n",
      "Epoch 462/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4558 - mywloss: 0.4558 - val_loss: 0.8525 - val_mywloss: 0.8525\n",
      "Epoch 463/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4624 - mywloss: 0.4624 - val_loss: 0.8061 - val_mywloss: 0.8061\n",
      "Epoch 464/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4674 - mywloss: 0.4674 - val_loss: 0.8361 - val_mywloss: 0.8361\n",
      "Epoch 465/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4711 - mywloss: 0.4711 - val_loss: 0.8354 - val_mywloss: 0.8354\n",
      "Epoch 466/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4651 - mywloss: 0.4651 - val_loss: 0.8141 - val_mywloss: 0.8141\n",
      "Epoch 467/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4761 - mywloss: 0.4761 - val_loss: 0.8279 - val_mywloss: 0.8279\n",
      "Epoch 468/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4673 - mywloss: 0.4673 - val_loss: 0.8076 - val_mywloss: 0.8076\n",
      "Epoch 469/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4744 - mywloss: 0.4744 - val_loss: 0.8005 - val_mywloss: 0.8005\n",
      "Epoch 470/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4458 - mywloss: 0.4458 - val_loss: 0.8120 - val_mywloss: 0.8120\n",
      "Epoch 471/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4573 - mywloss: 0.4573 - val_loss: 0.8087 - val_mywloss: 0.8087\n",
      "Epoch 472/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4506 - mywloss: 0.4506 - val_loss: 0.7916 - val_mywloss: 0.7916\n",
      "Epoch 473/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4723 - mywloss: 0.4723 - val_loss: 0.7902 - val_mywloss: 0.7902\n",
      "Epoch 474/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4452 - mywloss: 0.4452 - val_loss: 0.8382 - val_mywloss: 0.8382\n",
      "Epoch 475/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4545 - mywloss: 0.4545 - val_loss: 0.8328 - val_mywloss: 0.8328\n",
      "Epoch 476/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4656 - mywloss: 0.4656 - val_loss: 0.8284 - val_mywloss: 0.8284\n",
      "Epoch 477/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4807 - mywloss: 0.4807 - val_loss: 0.7930 - val_mywloss: 0.7930\n",
      "Epoch 478/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4593 - mywloss: 0.4593 - val_loss: 0.8222 - val_mywloss: 0.8222\n",
      "Epoch 479/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4447 - mywloss: 0.4447 - val_loss: 0.7931 - val_mywloss: 0.7931\n",
      "Epoch 480/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4628 - mywloss: 0.4628 - val_loss: 0.8026 - val_mywloss: 0.8026\n",
      "Epoch 481/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4684 - mywloss: 0.4684 - val_loss: 0.8307 - val_mywloss: 0.8307\n",
      "Epoch 482/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4374 - mywloss: 0.4374 - val_loss: 0.7979 - val_mywloss: 0.7979\n",
      "Epoch 483/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4495 - mywloss: 0.4495 - val_loss: 0.8045 - val_mywloss: 0.8045\n",
      "Epoch 484/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4565 - mywloss: 0.4565 - val_loss: 0.7907 - val_mywloss: 0.7907\n",
      "Epoch 485/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4564 - mywloss: 0.4564 - val_loss: 0.7645 - val_mywloss: 0.7645\n",
      "Epoch 486/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4589 - mywloss: 0.4589 - val_loss: 0.8017 - val_mywloss: 0.8017\n",
      "Epoch 487/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4577 - mywloss: 0.4577 - val_loss: 0.7991 - val_mywloss: 0.7991\n",
      "Epoch 488/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4830 - mywloss: 0.4830 - val_loss: 0.7840 - val_mywloss: 0.7840\n",
      "Epoch 489/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4772 - mywloss: 0.4772 - val_loss: 0.8137 - val_mywloss: 0.8137\n",
      "Epoch 490/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4409 - mywloss: 0.4409 - val_loss: 0.8206 - val_mywloss: 0.8206\n",
      "Epoch 491/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4770 - mywloss: 0.4770 - val_loss: 0.7768 - val_mywloss: 0.7768\n",
      "Epoch 492/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4595 - mywloss: 0.4595 - val_loss: 0.8095 - val_mywloss: 0.8095\n",
      "Epoch 493/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4625 - mywloss: 0.4625 - val_loss: 0.7786 - val_mywloss: 0.7786\n",
      "Epoch 494/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4649 - mywloss: 0.4649 - val_loss: 0.7925 - val_mywloss: 0.7925\n",
      "Epoch 495/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4663 - mywloss: 0.4663 - val_loss: 0.7727 - val_mywloss: 0.7727\n",
      "Epoch 496/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4575 - mywloss: 0.4575 - val_loss: 0.8088 - val_mywloss: 0.8088\n",
      "Epoch 497/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4410 - mywloss: 0.4410 - val_loss: 0.7835 - val_mywloss: 0.7835\n",
      "Epoch 498/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4561 - mywloss: 0.4561 - val_loss: 0.8143 - val_mywloss: 0.8143\n",
      "Epoch 499/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4511 - mywloss: 0.4511 - val_loss: 0.8300 - val_mywloss: 0.8300\n",
      "Epoch 500/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4513 - mywloss: 0.4513 - val_loss: 0.8232 - val_mywloss: 0.8232\n",
      "Epoch 501/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4578 - mywloss: 0.4578 - val_loss: 0.8331 - val_mywloss: 0.8331\n",
      "Epoch 502/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4528 - mywloss: 0.4528 - val_loss: 0.8318 - val_mywloss: 0.8318\n",
      "Epoch 503/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4722 - mywloss: 0.4722 - val_loss: 0.8190 - val_mywloss: 0.8190\n",
      "Epoch 504/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4465 - mywloss: 0.4465 - val_loss: 0.8202 - val_mywloss: 0.8202\n",
      "Epoch 505/600\n",
      "6281/6281 [==============================] - 0s 66us/step - loss: 0.4615 - mywloss: 0.4615 - val_loss: 0.8153 - val_mywloss: 0.8153\n",
      "Epoch 506/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4479 - mywloss: 0.4479 - val_loss: 0.8224 - val_mywloss: 0.8224\n",
      "Epoch 507/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4444 - mywloss: 0.4444 - val_loss: 0.8127 - val_mywloss: 0.8127\n",
      "Epoch 508/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4543 - mywloss: 0.4543 - val_loss: 0.8477 - val_mywloss: 0.8477\n",
      "Epoch 509/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4682 - mywloss: 0.4682 - val_loss: 0.8252 - val_mywloss: 0.8252\n",
      "Epoch 510/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4441 - mywloss: 0.4441 - val_loss: 0.8475 - val_mywloss: 0.8475\n",
      "Epoch 511/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4432 - mywloss: 0.4432 - val_loss: 0.8464 - val_mywloss: 0.8464\n",
      "Epoch 512/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4453 - mywloss: 0.4453 - val_loss: 0.7958 - val_mywloss: 0.7958\n",
      "Epoch 513/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4498 - mywloss: 0.4498 - val_loss: 0.8121 - val_mywloss: 0.8121\n",
      "Epoch 514/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4535 - mywloss: 0.4535 - val_loss: 0.8381 - val_mywloss: 0.8381\n",
      "Epoch 515/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4279 - mywloss: 0.4279 - val_loss: 0.8524 - val_mywloss: 0.8524\n",
      "Epoch 516/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4555 - mywloss: 0.4555 - val_loss: 0.8437 - val_mywloss: 0.8437\n",
      "Epoch 517/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4374 - mywloss: 0.4374 - val_loss: 0.8316 - val_mywloss: 0.8316\n",
      "Epoch 518/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4601 - mywloss: 0.4601 - val_loss: 0.8321 - val_mywloss: 0.8321\n",
      "Epoch 519/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4410 - mywloss: 0.4410 - val_loss: 0.8423 - val_mywloss: 0.8423\n",
      "Epoch 520/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4369 - mywloss: 0.4369 - val_loss: 0.8607 - val_mywloss: 0.8607\n",
      "Epoch 521/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4562 - mywloss: 0.4562 - val_loss: 0.8356 - val_mywloss: 0.8356\n",
      "Epoch 522/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4516 - mywloss: 0.4516 - val_loss: 0.8248 - val_mywloss: 0.8248\n",
      "Epoch 523/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4367 - mywloss: 0.4367 - val_loss: 0.8598 - val_mywloss: 0.8598\n",
      "Epoch 524/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4683 - mywloss: 0.4683 - val_loss: 0.8196 - val_mywloss: 0.8196\n",
      "Epoch 525/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4455 - mywloss: 0.4455 - val_loss: 0.8027 - val_mywloss: 0.8027\n",
      "Epoch 526/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4333 - mywloss: 0.4333 - val_loss: 0.8090 - val_mywloss: 0.8090\n",
      "Epoch 527/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4575 - mywloss: 0.4575 - val_loss: 0.8166 - val_mywloss: 0.8166\n",
      "Epoch 528/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4428 - mywloss: 0.4428 - val_loss: 0.8381 - val_mywloss: 0.8381\n",
      "Epoch 529/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.8290 - val_mywloss: 0.8290\n",
      "Epoch 530/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4438 - mywloss: 0.4438 - val_loss: 0.8211 - val_mywloss: 0.8211\n",
      "Epoch 531/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4353 - mywloss: 0.4353 - val_loss: 0.8151 - val_mywloss: 0.8151\n",
      "Epoch 532/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.8118 - val_mywloss: 0.8118\n",
      "Epoch 533/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4333 - mywloss: 0.4333 - val_loss: 0.8506 - val_mywloss: 0.8506\n",
      "Epoch 534/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.4508 - mywloss: 0.4508 - val_loss: 0.8378 - val_mywloss: 0.8378\n",
      "Epoch 535/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4385 - mywloss: 0.4385 - val_loss: 0.8351 - val_mywloss: 0.8351\n",
      "Epoch 536/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4298 - mywloss: 0.4298 - val_loss: 0.8274 - val_mywloss: 0.8274\n",
      "Epoch 537/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4341 - mywloss: 0.4341 - val_loss: 0.8229 - val_mywloss: 0.8229\n",
      "Epoch 538/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4473 - mywloss: 0.4473 - val_loss: 0.7996 - val_mywloss: 0.7996\n",
      "Epoch 539/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4542 - mywloss: 0.4542 - val_loss: 0.8086 - val_mywloss: 0.8086\n",
      "Epoch 540/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4471 - mywloss: 0.4471 - val_loss: 0.8231 - val_mywloss: 0.8231\n",
      "Epoch 541/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4376 - mywloss: 0.4376 - val_loss: 0.8081 - val_mywloss: 0.8081\n",
      "Epoch 542/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4349 - mywloss: 0.4349 - val_loss: 0.8341 - val_mywloss: 0.8341\n",
      "Epoch 543/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4346 - mywloss: 0.4346 - val_loss: 0.8093 - val_mywloss: 0.8093\n",
      "Epoch 544/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4313 - mywloss: 0.4313 - val_loss: 0.8072 - val_mywloss: 0.8072\n",
      "Epoch 545/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4420 - mywloss: 0.4420 - val_loss: 0.8276 - val_mywloss: 0.8276\n",
      "Epoch 546/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4373 - mywloss: 0.4373 - val_loss: 0.8280 - val_mywloss: 0.8280\n",
      "Epoch 547/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4278 - mywloss: 0.4278 - val_loss: 0.8570 - val_mywloss: 0.8570\n",
      "Epoch 548/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4678 - mywloss: 0.4678 - val_loss: 0.8742 - val_mywloss: 0.8742\n",
      "Epoch 549/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4286 - mywloss: 0.4286 - val_loss: 0.8620 - val_mywloss: 0.8620\n",
      "Epoch 550/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4546 - mywloss: 0.4546 - val_loss: 0.8740 - val_mywloss: 0.8740\n",
      "Epoch 551/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4368 - mywloss: 0.4368 - val_loss: 0.8560 - val_mywloss: 0.8560\n",
      "Epoch 552/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4420 - mywloss: 0.4420 - val_loss: 0.8182 - val_mywloss: 0.8182\n",
      "Epoch 553/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4226 - mywloss: 0.4226 - val_loss: 0.8268 - val_mywloss: 0.8268\n",
      "Epoch 554/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4590 - mywloss: 0.4590 - val_loss: 0.8242 - val_mywloss: 0.8242\n",
      "Epoch 555/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4280 - mywloss: 0.4280 - val_loss: 0.8255 - val_mywloss: 0.8255\n",
      "Epoch 556/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4375 - mywloss: 0.4375 - val_loss: 0.8288 - val_mywloss: 0.8288\n",
      "Epoch 557/600\n",
      "6281/6281 [==============================] - 0s 70us/step - loss: 0.4284 - mywloss: 0.4284 - val_loss: 0.8239 - val_mywloss: 0.8239\n",
      "Epoch 558/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4431 - mywloss: 0.4431 - val_loss: 0.8295 - val_mywloss: 0.8295\n",
      "Epoch 559/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4350 - mywloss: 0.4350 - val_loss: 0.8447 - val_mywloss: 0.8447\n",
      "Epoch 560/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4286 - mywloss: 0.4286 - val_loss: 0.8280 - val_mywloss: 0.8280\n",
      "Epoch 561/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4572 - mywloss: 0.4572 - val_loss: 0.8603 - val_mywloss: 0.8603\n",
      "Epoch 562/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4330 - mywloss: 0.4330 - val_loss: 0.8224 - val_mywloss: 0.8224\n",
      "Epoch 563/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4193 - mywloss: 0.4193 - val_loss: 0.8153 - val_mywloss: 0.8153\n",
      "Epoch 564/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4346 - mywloss: 0.4346 - val_loss: 0.8304 - val_mywloss: 0.8304\n",
      "Epoch 565/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4455 - mywloss: 0.4455 - val_loss: 0.8306 - val_mywloss: 0.8306\n",
      "Epoch 566/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4272 - mywloss: 0.4272 - val_loss: 0.8448 - val_mywloss: 0.8448\n",
      "Epoch 567/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4405 - mywloss: 0.4405 - val_loss: 0.8442 - val_mywloss: 0.8442\n",
      "Epoch 568/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4308 - mywloss: 0.4308 - val_loss: 0.8610 - val_mywloss: 0.8610\n",
      "Epoch 569/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4351 - mywloss: 0.4351 - val_loss: 0.8546 - val_mywloss: 0.8546\n",
      "Epoch 570/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4458 - mywloss: 0.4458 - val_loss: 0.8627 - val_mywloss: 0.8627\n",
      "Epoch 571/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4360 - mywloss: 0.4360 - val_loss: 0.8675 - val_mywloss: 0.8675\n",
      "Epoch 572/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4240 - mywloss: 0.4240 - val_loss: 0.8524 - val_mywloss: 0.8524\n",
      "Epoch 573/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4250 - mywloss: 0.4250 - val_loss: 0.8476 - val_mywloss: 0.8476\n",
      "Epoch 574/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4293 - mywloss: 0.4293 - val_loss: 0.8583 - val_mywloss: 0.8583\n",
      "Epoch 575/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4643 - mywloss: 0.4643 - val_loss: 0.8238 - val_mywloss: 0.8238\n",
      "Epoch 576/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4421 - mywloss: 0.4421 - val_loss: 0.8375 - val_mywloss: 0.8375\n",
      "Epoch 577/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4355 - mywloss: 0.4355 - val_loss: 0.8521 - val_mywloss: 0.8521\n",
      "Epoch 578/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4568 - mywloss: 0.4568 - val_loss: 0.8308 - val_mywloss: 0.8308\n",
      "Epoch 579/600\n",
      "6281/6281 [==============================] - 0s 71us/step - loss: 0.4093 - mywloss: 0.4093 - val_loss: 0.8523 - val_mywloss: 0.8523\n",
      "Epoch 580/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4560 - mywloss: 0.4560 - val_loss: 0.8214 - val_mywloss: 0.8214\n",
      "Epoch 581/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4433 - mywloss: 0.4433 - val_loss: 0.8453 - val_mywloss: 0.8453\n",
      "Epoch 582/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4462 - mywloss: 0.4462 - val_loss: 0.8141 - val_mywloss: 0.8141\n",
      "Epoch 583/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4502 - mywloss: 0.4502 - val_loss: 0.8297 - val_mywloss: 0.8297\n",
      "Epoch 584/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4437 - mywloss: 0.4437 - val_loss: 0.8221 - val_mywloss: 0.8221\n",
      "Epoch 585/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4172 - mywloss: 0.4172 - val_loss: 0.7855 - val_mywloss: 0.7855\n",
      "Epoch 586/600\n",
      "6281/6281 [==============================] - 0s 67us/step - loss: 0.4207 - mywloss: 0.4207 - val_loss: 0.8200 - val_mywloss: 0.8200\n",
      "Epoch 587/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4138 - mywloss: 0.4138 - val_loss: 0.8049 - val_mywloss: 0.8049\n",
      "Epoch 588/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4291 - mywloss: 0.4291 - val_loss: 0.8017 - val_mywloss: 0.8017\n",
      "Epoch 589/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4239 - mywloss: 0.4239 - val_loss: 0.8030 - val_mywloss: 0.8030\n",
      "Epoch 590/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4406 - mywloss: 0.4406 - val_loss: 0.7850 - val_mywloss: 0.7850\n",
      "Epoch 591/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4417 - mywloss: 0.4417 - val_loss: 0.7438 - val_mywloss: 0.7438\n",
      "Epoch 592/600\n",
      "6281/6281 [==============================] - 0s 69us/step - loss: 0.4495 - mywloss: 0.4495 - val_loss: 0.7754 - val_mywloss: 0.7754\n",
      "Epoch 593/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4241 - mywloss: 0.4241 - val_loss: 0.7762 - val_mywloss: 0.7762\n",
      "Epoch 594/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4237 - mywloss: 0.4237 - val_loss: 0.7639 - val_mywloss: 0.7639\n",
      "Epoch 595/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4282 - mywloss: 0.4282 - val_loss: 0.7583 - val_mywloss: 0.7583\n",
      "Epoch 596/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4380 - mywloss: 0.4380 - val_loss: 0.8168 - val_mywloss: 0.8168\n",
      "Epoch 597/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4303 - mywloss: 0.4303 - val_loss: 0.8056 - val_mywloss: 0.8056\n",
      "Epoch 598/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4248 - mywloss: 0.4248 - val_loss: 0.8402 - val_mywloss: 0.8402\n",
      "Epoch 599/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4383 - mywloss: 0.4383 - val_loss: 0.8215 - val_mywloss: 0.8215\n",
      "Epoch 600/600\n",
      "6281/6281 [==============================] - 0s 68us/step - loss: 0.4251 - mywloss: 0.4251 - val_loss: 0.8249 - val_mywloss: 0.8249\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb60e02dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model\n",
      "0.7272001779415216\n",
      "Train on 6284 samples, validate on 1564 samples\n",
      "Epoch 1/600\n",
      "6284/6284 [==============================] - 2s 270us/step - loss: 2.5007 - mywloss: 2.5007 - val_loss: 1.5454 - val_mywloss: 1.5454\n",
      "Epoch 2/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.9166 - mywloss: 1.9166 - val_loss: 1.3607 - val_mywloss: 1.3607\n",
      "Epoch 3/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.6559 - mywloss: 1.6559 - val_loss: 1.2670 - val_mywloss: 1.2670\n",
      "Epoch 4/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.5020 - mywloss: 1.5020 - val_loss: 1.1743 - val_mywloss: 1.1743\n",
      "Epoch 5/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.4467 - mywloss: 1.4467 - val_loss: 1.1430 - val_mywloss: 1.1430\n",
      "Epoch 6/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.3773 - mywloss: 1.3773 - val_loss: 1.1016 - val_mywloss: 1.1016\n",
      "Epoch 7/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 1.2897 - mywloss: 1.2897 - val_loss: 1.0655 - val_mywloss: 1.0655\n",
      "Epoch 8/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 1.2296 - mywloss: 1.2296 - val_loss: 1.0405 - val_mywloss: 1.0405\n",
      "Epoch 9/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.2242 - mywloss: 1.2242 - val_loss: 1.0468 - val_mywloss: 1.0468\n",
      "Epoch 10/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.1561 - mywloss: 1.1561 - val_loss: 1.0325 - val_mywloss: 1.0325\n",
      "Epoch 11/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 1.1617 - mywloss: 1.1617 - val_loss: 1.0001 - val_mywloss: 1.0001\n",
      "Epoch 12/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.1355 - mywloss: 1.1355 - val_loss: 1.0048 - val_mywloss: 1.0048\n",
      "Epoch 13/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.0894 - mywloss: 1.0894 - val_loss: 0.9575 - val_mywloss: 0.9575\n",
      "Epoch 14/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.0683 - mywloss: 1.0683 - val_loss: 0.9393 - val_mywloss: 0.9393\n",
      "Epoch 15/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.0521 - mywloss: 1.0521 - val_loss: 0.9378 - val_mywloss: 0.9378\n",
      "Epoch 16/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.0517 - mywloss: 1.0517 - val_loss: 0.9420 - val_mywloss: 0.9420\n",
      "Epoch 17/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 1.0493 - mywloss: 1.0493 - val_loss: 0.9441 - val_mywloss: 0.9441\n",
      "Epoch 18/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9976 - mywloss: 0.9976 - val_loss: 0.9468 - val_mywloss: 0.9468\n",
      "Epoch 19/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9972 - mywloss: 0.9972 - val_loss: 0.9261 - val_mywloss: 0.9261\n",
      "Epoch 20/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9881 - mywloss: 0.9881 - val_loss: 0.9212 - val_mywloss: 0.9212\n",
      "Epoch 21/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.9742 - mywloss: 0.9742 - val_loss: 0.9231 - val_mywloss: 0.9231\n",
      "Epoch 22/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.9514 - mywloss: 0.9514 - val_loss: 0.9001 - val_mywloss: 0.9001\n",
      "Epoch 23/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9569 - mywloss: 0.9569 - val_loss: 0.9236 - val_mywloss: 0.9236\n",
      "Epoch 24/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9505 - mywloss: 0.9505 - val_loss: 0.8948 - val_mywloss: 0.8948\n",
      "Epoch 25/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9326 - mywloss: 0.9326 - val_loss: 0.9001 - val_mywloss: 0.9001\n",
      "Epoch 26/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9347 - mywloss: 0.9347 - val_loss: 0.9054 - val_mywloss: 0.9054\n",
      "Epoch 27/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9466 - mywloss: 0.9466 - val_loss: 0.8783 - val_mywloss: 0.8783\n",
      "Epoch 28/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.9063 - mywloss: 0.9063 - val_loss: 0.8751 - val_mywloss: 0.8751\n",
      "Epoch 29/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.8954 - mywloss: 0.8954 - val_loss: 0.8713 - val_mywloss: 0.8713\n",
      "Epoch 30/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8858 - mywloss: 0.8858 - val_loss: 0.8826 - val_mywloss: 0.8826\n",
      "Epoch 31/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8876 - mywloss: 0.8876 - val_loss: 0.8584 - val_mywloss: 0.8584\n",
      "Epoch 32/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8745 - mywloss: 0.8745 - val_loss: 0.8583 - val_mywloss: 0.8583\n",
      "Epoch 33/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8659 - mywloss: 0.8659 - val_loss: 0.8507 - val_mywloss: 0.8507\n",
      "Epoch 34/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8871 - mywloss: 0.8871 - val_loss: 0.8721 - val_mywloss: 0.8721\n",
      "Epoch 35/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8750 - mywloss: 0.8750 - val_loss: 0.8454 - val_mywloss: 0.8454\n",
      "Epoch 36/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8583 - mywloss: 0.8583 - val_loss: 0.8624 - val_mywloss: 0.8624\n",
      "Epoch 37/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8631 - mywloss: 0.8631 - val_loss: 0.8406 - val_mywloss: 0.8406\n",
      "Epoch 38/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8501 - mywloss: 0.8501 - val_loss: 0.8598 - val_mywloss: 0.8598\n",
      "Epoch 39/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8518 - mywloss: 0.8518 - val_loss: 0.8270 - val_mywloss: 0.8270\n",
      "Epoch 40/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8346 - mywloss: 0.8346 - val_loss: 0.8342 - val_mywloss: 0.8342\n",
      "Epoch 41/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8264 - mywloss: 0.8264 - val_loss: 0.8182 - val_mywloss: 0.8182\n",
      "Epoch 42/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8647 - mywloss: 0.8647 - val_loss: 0.8440 - val_mywloss: 0.8440\n",
      "Epoch 43/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.8437 - mywloss: 0.8437 - val_loss: 0.8411 - val_mywloss: 0.8411\n",
      "Epoch 44/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8275 - mywloss: 0.8275 - val_loss: 0.8401 - val_mywloss: 0.8401\n",
      "Epoch 45/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8247 - mywloss: 0.8247 - val_loss: 0.8341 - val_mywloss: 0.8341\n",
      "Epoch 46/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8340 - mywloss: 0.8340 - val_loss: 0.8448 - val_mywloss: 0.8448\n",
      "Epoch 47/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8105 - mywloss: 0.8105 - val_loss: 0.8343 - val_mywloss: 0.8343\n",
      "Epoch 48/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8269 - mywloss: 0.8269 - val_loss: 0.8289 - val_mywloss: 0.8289\n",
      "Epoch 49/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8137 - mywloss: 0.8137 - val_loss: 0.8741 - val_mywloss: 0.8741\n",
      "Epoch 50/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8109 - mywloss: 0.8109 - val_loss: 0.8155 - val_mywloss: 0.8155\n",
      "Epoch 51/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7870 - mywloss: 0.7870 - val_loss: 0.8364 - val_mywloss: 0.8364\n",
      "Epoch 52/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7987 - mywloss: 0.7987 - val_loss: 0.8062 - val_mywloss: 0.8062\n",
      "Epoch 53/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8033 - mywloss: 0.8033 - val_loss: 0.8235 - val_mywloss: 0.8235\n",
      "Epoch 54/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8044 - mywloss: 0.8044 - val_loss: 0.8478 - val_mywloss: 0.8478\n",
      "Epoch 55/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7993 - mywloss: 0.7993 - val_loss: 0.8102 - val_mywloss: 0.8102\n",
      "Epoch 56/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8021 - mywloss: 0.8021 - val_loss: 0.8064 - val_mywloss: 0.8064\n",
      "Epoch 57/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7866 - mywloss: 0.7866 - val_loss: 0.8377 - val_mywloss: 0.8377\n",
      "Epoch 58/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7732 - mywloss: 0.7732 - val_loss: 0.8054 - val_mywloss: 0.8054\n",
      "Epoch 59/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.8184 - mywloss: 0.8184 - val_loss: 0.8193 - val_mywloss: 0.8193\n",
      "Epoch 60/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7770 - mywloss: 0.7770 - val_loss: 0.8055 - val_mywloss: 0.8055\n",
      "Epoch 61/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7683 - mywloss: 0.7683 - val_loss: 0.7999 - val_mywloss: 0.7999\n",
      "Epoch 62/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7713 - mywloss: 0.7713 - val_loss: 0.8049 - val_mywloss: 0.8049\n",
      "Epoch 63/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7964 - mywloss: 0.7964 - val_loss: 0.8256 - val_mywloss: 0.8256\n",
      "Epoch 64/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7645 - mywloss: 0.7645 - val_loss: 0.8034 - val_mywloss: 0.8034\n",
      "Epoch 65/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.7581 - mywloss: 0.7581 - val_loss: 0.8178 - val_mywloss: 0.8178\n",
      "Epoch 66/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7840 - mywloss: 0.7840 - val_loss: 0.7736 - val_mywloss: 0.7736\n",
      "Epoch 67/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7753 - mywloss: 0.7753 - val_loss: 0.7689 - val_mywloss: 0.7689\n",
      "Epoch 68/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7592 - mywloss: 0.7592 - val_loss: 0.7869 - val_mywloss: 0.7869\n",
      "Epoch 69/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7613 - mywloss: 0.7613 - val_loss: 0.7756 - val_mywloss: 0.7756\n",
      "Epoch 70/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7595 - mywloss: 0.7595 - val_loss: 0.7961 - val_mywloss: 0.7961\n",
      "Epoch 71/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7568 - mywloss: 0.7568 - val_loss: 0.7888 - val_mywloss: 0.7888\n",
      "Epoch 72/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7509 - mywloss: 0.7509 - val_loss: 0.7555 - val_mywloss: 0.7555\n",
      "Epoch 73/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7456 - mywloss: 0.7456 - val_loss: 0.7715 - val_mywloss: 0.7715\n",
      "Epoch 74/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7479 - mywloss: 0.7479 - val_loss: 0.7669 - val_mywloss: 0.7669\n",
      "Epoch 75/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7294 - mywloss: 0.7294 - val_loss: 0.7462 - val_mywloss: 0.7462\n",
      "Epoch 76/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7310 - mywloss: 0.7310 - val_loss: 0.7797 - val_mywloss: 0.7797\n",
      "Epoch 77/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7414 - mywloss: 0.7414 - val_loss: 0.7660 - val_mywloss: 0.7660\n",
      "Epoch 78/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7527 - mywloss: 0.7527 - val_loss: 0.7800 - val_mywloss: 0.7800\n",
      "Epoch 79/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7374 - mywloss: 0.7374 - val_loss: 0.7879 - val_mywloss: 0.7879\n",
      "Epoch 80/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7418 - mywloss: 0.7418 - val_loss: 0.7852 - val_mywloss: 0.7852\n",
      "Epoch 81/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7301 - mywloss: 0.7301 - val_loss: 0.7481 - val_mywloss: 0.7481\n",
      "Epoch 82/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7308 - mywloss: 0.7308 - val_loss: 0.7498 - val_mywloss: 0.7498\n",
      "Epoch 83/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7426 - mywloss: 0.7426 - val_loss: 0.7708 - val_mywloss: 0.7708\n",
      "Epoch 84/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7279 - mywloss: 0.7279 - val_loss: 0.7478 - val_mywloss: 0.7478\n",
      "Epoch 85/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7260 - mywloss: 0.7260 - val_loss: 0.7660 - val_mywloss: 0.7660\n",
      "Epoch 86/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7508 - mywloss: 0.7508 - val_loss: 0.7830 - val_mywloss: 0.7830\n",
      "Epoch 87/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7365 - mywloss: 0.7365 - val_loss: 0.7565 - val_mywloss: 0.7565\n",
      "Epoch 88/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7056 - mywloss: 0.7056 - val_loss: 0.7671 - val_mywloss: 0.7671\n",
      "Epoch 89/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7135 - mywloss: 0.7135 - val_loss: 0.7408 - val_mywloss: 0.7408\n",
      "Epoch 90/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7220 - mywloss: 0.7220 - val_loss: 0.7687 - val_mywloss: 0.7687\n",
      "Epoch 91/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7067 - mywloss: 0.7067 - val_loss: 0.7534 - val_mywloss: 0.7534\n",
      "Epoch 92/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7054 - mywloss: 0.7054 - val_loss: 0.7806 - val_mywloss: 0.7806\n",
      "Epoch 93/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7027 - mywloss: 0.7027 - val_loss: 0.7603 - val_mywloss: 0.7603\n",
      "Epoch 94/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.7020 - mywloss: 0.7020 - val_loss: 0.7524 - val_mywloss: 0.7524\n",
      "Epoch 95/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7241 - mywloss: 0.7241 - val_loss: 0.7616 - val_mywloss: 0.7616\n",
      "Epoch 96/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7171 - mywloss: 0.7171 - val_loss: 0.7531 - val_mywloss: 0.7531\n",
      "Epoch 97/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7000 - mywloss: 0.7000 - val_loss: 0.7475 - val_mywloss: 0.7475\n",
      "Epoch 98/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7057 - mywloss: 0.7057 - val_loss: 0.7486 - val_mywloss: 0.7486\n",
      "Epoch 99/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.6906 - mywloss: 0.6906 - val_loss: 0.7482 - val_mywloss: 0.7482\n",
      "Epoch 100/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6979 - mywloss: 0.6979 - val_loss: 0.7667 - val_mywloss: 0.7667\n",
      "Epoch 101/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7045 - mywloss: 0.7045 - val_loss: 0.7516 - val_mywloss: 0.7516\n",
      "Epoch 102/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6878 - mywloss: 0.6878 - val_loss: 0.7650 - val_mywloss: 0.7650\n",
      "Epoch 103/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6917 - mywloss: 0.6917 - val_loss: 0.7392 - val_mywloss: 0.7392\n",
      "Epoch 104/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6612 - mywloss: 0.6612 - val_loss: 0.7342 - val_mywloss: 0.7342\n",
      "Epoch 105/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7162 - mywloss: 0.7162 - val_loss: 0.7481 - val_mywloss: 0.7481\n",
      "Epoch 106/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.7060 - mywloss: 0.7060 - val_loss: 0.7309 - val_mywloss: 0.7309\n",
      "Epoch 107/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6839 - mywloss: 0.6839 - val_loss: 0.7316 - val_mywloss: 0.7316\n",
      "Epoch 108/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6947 - mywloss: 0.6947 - val_loss: 0.7445 - val_mywloss: 0.7445\n",
      "Epoch 109/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6950 - mywloss: 0.6950 - val_loss: 0.7414 - val_mywloss: 0.7414\n",
      "Epoch 110/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6958 - mywloss: 0.6958 - val_loss: 0.7394 - val_mywloss: 0.7394\n",
      "Epoch 111/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6568 - mywloss: 0.6568 - val_loss: 0.7164 - val_mywloss: 0.7164\n",
      "Epoch 112/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6927 - mywloss: 0.6927 - val_loss: 0.7374 - val_mywloss: 0.7374\n",
      "Epoch 113/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6640 - mywloss: 0.6640 - val_loss: 0.7262 - val_mywloss: 0.7262\n",
      "Epoch 114/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6830 - mywloss: 0.6830 - val_loss: 0.7385 - val_mywloss: 0.7385\n",
      "Epoch 115/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6840 - mywloss: 0.6840 - val_loss: 0.7476 - val_mywloss: 0.7476\n",
      "Epoch 116/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6734 - mywloss: 0.6734 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 117/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6667 - mywloss: 0.6667 - val_loss: 0.7403 - val_mywloss: 0.7403\n",
      "Epoch 118/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6911 - mywloss: 0.6911 - val_loss: 0.7330 - val_mywloss: 0.7330\n",
      "Epoch 119/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6871 - mywloss: 0.6871 - val_loss: 0.7530 - val_mywloss: 0.7530\n",
      "Epoch 120/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6649 - mywloss: 0.6649 - val_loss: 0.7271 - val_mywloss: 0.7271\n",
      "Epoch 121/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6889 - mywloss: 0.6889 - val_loss: 0.7466 - val_mywloss: 0.7466\n",
      "Epoch 122/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6718 - mywloss: 0.6718 - val_loss: 0.7396 - val_mywloss: 0.7396\n",
      "Epoch 123/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6672 - mywloss: 0.6672 - val_loss: 0.7320 - val_mywloss: 0.7320\n",
      "Epoch 124/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6879 - mywloss: 0.6879 - val_loss: 0.7205 - val_mywloss: 0.7205\n",
      "Epoch 125/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6902 - mywloss: 0.6902 - val_loss: 0.7213 - val_mywloss: 0.7213\n",
      "Epoch 126/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6809 - mywloss: 0.6809 - val_loss: 0.7587 - val_mywloss: 0.7587\n",
      "Epoch 127/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6745 - mywloss: 0.6745 - val_loss: 0.7098 - val_mywloss: 0.7098\n",
      "Epoch 128/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6670 - mywloss: 0.6670 - val_loss: 0.7276 - val_mywloss: 0.7276\n",
      "Epoch 129/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6619 - mywloss: 0.6619 - val_loss: 0.7339 - val_mywloss: 0.7339\n",
      "Epoch 130/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6497 - mywloss: 0.6497 - val_loss: 0.7413 - val_mywloss: 0.7413\n",
      "Epoch 131/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6751 - mywloss: 0.6751 - val_loss: 0.7365 - val_mywloss: 0.7365\n",
      "Epoch 132/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6314 - mywloss: 0.6314 - val_loss: 0.7244 - val_mywloss: 0.7244\n",
      "Epoch 133/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6483 - mywloss: 0.6483 - val_loss: 0.7126 - val_mywloss: 0.7126\n",
      "Epoch 134/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6460 - mywloss: 0.6460 - val_loss: 0.7083 - val_mywloss: 0.7083\n",
      "Epoch 135/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6751 - mywloss: 0.6751 - val_loss: 0.7231 - val_mywloss: 0.7231\n",
      "Epoch 136/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6589 - mywloss: 0.6589 - val_loss: 0.7350 - val_mywloss: 0.7350\n",
      "Epoch 137/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6500 - mywloss: 0.6500 - val_loss: 0.7202 - val_mywloss: 0.7202\n",
      "Epoch 138/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6505 - mywloss: 0.6505 - val_loss: 0.7107 - val_mywloss: 0.7107\n",
      "Epoch 139/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6546 - mywloss: 0.6546 - val_loss: 0.7242 - val_mywloss: 0.7242\n",
      "Epoch 140/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6515 - mywloss: 0.6515 - val_loss: 0.7273 - val_mywloss: 0.7273\n",
      "Epoch 141/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6498 - mywloss: 0.6498 - val_loss: 0.7224 - val_mywloss: 0.7224\n",
      "Epoch 142/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6528 - mywloss: 0.6528 - val_loss: 0.7216 - val_mywloss: 0.7216\n",
      "Epoch 143/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6490 - mywloss: 0.6490 - val_loss: 0.7511 - val_mywloss: 0.7511\n",
      "Epoch 144/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6532 - mywloss: 0.6532 - val_loss: 0.6953 - val_mywloss: 0.6953\n",
      "Epoch 145/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6521 - mywloss: 0.6521 - val_loss: 0.7335 - val_mywloss: 0.7335\n",
      "Epoch 146/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6516 - mywloss: 0.6516 - val_loss: 0.7230 - val_mywloss: 0.7230\n",
      "Epoch 147/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6504 - mywloss: 0.6504 - val_loss: 0.7066 - val_mywloss: 0.7066\n",
      "Epoch 148/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6439 - mywloss: 0.6439 - val_loss: 0.6960 - val_mywloss: 0.6960\n",
      "Epoch 149/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6382 - mywloss: 0.6382 - val_loss: 0.6999 - val_mywloss: 0.6999\n",
      "Epoch 150/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6569 - mywloss: 0.6569 - val_loss: 0.6887 - val_mywloss: 0.6887\n",
      "Epoch 151/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6412 - mywloss: 0.6412 - val_loss: 0.7051 - val_mywloss: 0.7051\n",
      "Epoch 152/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.6352 - mywloss: 0.6352 - val_loss: 0.7128 - val_mywloss: 0.7128\n",
      "Epoch 153/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6534 - mywloss: 0.6534 - val_loss: 0.7062 - val_mywloss: 0.7062\n",
      "Epoch 154/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6242 - mywloss: 0.6242 - val_loss: 0.6970 - val_mywloss: 0.6970\n",
      "Epoch 155/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6338 - mywloss: 0.6338 - val_loss: 0.7141 - val_mywloss: 0.7141\n",
      "Epoch 156/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6279 - mywloss: 0.6279 - val_loss: 0.7006 - val_mywloss: 0.7006\n",
      "Epoch 157/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6396 - mywloss: 0.6396 - val_loss: 0.7231 - val_mywloss: 0.7231\n",
      "Epoch 158/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6308 - mywloss: 0.6308 - val_loss: 0.7059 - val_mywloss: 0.7059\n",
      "Epoch 159/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6148 - mywloss: 0.6148 - val_loss: 0.6849 - val_mywloss: 0.6849\n",
      "Epoch 160/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6444 - mywloss: 0.6444 - val_loss: 0.7166 - val_mywloss: 0.7166\n",
      "Epoch 161/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6473 - mywloss: 0.6473 - val_loss: 0.7118 - val_mywloss: 0.7118\n",
      "Epoch 162/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6321 - mywloss: 0.6321 - val_loss: 0.6857 - val_mywloss: 0.6857\n",
      "Epoch 163/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6352 - mywloss: 0.6352 - val_loss: 0.6910 - val_mywloss: 0.6910\n",
      "Epoch 164/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6377 - mywloss: 0.6377 - val_loss: 0.6833 - val_mywloss: 0.6833\n",
      "Epoch 165/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6156 - mywloss: 0.6156 - val_loss: 0.6796 - val_mywloss: 0.6796\n",
      "Epoch 166/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6523 - mywloss: 0.6523 - val_loss: 0.7526 - val_mywloss: 0.7526\n",
      "Epoch 167/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6177 - mywloss: 0.6177 - val_loss: 0.7141 - val_mywloss: 0.7141\n",
      "Epoch 168/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6104 - mywloss: 0.6104 - val_loss: 0.6996 - val_mywloss: 0.6996\n",
      "Epoch 169/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6128 - mywloss: 0.6128 - val_loss: 0.7180 - val_mywloss: 0.7180\n",
      "Epoch 170/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.6143 - mywloss: 0.6143 - val_loss: 0.7043 - val_mywloss: 0.7043\n",
      "Epoch 171/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6197 - mywloss: 0.6197 - val_loss: 0.7072 - val_mywloss: 0.7072\n",
      "Epoch 172/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6247 - mywloss: 0.6247 - val_loss: 0.6977 - val_mywloss: 0.6977\n",
      "Epoch 173/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6125 - mywloss: 0.6125 - val_loss: 0.6978 - val_mywloss: 0.6978\n",
      "Epoch 174/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6139 - mywloss: 0.6139 - val_loss: 0.6772 - val_mywloss: 0.6772\n",
      "Epoch 175/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6320 - mywloss: 0.6320 - val_loss: 0.6957 - val_mywloss: 0.6957\n",
      "Epoch 176/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6364 - mywloss: 0.6364 - val_loss: 0.7135 - val_mywloss: 0.7135\n",
      "Epoch 177/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6168 - mywloss: 0.6168 - val_loss: 0.6837 - val_mywloss: 0.6837\n",
      "Epoch 178/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6165 - mywloss: 0.6165 - val_loss: 0.7081 - val_mywloss: 0.7081\n",
      "Epoch 179/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6356 - mywloss: 0.6356 - val_loss: 0.6996 - val_mywloss: 0.6996\n",
      "Epoch 180/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5970 - mywloss: 0.5970 - val_loss: 0.6797 - val_mywloss: 0.6797\n",
      "Epoch 181/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6033 - mywloss: 0.6033 - val_loss: 0.6741 - val_mywloss: 0.6741\n",
      "Epoch 182/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6035 - mywloss: 0.6035 - val_loss: 0.7234 - val_mywloss: 0.7234\n",
      "Epoch 183/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6069 - mywloss: 0.6069 - val_loss: 0.6773 - val_mywloss: 0.6773\n",
      "Epoch 184/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6076 - mywloss: 0.6076 - val_loss: 0.6832 - val_mywloss: 0.6832\n",
      "Epoch 185/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5885 - mywloss: 0.5885 - val_loss: 0.6632 - val_mywloss: 0.6632\n",
      "Epoch 186/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6186 - mywloss: 0.6186 - val_loss: 0.6749 - val_mywloss: 0.6749\n",
      "Epoch 187/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6040 - mywloss: 0.6040 - val_loss: 0.6861 - val_mywloss: 0.6861\n",
      "Epoch 188/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6312 - mywloss: 0.6312 - val_loss: 0.6914 - val_mywloss: 0.6914\n",
      "Epoch 189/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6025 - mywloss: 0.6025 - val_loss: 0.6672 - val_mywloss: 0.6672\n",
      "Epoch 190/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6218 - mywloss: 0.6218 - val_loss: 0.6565 - val_mywloss: 0.6565\n",
      "Epoch 191/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6332 - mywloss: 0.6332 - val_loss: 0.6999 - val_mywloss: 0.6999\n",
      "Epoch 192/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.6124 - mywloss: 0.6124 - val_loss: 0.7076 - val_mywloss: 0.7076\n",
      "Epoch 193/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6124 - mywloss: 0.6124 - val_loss: 0.6837 - val_mywloss: 0.6837\n",
      "Epoch 194/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.6304 - mywloss: 0.6304 - val_loss: 0.6711 - val_mywloss: 0.6711\n",
      "Epoch 195/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6050 - mywloss: 0.6050 - val_loss: 0.6940 - val_mywloss: 0.6940\n",
      "Epoch 196/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6096 - mywloss: 0.6096 - val_loss: 0.6999 - val_mywloss: 0.6999\n",
      "Epoch 197/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5905 - mywloss: 0.5905 - val_loss: 0.7064 - val_mywloss: 0.7064\n",
      "Epoch 198/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5881 - mywloss: 0.5881 - val_loss: 0.6936 - val_mywloss: 0.6936\n",
      "Epoch 199/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6013 - mywloss: 0.6013 - val_loss: 0.7162 - val_mywloss: 0.7162\n",
      "Epoch 200/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5875 - mywloss: 0.5875 - val_loss: 0.6732 - val_mywloss: 0.6732\n",
      "Epoch 201/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5806 - mywloss: 0.5806 - val_loss: 0.6891 - val_mywloss: 0.6891\n",
      "Epoch 202/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5943 - mywloss: 0.5943 - val_loss: 0.6777 - val_mywloss: 0.6777\n",
      "Epoch 203/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6068 - mywloss: 0.6068 - val_loss: 0.6797 - val_mywloss: 0.6797\n",
      "Epoch 204/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5926 - mywloss: 0.5926 - val_loss: 0.6930 - val_mywloss: 0.6930\n",
      "Epoch 205/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5860 - mywloss: 0.5860 - val_loss: 0.7115 - val_mywloss: 0.7115\n",
      "Epoch 206/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6047 - mywloss: 0.6047 - val_loss: 0.6831 - val_mywloss: 0.6831\n",
      "Epoch 207/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6108 - mywloss: 0.6108 - val_loss: 0.6995 - val_mywloss: 0.6995\n",
      "Epoch 208/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5922 - mywloss: 0.5922 - val_loss: 0.6790 - val_mywloss: 0.6790\n",
      "Epoch 209/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.6051 - mywloss: 0.6051 - val_loss: 0.6721 - val_mywloss: 0.6721\n",
      "Epoch 210/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5836 - mywloss: 0.5836 - val_loss: 0.7043 - val_mywloss: 0.7043\n",
      "Epoch 211/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6095 - mywloss: 0.6095 - val_loss: 0.7043 - val_mywloss: 0.7043\n",
      "Epoch 212/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.6121 - mywloss: 0.6121 - val_loss: 0.6777 - val_mywloss: 0.6777\n",
      "Epoch 213/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5937 - mywloss: 0.5937 - val_loss: 0.6756 - val_mywloss: 0.6756\n",
      "Epoch 214/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5977 - mywloss: 0.5977 - val_loss: 0.6897 - val_mywloss: 0.6897\n",
      "Epoch 215/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5747 - mywloss: 0.5747 - val_loss: 0.6760 - val_mywloss: 0.6760\n",
      "Epoch 216/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5945 - mywloss: 0.5945 - val_loss: 0.6821 - val_mywloss: 0.6821\n",
      "Epoch 217/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5959 - mywloss: 0.5959 - val_loss: 0.6846 - val_mywloss: 0.6846\n",
      "Epoch 218/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5715 - mywloss: 0.5715 - val_loss: 0.6943 - val_mywloss: 0.6943\n",
      "Epoch 219/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5584 - mywloss: 0.5584 - val_loss: 0.6845 - val_mywloss: 0.6845\n",
      "Epoch 220/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5913 - mywloss: 0.5913 - val_loss: 0.6664 - val_mywloss: 0.6664\n",
      "Epoch 221/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5788 - mywloss: 0.5788 - val_loss: 0.6772 - val_mywloss: 0.6772\n",
      "Epoch 222/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5894 - mywloss: 0.5894 - val_loss: 0.6769 - val_mywloss: 0.6769\n",
      "Epoch 223/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5711 - mywloss: 0.5711 - val_loss: 0.6806 - val_mywloss: 0.6806\n",
      "Epoch 224/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5727 - mywloss: 0.5727 - val_loss: 0.6805 - val_mywloss: 0.6805\n",
      "Epoch 225/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5786 - mywloss: 0.5786 - val_loss: 0.6822 - val_mywloss: 0.6822\n",
      "Epoch 226/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5949 - mywloss: 0.5949 - val_loss: 0.6534 - val_mywloss: 0.6534\n",
      "Epoch 227/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5756 - mywloss: 0.5756 - val_loss: 0.6690 - val_mywloss: 0.6690\n",
      "Epoch 228/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5787 - mywloss: 0.5787 - val_loss: 0.6768 - val_mywloss: 0.6768\n",
      "Epoch 229/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5656 - mywloss: 0.5656 - val_loss: 0.6649 - val_mywloss: 0.6649\n",
      "Epoch 230/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5932 - mywloss: 0.5932 - val_loss: 0.6726 - val_mywloss: 0.6726\n",
      "Epoch 231/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5682 - mywloss: 0.5682 - val_loss: 0.6607 - val_mywloss: 0.6607\n",
      "Epoch 232/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5711 - mywloss: 0.5711 - val_loss: 0.6641 - val_mywloss: 0.6641\n",
      "Epoch 233/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5804 - mywloss: 0.5804 - val_loss: 0.6782 - val_mywloss: 0.6782\n",
      "Epoch 234/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5805 - mywloss: 0.5805 - val_loss: 0.6876 - val_mywloss: 0.6876\n",
      "Epoch 235/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5688 - mywloss: 0.5688 - val_loss: 0.6921 - val_mywloss: 0.6921\n",
      "Epoch 236/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5785 - mywloss: 0.5785 - val_loss: 0.6835 - val_mywloss: 0.6835\n",
      "Epoch 237/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5639 - mywloss: 0.5639 - val_loss: 0.6703 - val_mywloss: 0.6703\n",
      "Epoch 238/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5759 - mywloss: 0.5759 - val_loss: 0.6745 - val_mywloss: 0.6745\n",
      "Epoch 239/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5557 - mywloss: 0.5557 - val_loss: 0.6718 - val_mywloss: 0.6718\n",
      "Epoch 240/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5829 - mywloss: 0.5829 - val_loss: 0.6887 - val_mywloss: 0.6887\n",
      "Epoch 241/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5667 - mywloss: 0.5667 - val_loss: 0.6964 - val_mywloss: 0.6964\n",
      "Epoch 242/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5650 - mywloss: 0.5650 - val_loss: 0.6859 - val_mywloss: 0.6859\n",
      "Epoch 243/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5788 - mywloss: 0.5788 - val_loss: 0.6708 - val_mywloss: 0.6708\n",
      "Epoch 244/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5794 - mywloss: 0.5794 - val_loss: 0.6877 - val_mywloss: 0.6877\n",
      "Epoch 245/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5659 - mywloss: 0.5659 - val_loss: 0.6874 - val_mywloss: 0.6874\n",
      "Epoch 246/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5636 - mywloss: 0.5636 - val_loss: 0.6818 - val_mywloss: 0.6818\n",
      "Epoch 247/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5692 - mywloss: 0.5692 - val_loss: 0.7150 - val_mywloss: 0.7150\n",
      "Epoch 248/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5767 - mywloss: 0.5767 - val_loss: 0.6879 - val_mywloss: 0.6879\n",
      "Epoch 249/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5542 - mywloss: 0.5542 - val_loss: 0.6753 - val_mywloss: 0.6753\n",
      "Epoch 250/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5815 - mywloss: 0.5815 - val_loss: 0.6668 - val_mywloss: 0.6668\n",
      "Epoch 251/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5532 - mywloss: 0.5532 - val_loss: 0.6601 - val_mywloss: 0.6601\n",
      "Epoch 252/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5604 - mywloss: 0.5604 - val_loss: 0.6843 - val_mywloss: 0.6843\n",
      "Epoch 253/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5707 - mywloss: 0.5707 - val_loss: 0.6682 - val_mywloss: 0.6682\n",
      "Epoch 254/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5593 - mywloss: 0.5593 - val_loss: 0.6617 - val_mywloss: 0.6617\n",
      "Epoch 255/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5607 - mywloss: 0.5607 - val_loss: 0.6733 - val_mywloss: 0.6733\n",
      "Epoch 256/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5648 - mywloss: 0.5648 - val_loss: 0.6650 - val_mywloss: 0.6650\n",
      "Epoch 257/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5477 - mywloss: 0.5477 - val_loss: 0.6705 - val_mywloss: 0.6705\n",
      "Epoch 258/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5487 - mywloss: 0.5487 - val_loss: 0.6568 - val_mywloss: 0.6568\n",
      "Epoch 259/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5673 - mywloss: 0.5673 - val_loss: 0.6704 - val_mywloss: 0.6704\n",
      "Epoch 260/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5697 - mywloss: 0.5697 - val_loss: 0.6567 - val_mywloss: 0.6567\n",
      "Epoch 261/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5835 - mywloss: 0.5835 - val_loss: 0.6601 - val_mywloss: 0.6601\n",
      "Epoch 262/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5647 - mywloss: 0.5647 - val_loss: 0.6830 - val_mywloss: 0.6830\n",
      "Epoch 263/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5466 - mywloss: 0.5466 - val_loss: 0.6880 - val_mywloss: 0.6880\n",
      "Epoch 264/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5914 - mywloss: 0.5914 - val_loss: 0.7031 - val_mywloss: 0.7031\n",
      "Epoch 265/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5635 - mywloss: 0.5635 - val_loss: 0.6764 - val_mywloss: 0.6764\n",
      "Epoch 266/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5751 - mywloss: 0.5751 - val_loss: 0.6895 - val_mywloss: 0.6895\n",
      "Epoch 267/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5639 - mywloss: 0.5639 - val_loss: 0.6972 - val_mywloss: 0.6972\n",
      "Epoch 268/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5570 - mywloss: 0.5570 - val_loss: 0.6938 - val_mywloss: 0.6938\n",
      "Epoch 269/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5586 - mywloss: 0.5586 - val_loss: 0.6962 - val_mywloss: 0.6962\n",
      "Epoch 270/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5535 - mywloss: 0.5535 - val_loss: 0.6861 - val_mywloss: 0.6861\n",
      "Epoch 271/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5339 - mywloss: 0.5339 - val_loss: 0.6691 - val_mywloss: 0.6691\n",
      "Epoch 272/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5447 - mywloss: 0.5447 - val_loss: 0.6838 - val_mywloss: 0.6838\n",
      "Epoch 273/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5480 - mywloss: 0.5480 - val_loss: 0.6963 - val_mywloss: 0.6963\n",
      "Epoch 274/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5857 - mywloss: 0.5857 - val_loss: 0.6891 - val_mywloss: 0.6891\n",
      "Epoch 275/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5692 - mywloss: 0.5692 - val_loss: 0.6862 - val_mywloss: 0.6862\n",
      "Epoch 276/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5547 - mywloss: 0.5547 - val_loss: 0.6747 - val_mywloss: 0.6747\n",
      "Epoch 277/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5484 - mywloss: 0.5484 - val_loss: 0.6613 - val_mywloss: 0.6613\n",
      "Epoch 278/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5824 - mywloss: 0.5824 - val_loss: 0.6864 - val_mywloss: 0.6864\n",
      "Epoch 279/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5601 - mywloss: 0.5601 - val_loss: 0.6712 - val_mywloss: 0.6712\n",
      "Epoch 280/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5390 - mywloss: 0.5390 - val_loss: 0.6777 - val_mywloss: 0.6777\n",
      "Epoch 281/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5657 - mywloss: 0.5657 - val_loss: 0.6864 - val_mywloss: 0.6864\n",
      "Epoch 282/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5450 - mywloss: 0.5450 - val_loss: 0.6908 - val_mywloss: 0.6908\n",
      "Epoch 283/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5351 - mywloss: 0.5351 - val_loss: 0.6591 - val_mywloss: 0.6591\n",
      "Epoch 284/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5609 - mywloss: 0.5609 - val_loss: 0.6548 - val_mywloss: 0.6548\n",
      "Epoch 285/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5486 - mywloss: 0.5486 - val_loss: 0.6837 - val_mywloss: 0.6837\n",
      "Epoch 286/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5325 - mywloss: 0.5325 - val_loss: 0.6700 - val_mywloss: 0.6700\n",
      "Epoch 287/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5475 - mywloss: 0.5475 - val_loss: 0.6583 - val_mywloss: 0.6583\n",
      "Epoch 288/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5659 - mywloss: 0.5659 - val_loss: 0.6506 - val_mywloss: 0.6506\n",
      "Epoch 289/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5346 - mywloss: 0.5346 - val_loss: 0.6670 - val_mywloss: 0.6670\n",
      "Epoch 290/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5379 - mywloss: 0.5379 - val_loss: 0.6850 - val_mywloss: 0.6850\n",
      "Epoch 291/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5495 - mywloss: 0.5495 - val_loss: 0.6751 - val_mywloss: 0.6751\n",
      "Epoch 292/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5412 - mywloss: 0.5412 - val_loss: 0.6740 - val_mywloss: 0.6740\n",
      "Epoch 293/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5354 - mywloss: 0.5354 - val_loss: 0.6621 - val_mywloss: 0.6621\n",
      "Epoch 294/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5462 - mywloss: 0.5462 - val_loss: 0.6716 - val_mywloss: 0.6716\n",
      "Epoch 295/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5236 - mywloss: 0.5236 - val_loss: 0.6649 - val_mywloss: 0.6649\n",
      "Epoch 296/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5485 - mywloss: 0.5485 - val_loss: 0.6696 - val_mywloss: 0.6696\n",
      "Epoch 297/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5403 - mywloss: 0.5403 - val_loss: 0.6832 - val_mywloss: 0.6832\n",
      "Epoch 298/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5415 - mywloss: 0.5415 - val_loss: 0.6712 - val_mywloss: 0.6712\n",
      "Epoch 299/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5273 - mywloss: 0.5273 - val_loss: 0.6720 - val_mywloss: 0.6720\n",
      "Epoch 300/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5343 - mywloss: 0.5343 - val_loss: 0.6896 - val_mywloss: 0.6896\n",
      "Epoch 301/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5468 - mywloss: 0.5468 - val_loss: 0.6989 - val_mywloss: 0.6989\n",
      "Epoch 302/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5365 - mywloss: 0.5365 - val_loss: 0.6843 - val_mywloss: 0.6843\n",
      "Epoch 303/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5330 - mywloss: 0.5330 - val_loss: 0.6829 - val_mywloss: 0.6829\n",
      "Epoch 304/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5443 - mywloss: 0.5443 - val_loss: 0.6788 - val_mywloss: 0.6788\n",
      "Epoch 305/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5331 - mywloss: 0.5331 - val_loss: 0.6646 - val_mywloss: 0.6646\n",
      "Epoch 306/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5325 - mywloss: 0.5325 - val_loss: 0.6778 - val_mywloss: 0.6778\n",
      "Epoch 307/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5253 - mywloss: 0.5253 - val_loss: 0.6870 - val_mywloss: 0.6870\n",
      "Epoch 308/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5420 - mywloss: 0.5420 - val_loss: 0.6888 - val_mywloss: 0.6888\n",
      "Epoch 309/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5142 - mywloss: 0.5142 - val_loss: 0.6792 - val_mywloss: 0.6792\n",
      "Epoch 310/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5650 - mywloss: 0.5650 - val_loss: 0.6705 - val_mywloss: 0.6705\n",
      "Epoch 311/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5159 - mywloss: 0.5159 - val_loss: 0.6662 - val_mywloss: 0.6662\n",
      "Epoch 312/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5178 - mywloss: 0.5178 - val_loss: 0.6736 - val_mywloss: 0.6736\n",
      "Epoch 313/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5526 - mywloss: 0.5526 - val_loss: 0.6794 - val_mywloss: 0.6794\n",
      "Epoch 314/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5450 - mywloss: 0.5450 - val_loss: 0.6784 - val_mywloss: 0.6784\n",
      "Epoch 315/600\n",
      "6284/6284 [==============================] - 0s 66us/step - loss: 0.5172 - mywloss: 0.5172 - val_loss: 0.6663 - val_mywloss: 0.6663\n",
      "Epoch 316/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5137 - mywloss: 0.5137 - val_loss: 0.6707 - val_mywloss: 0.6707\n",
      "Epoch 317/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5369 - mywloss: 0.5369 - val_loss: 0.6722 - val_mywloss: 0.6722\n",
      "Epoch 318/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5424 - mywloss: 0.5424 - val_loss: 0.6768 - val_mywloss: 0.6768\n",
      "Epoch 319/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5318 - mywloss: 0.5318 - val_loss: 0.6580 - val_mywloss: 0.6580\n",
      "Epoch 320/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5166 - mywloss: 0.5166 - val_loss: 0.6791 - val_mywloss: 0.6791\n",
      "Epoch 321/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5434 - mywloss: 0.5434 - val_loss: 0.6905 - val_mywloss: 0.6905\n",
      "Epoch 322/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5290 - mywloss: 0.5290 - val_loss: 0.6680 - val_mywloss: 0.6680\n",
      "Epoch 323/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5232 - mywloss: 0.5232 - val_loss: 0.6753 - val_mywloss: 0.6753\n",
      "Epoch 324/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5251 - mywloss: 0.5251 - val_loss: 0.6730 - val_mywloss: 0.6730\n",
      "Epoch 325/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5225 - mywloss: 0.5225 - val_loss: 0.6701 - val_mywloss: 0.6701\n",
      "Epoch 326/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5102 - mywloss: 0.5102 - val_loss: 0.6673 - val_mywloss: 0.6673\n",
      "Epoch 327/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5087 - mywloss: 0.5087 - val_loss: 0.6504 - val_mywloss: 0.6504\n",
      "Epoch 328/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5312 - mywloss: 0.5312 - val_loss: 0.6821 - val_mywloss: 0.6821\n",
      "Epoch 329/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5250 - mywloss: 0.5250 - val_loss: 0.6830 - val_mywloss: 0.6830\n",
      "Epoch 330/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5195 - mywloss: 0.5195 - val_loss: 0.6722 - val_mywloss: 0.6722\n",
      "Epoch 331/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5294 - mywloss: 0.5294 - val_loss: 0.6844 - val_mywloss: 0.6844\n",
      "Epoch 332/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5237 - mywloss: 0.5237 - val_loss: 0.6724 - val_mywloss: 0.6724\n",
      "Epoch 333/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5270 - mywloss: 0.5270 - val_loss: 0.6770 - val_mywloss: 0.6770\n",
      "Epoch 334/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5332 - mywloss: 0.5332 - val_loss: 0.6639 - val_mywloss: 0.6639\n",
      "Epoch 335/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5170 - mywloss: 0.5170 - val_loss: 0.6646 - val_mywloss: 0.6646\n",
      "Epoch 336/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5103 - mywloss: 0.5103 - val_loss: 0.6756 - val_mywloss: 0.6756\n",
      "Epoch 337/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5060 - mywloss: 0.5060 - val_loss: 0.6718 - val_mywloss: 0.6718\n",
      "Epoch 338/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5137 - mywloss: 0.5137 - val_loss: 0.6613 - val_mywloss: 0.6613\n",
      "Epoch 339/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5205 - mywloss: 0.5205 - val_loss: 0.6671 - val_mywloss: 0.6671\n",
      "Epoch 340/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5146 - mywloss: 0.5146 - val_loss: 0.6618 - val_mywloss: 0.6618\n",
      "Epoch 341/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5225 - mywloss: 0.5225 - val_loss: 0.6699 - val_mywloss: 0.6699\n",
      "Epoch 342/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5017 - mywloss: 0.5017 - val_loss: 0.6747 - val_mywloss: 0.6747\n",
      "Epoch 343/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5132 - mywloss: 0.5132 - val_loss: 0.7053 - val_mywloss: 0.7053\n",
      "Epoch 344/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5371 - mywloss: 0.5371 - val_loss: 0.6736 - val_mywloss: 0.6736\n",
      "Epoch 345/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5130 - mywloss: 0.5130 - val_loss: 0.6517 - val_mywloss: 0.6517\n",
      "Epoch 346/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5204 - mywloss: 0.5204 - val_loss: 0.6647 - val_mywloss: 0.6647\n",
      "Epoch 347/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5183 - mywloss: 0.5183 - val_loss: 0.6516 - val_mywloss: 0.6516\n",
      "Epoch 348/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5062 - mywloss: 0.5062 - val_loss: 0.6549 - val_mywloss: 0.6549\n",
      "Epoch 349/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5007 - mywloss: 0.5007 - val_loss: 0.6512 - val_mywloss: 0.6512\n",
      "Epoch 350/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5065 - mywloss: 0.5065 - val_loss: 0.6629 - val_mywloss: 0.6629\n",
      "Epoch 351/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5007 - mywloss: 0.5007 - val_loss: 0.6661 - val_mywloss: 0.6661\n",
      "Epoch 352/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5167 - mywloss: 0.5167 - val_loss: 0.6444 - val_mywloss: 0.6444\n",
      "Epoch 353/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5207 - mywloss: 0.5207 - val_loss: 0.6467 - val_mywloss: 0.6467\n",
      "Epoch 354/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5309 - mywloss: 0.5309 - val_loss: 0.6757 - val_mywloss: 0.6757\n",
      "Epoch 355/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5020 - mywloss: 0.5020 - val_loss: 0.6607 - val_mywloss: 0.6607\n",
      "Epoch 356/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5177 - mywloss: 0.5177 - val_loss: 0.6685 - val_mywloss: 0.6685\n",
      "Epoch 357/600\n",
      "6284/6284 [==============================] - 0s 72us/step - loss: 0.5041 - mywloss: 0.5041 - val_loss: 0.6678 - val_mywloss: 0.6678\n",
      "Epoch 358/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5140 - mywloss: 0.5140 - val_loss: 0.6890 - val_mywloss: 0.6890\n",
      "Epoch 359/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5171 - mywloss: 0.5171 - val_loss: 0.6657 - val_mywloss: 0.6657\n",
      "Epoch 360/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5118 - mywloss: 0.5118 - val_loss: 0.6678 - val_mywloss: 0.6678\n",
      "Epoch 361/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5225 - mywloss: 0.5225 - val_loss: 0.6582 - val_mywloss: 0.6582\n",
      "Epoch 362/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5116 - mywloss: 0.5116 - val_loss: 0.6810 - val_mywloss: 0.6810\n",
      "Epoch 363/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4931 - mywloss: 0.4931 - val_loss: 0.6566 - val_mywloss: 0.6566\n",
      "Epoch 364/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4998 - mywloss: 0.4998 - val_loss: 0.6785 - val_mywloss: 0.6785\n",
      "Epoch 365/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5136 - mywloss: 0.5136 - val_loss: 0.6895 - val_mywloss: 0.6895\n",
      "Epoch 366/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5176 - mywloss: 0.5176 - val_loss: 0.6851 - val_mywloss: 0.6851\n",
      "Epoch 367/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4950 - mywloss: 0.4950 - val_loss: 0.6712 - val_mywloss: 0.6712\n",
      "Epoch 368/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4867 - mywloss: 0.4867 - val_loss: 0.6902 - val_mywloss: 0.6902\n",
      "Epoch 369/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5182 - mywloss: 0.5182 - val_loss: 0.6910 - val_mywloss: 0.6910\n",
      "Epoch 370/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4965 - mywloss: 0.4965 - val_loss: 0.6757 - val_mywloss: 0.6757\n",
      "Epoch 371/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5139 - mywloss: 0.5139 - val_loss: 0.6800 - val_mywloss: 0.6800\n",
      "Epoch 372/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4873 - mywloss: 0.4873 - val_loss: 0.6806 - val_mywloss: 0.6806\n",
      "Epoch 373/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4891 - mywloss: 0.4891 - val_loss: 0.6987 - val_mywloss: 0.6987\n",
      "Epoch 374/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5184 - mywloss: 0.5184 - val_loss: 0.6861 - val_mywloss: 0.6861\n",
      "Epoch 375/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5028 - mywloss: 0.5028 - val_loss: 0.6818 - val_mywloss: 0.6818\n",
      "Epoch 376/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5101 - mywloss: 0.5101 - val_loss: 0.6862 - val_mywloss: 0.6862\n",
      "Epoch 377/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5089 - mywloss: 0.5089 - val_loss: 0.6854 - val_mywloss: 0.6854\n",
      "Epoch 378/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5036 - mywloss: 0.5036 - val_loss: 0.6682 - val_mywloss: 0.6682\n",
      "Epoch 379/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5012 - mywloss: 0.5012 - val_loss: 0.6590 - val_mywloss: 0.6590\n",
      "Epoch 380/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5069 - mywloss: 0.5069 - val_loss: 0.6806 - val_mywloss: 0.6806\n",
      "Epoch 381/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5074 - mywloss: 0.5074 - val_loss: 0.6696 - val_mywloss: 0.6696\n",
      "Epoch 382/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5195 - mywloss: 0.5195 - val_loss: 0.6781 - val_mywloss: 0.6781\n",
      "Epoch 383/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5193 - mywloss: 0.5193 - val_loss: 0.6808 - val_mywloss: 0.6808\n",
      "Epoch 384/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4863 - mywloss: 0.4863 - val_loss: 0.6738 - val_mywloss: 0.6738\n",
      "Epoch 385/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4997 - mywloss: 0.4997 - val_loss: 0.6678 - val_mywloss: 0.6678\n",
      "Epoch 386/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5061 - mywloss: 0.5061 - val_loss: 0.6780 - val_mywloss: 0.6780\n",
      "Epoch 387/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5205 - mywloss: 0.5205 - val_loss: 0.6976 - val_mywloss: 0.6976\n",
      "Epoch 388/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4902 - mywloss: 0.4902 - val_loss: 0.6955 - val_mywloss: 0.6955\n",
      "Epoch 389/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5057 - mywloss: 0.5057 - val_loss: 0.6845 - val_mywloss: 0.6845\n",
      "Epoch 390/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5194 - mywloss: 0.5194 - val_loss: 0.6817 - val_mywloss: 0.6817\n",
      "Epoch 391/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4876 - mywloss: 0.4876 - val_loss: 0.6825 - val_mywloss: 0.6825\n",
      "Epoch 392/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5025 - mywloss: 0.5025 - val_loss: 0.6795 - val_mywloss: 0.6795\n",
      "Epoch 393/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4941 - mywloss: 0.4941 - val_loss: 0.6760 - val_mywloss: 0.6760\n",
      "Epoch 394/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5153 - mywloss: 0.5153 - val_loss: 0.7026 - val_mywloss: 0.7026\n",
      "Epoch 395/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5020 - mywloss: 0.5020 - val_loss: 0.6723 - val_mywloss: 0.6723\n",
      "Epoch 396/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4933 - mywloss: 0.4933 - val_loss: 0.6846 - val_mywloss: 0.6846\n",
      "Epoch 397/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5015 - mywloss: 0.5015 - val_loss: 0.6597 - val_mywloss: 0.6597\n",
      "Epoch 398/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4961 - mywloss: 0.4961 - val_loss: 0.6681 - val_mywloss: 0.6681\n",
      "Epoch 399/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5101 - mywloss: 0.5101 - val_loss: 0.6836 - val_mywloss: 0.6836\n",
      "Epoch 400/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.5119 - mywloss: 0.5119 - val_loss: 0.6633 - val_mywloss: 0.6633\n",
      "Epoch 401/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4878 - mywloss: 0.4878 - val_loss: 0.6785 - val_mywloss: 0.6785\n",
      "Epoch 402/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4977 - mywloss: 0.4977 - val_loss: 0.6831 - val_mywloss: 0.6831\n",
      "Epoch 403/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4802 - mywloss: 0.4802 - val_loss: 0.6826 - val_mywloss: 0.6826\n",
      "Epoch 404/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4904 - mywloss: 0.4904 - val_loss: 0.6955 - val_mywloss: 0.6955\n",
      "Epoch 405/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4884 - mywloss: 0.4884 - val_loss: 0.6990 - val_mywloss: 0.6990\n",
      "Epoch 406/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4851 - mywloss: 0.4851 - val_loss: 0.6872 - val_mywloss: 0.6872\n",
      "Epoch 407/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.5074 - mywloss: 0.5074 - val_loss: 0.6930 - val_mywloss: 0.6930\n",
      "Epoch 408/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4965 - mywloss: 0.4965 - val_loss: 0.6687 - val_mywloss: 0.6687\n",
      "Epoch 409/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4839 - mywloss: 0.4839 - val_loss: 0.6765 - val_mywloss: 0.6765\n",
      "Epoch 410/600\n",
      "6284/6284 [==============================] - 0s 72us/step - loss: 0.5009 - mywloss: 0.5009 - val_loss: 0.6633 - val_mywloss: 0.6633\n",
      "Epoch 411/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4791 - mywloss: 0.4791 - val_loss: 0.6665 - val_mywloss: 0.6665\n",
      "Epoch 412/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.5121 - mywloss: 0.5121 - val_loss: 0.6820 - val_mywloss: 0.6820\n",
      "Epoch 413/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4916 - mywloss: 0.4916 - val_loss: 0.6726 - val_mywloss: 0.6726\n",
      "Epoch 414/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4698 - mywloss: 0.4698 - val_loss: 0.6705 - val_mywloss: 0.6705\n",
      "Epoch 415/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4822 - mywloss: 0.4822 - val_loss: 0.7030 - val_mywloss: 0.7030\n",
      "Epoch 416/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4869 - mywloss: 0.4869 - val_loss: 0.6790 - val_mywloss: 0.6790\n",
      "Epoch 417/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4946 - mywloss: 0.4946 - val_loss: 0.6881 - val_mywloss: 0.6881\n",
      "Epoch 418/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4862 - mywloss: 0.4862 - val_loss: 0.7061 - val_mywloss: 0.7061\n",
      "Epoch 419/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5148 - mywloss: 0.5148 - val_loss: 0.6940 - val_mywloss: 0.6940\n",
      "Epoch 420/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4786 - mywloss: 0.4786 - val_loss: 0.6945 - val_mywloss: 0.6945\n",
      "Epoch 421/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4812 - mywloss: 0.4812 - val_loss: 0.6802 - val_mywloss: 0.6802\n",
      "Epoch 422/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5068 - mywloss: 0.5068 - val_loss: 0.6694 - val_mywloss: 0.6694\n",
      "Epoch 423/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5168 - mywloss: 0.5168 - val_loss: 0.6502 - val_mywloss: 0.6502\n",
      "Epoch 424/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.5055 - mywloss: 0.5055 - val_loss: 0.6602 - val_mywloss: 0.6602\n",
      "Epoch 425/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5016 - mywloss: 0.5016 - val_loss: 0.6785 - val_mywloss: 0.6785\n",
      "Epoch 426/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4961 - mywloss: 0.4961 - val_loss: 0.6672 - val_mywloss: 0.6672\n",
      "Epoch 427/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4907 - mywloss: 0.4907 - val_loss: 0.6529 - val_mywloss: 0.6529\n",
      "Epoch 428/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4690 - mywloss: 0.4690 - val_loss: 0.6541 - val_mywloss: 0.6541\n",
      "Epoch 429/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4858 - mywloss: 0.4858 - val_loss: 0.6592 - val_mywloss: 0.6592\n",
      "Epoch 430/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4932 - mywloss: 0.4932 - val_loss: 0.6617 - val_mywloss: 0.6617\n",
      "Epoch 431/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4829 - mywloss: 0.4829 - val_loss: 0.6862 - val_mywloss: 0.6862\n",
      "Epoch 432/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4855 - mywloss: 0.4855 - val_loss: 0.6669 - val_mywloss: 0.6669\n",
      "Epoch 433/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4738 - mywloss: 0.4738 - val_loss: 0.6811 - val_mywloss: 0.6811\n",
      "Epoch 434/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4774 - mywloss: 0.4774 - val_loss: 0.6883 - val_mywloss: 0.6883\n",
      "Epoch 435/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4901 - mywloss: 0.4901 - val_loss: 0.6875 - val_mywloss: 0.6875\n",
      "Epoch 436/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4918 - mywloss: 0.4918 - val_loss: 0.6999 - val_mywloss: 0.6999\n",
      "Epoch 437/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.5012 - mywloss: 0.5012 - val_loss: 0.6662 - val_mywloss: 0.6662\n",
      "Epoch 438/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4773 - mywloss: 0.4773 - val_loss: 0.6710 - val_mywloss: 0.6710\n",
      "Epoch 439/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4756 - mywloss: 0.4756 - val_loss: 0.6726 - val_mywloss: 0.6726\n",
      "Epoch 440/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4992 - mywloss: 0.4992 - val_loss: 0.6993 - val_mywloss: 0.6993\n",
      "Epoch 441/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4884 - mywloss: 0.4884 - val_loss: 0.7000 - val_mywloss: 0.7000\n",
      "Epoch 442/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4839 - mywloss: 0.4839 - val_loss: 0.6892 - val_mywloss: 0.6892\n",
      "Epoch 443/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4918 - mywloss: 0.4918 - val_loss: 0.6793 - val_mywloss: 0.6793\n",
      "Epoch 444/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4834 - mywloss: 0.4834 - val_loss: 0.6791 - val_mywloss: 0.6791\n",
      "Epoch 445/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4809 - mywloss: 0.4809 - val_loss: 0.6750 - val_mywloss: 0.6750\n",
      "Epoch 446/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4699 - mywloss: 0.4699 - val_loss: 0.6865 - val_mywloss: 0.6865\n",
      "Epoch 447/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4902 - mywloss: 0.4902 - val_loss: 0.6859 - val_mywloss: 0.6859\n",
      "Epoch 448/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4746 - mywloss: 0.4746 - val_loss: 0.6795 - val_mywloss: 0.6795\n",
      "Epoch 449/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4722 - mywloss: 0.4722 - val_loss: 0.6724 - val_mywloss: 0.6724\n",
      "Epoch 450/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4934 - mywloss: 0.4934 - val_loss: 0.6853 - val_mywloss: 0.6853\n",
      "Epoch 451/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4768 - mywloss: 0.4768 - val_loss: 0.6706 - val_mywloss: 0.6706\n",
      "Epoch 452/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4640 - mywloss: 0.4640 - val_loss: 0.6833 - val_mywloss: 0.6833\n",
      "Epoch 453/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4779 - mywloss: 0.4779 - val_loss: 0.6789 - val_mywloss: 0.6789\n",
      "Epoch 454/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4621 - mywloss: 0.4621 - val_loss: 0.6663 - val_mywloss: 0.6663\n",
      "Epoch 455/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4729 - mywloss: 0.4729 - val_loss: 0.6870 - val_mywloss: 0.6870\n",
      "Epoch 456/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4656 - mywloss: 0.4656 - val_loss: 0.6869 - val_mywloss: 0.6869\n",
      "Epoch 457/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4707 - mywloss: 0.4707 - val_loss: 0.6882 - val_mywloss: 0.6882\n",
      "Epoch 458/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4643 - mywloss: 0.4643 - val_loss: 0.6866 - val_mywloss: 0.6866\n",
      "Epoch 459/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4793 - mywloss: 0.4793 - val_loss: 0.6976 - val_mywloss: 0.6976\n",
      "Epoch 460/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4698 - mywloss: 0.4698 - val_loss: 0.7068 - val_mywloss: 0.7068\n",
      "Epoch 461/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4842 - mywloss: 0.4842 - val_loss: 0.7149 - val_mywloss: 0.7149\n",
      "Epoch 462/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4691 - mywloss: 0.4691 - val_loss: 0.7052 - val_mywloss: 0.7052\n",
      "Epoch 463/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4762 - mywloss: 0.4762 - val_loss: 0.7024 - val_mywloss: 0.7024\n",
      "Epoch 464/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4737 - mywloss: 0.4737 - val_loss: 0.7004 - val_mywloss: 0.7004\n",
      "Epoch 465/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4759 - mywloss: 0.4759 - val_loss: 0.7009 - val_mywloss: 0.7009\n",
      "Epoch 466/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4621 - mywloss: 0.4621 - val_loss: 0.6904 - val_mywloss: 0.6904\n",
      "Epoch 467/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4616 - mywloss: 0.4616 - val_loss: 0.7068 - val_mywloss: 0.7068\n",
      "Epoch 468/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4683 - mywloss: 0.4683 - val_loss: 0.6913 - val_mywloss: 0.6913\n",
      "Epoch 469/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4613 - mywloss: 0.4613 - val_loss: 0.6909 - val_mywloss: 0.6909\n",
      "Epoch 470/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4736 - mywloss: 0.4736 - val_loss: 0.6959 - val_mywloss: 0.6959\n",
      "Epoch 471/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4847 - mywloss: 0.4847 - val_loss: 0.6973 - val_mywloss: 0.6973\n",
      "Epoch 472/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4789 - mywloss: 0.4789 - val_loss: 0.6909 - val_mywloss: 0.6909\n",
      "Epoch 473/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4916 - mywloss: 0.4916 - val_loss: 0.6880 - val_mywloss: 0.6880\n",
      "Epoch 474/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4864 - mywloss: 0.4864 - val_loss: 0.6890 - val_mywloss: 0.6890\n",
      "Epoch 475/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4666 - mywloss: 0.4666 - val_loss: 0.6974 - val_mywloss: 0.6974\n",
      "Epoch 476/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4506 - mywloss: 0.4506 - val_loss: 0.6865 - val_mywloss: 0.6865\n",
      "Epoch 477/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4840 - mywloss: 0.4840 - val_loss: 0.6980 - val_mywloss: 0.6980\n",
      "Epoch 478/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4570 - mywloss: 0.4570 - val_loss: 0.6959 - val_mywloss: 0.6959\n",
      "Epoch 479/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4757 - mywloss: 0.4757 - val_loss: 0.6960 - val_mywloss: 0.6960\n",
      "Epoch 480/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4703 - mywloss: 0.4703 - val_loss: 0.6846 - val_mywloss: 0.6846\n",
      "Epoch 481/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4696 - mywloss: 0.4696 - val_loss: 0.6948 - val_mywloss: 0.6948\n",
      "Epoch 482/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4650 - mywloss: 0.4650 - val_loss: 0.6887 - val_mywloss: 0.6887\n",
      "Epoch 483/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4720 - mywloss: 0.4720 - val_loss: 0.6961 - val_mywloss: 0.6961\n",
      "Epoch 484/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4673 - mywloss: 0.4673 - val_loss: 0.7032 - val_mywloss: 0.7032\n",
      "Epoch 485/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4724 - mywloss: 0.4724 - val_loss: 0.6972 - val_mywloss: 0.6972\n",
      "Epoch 486/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4675 - mywloss: 0.4675 - val_loss: 0.6967 - val_mywloss: 0.6967\n",
      "Epoch 487/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4619 - mywloss: 0.4619 - val_loss: 0.6901 - val_mywloss: 0.6901\n",
      "Epoch 488/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4773 - mywloss: 0.4773 - val_loss: 0.6994 - val_mywloss: 0.6994\n",
      "Epoch 489/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4705 - mywloss: 0.4705 - val_loss: 0.6975 - val_mywloss: 0.6975\n",
      "Epoch 490/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4344 - mywloss: 0.4344 - val_loss: 0.6869 - val_mywloss: 0.6869\n",
      "Epoch 491/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4634 - mywloss: 0.4634 - val_loss: 0.7004 - val_mywloss: 0.7004\n",
      "Epoch 492/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4647 - mywloss: 0.4647 - val_loss: 0.6968 - val_mywloss: 0.6968\n",
      "Epoch 493/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4412 - mywloss: 0.4412 - val_loss: 0.6870 - val_mywloss: 0.6870\n",
      "Epoch 494/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4492 - mywloss: 0.4492 - val_loss: 0.6855 - val_mywloss: 0.6855\n",
      "Epoch 495/600\n",
      "6284/6284 [==============================] - 0s 67us/step - loss: 0.4583 - mywloss: 0.4583 - val_loss: 0.6810 - val_mywloss: 0.6810\n",
      "Epoch 496/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4632 - mywloss: 0.4632 - val_loss: 0.7092 - val_mywloss: 0.7092\n",
      "Epoch 497/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4649 - mywloss: 0.4649 - val_loss: 0.6819 - val_mywloss: 0.6819\n",
      "Epoch 498/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4718 - mywloss: 0.4718 - val_loss: 0.6847 - val_mywloss: 0.6847\n",
      "Epoch 499/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4599 - mywloss: 0.4599 - val_loss: 0.6749 - val_mywloss: 0.6749\n",
      "Epoch 500/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4693 - mywloss: 0.4693 - val_loss: 0.6942 - val_mywloss: 0.6942\n",
      "Epoch 501/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4550 - mywloss: 0.4550 - val_loss: 0.7064 - val_mywloss: 0.7064\n",
      "Epoch 502/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4639 - mywloss: 0.4639 - val_loss: 0.7086 - val_mywloss: 0.7086\n",
      "Epoch 503/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4715 - mywloss: 0.4715 - val_loss: 0.7073 - val_mywloss: 0.7073\n",
      "Epoch 504/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4887 - mywloss: 0.4887 - val_loss: 0.7116 - val_mywloss: 0.7116\n",
      "Epoch 505/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4631 - mywloss: 0.4631 - val_loss: 0.6965 - val_mywloss: 0.6965\n",
      "Epoch 506/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4706 - mywloss: 0.4706 - val_loss: 0.7021 - val_mywloss: 0.7021\n",
      "Epoch 507/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4661 - mywloss: 0.4661 - val_loss: 0.6865 - val_mywloss: 0.6865\n",
      "Epoch 508/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4624 - mywloss: 0.4624 - val_loss: 0.6835 - val_mywloss: 0.6835\n",
      "Epoch 509/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4451 - mywloss: 0.4451 - val_loss: 0.6906 - val_mywloss: 0.6906\n",
      "Epoch 510/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4493 - mywloss: 0.4493 - val_loss: 0.6904 - val_mywloss: 0.6904\n",
      "Epoch 511/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4612 - mywloss: 0.4612 - val_loss: 0.6949 - val_mywloss: 0.6949\n",
      "Epoch 512/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4640 - mywloss: 0.4640 - val_loss: 0.7028 - val_mywloss: 0.7028\n",
      "Epoch 513/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4646 - mywloss: 0.4646 - val_loss: 0.6955 - val_mywloss: 0.6955\n",
      "Epoch 514/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4622 - mywloss: 0.4622 - val_loss: 0.6914 - val_mywloss: 0.6914\n",
      "Epoch 515/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4666 - mywloss: 0.4666 - val_loss: 0.7005 - val_mywloss: 0.7005\n",
      "Epoch 516/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4541 - mywloss: 0.4541 - val_loss: 0.6996 - val_mywloss: 0.6996\n",
      "Epoch 517/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4492 - mywloss: 0.4492 - val_loss: 0.6942 - val_mywloss: 0.6942\n",
      "Epoch 518/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4558 - mywloss: 0.4558 - val_loss: 0.6916 - val_mywloss: 0.6916\n",
      "Epoch 519/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4710 - mywloss: 0.4710 - val_loss: 0.6918 - val_mywloss: 0.6918\n",
      "Epoch 520/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4626 - mywloss: 0.4626 - val_loss: 0.7061 - val_mywloss: 0.7061\n",
      "Epoch 521/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4569 - mywloss: 0.4569 - val_loss: 0.6964 - val_mywloss: 0.6964\n",
      "Epoch 522/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4586 - mywloss: 0.4586 - val_loss: 0.6985 - val_mywloss: 0.6985\n",
      "Epoch 523/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4437 - mywloss: 0.4437 - val_loss: 0.6896 - val_mywloss: 0.6896\n",
      "Epoch 524/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4629 - mywloss: 0.4629 - val_loss: 0.6968 - val_mywloss: 0.6968\n",
      "Epoch 525/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4611 - mywloss: 0.4611 - val_loss: 0.7023 - val_mywloss: 0.7023\n",
      "Epoch 526/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4527 - mywloss: 0.4527 - val_loss: 0.7092 - val_mywloss: 0.7092\n",
      "Epoch 527/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4361 - mywloss: 0.4361 - val_loss: 0.7068 - val_mywloss: 0.7068\n",
      "Epoch 528/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4559 - mywloss: 0.4559 - val_loss: 0.7038 - val_mywloss: 0.7038\n",
      "Epoch 529/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4384 - mywloss: 0.4384 - val_loss: 0.6972 - val_mywloss: 0.6972\n",
      "Epoch 530/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4511 - mywloss: 0.4511 - val_loss: 0.7242 - val_mywloss: 0.7242\n",
      "Epoch 531/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4502 - mywloss: 0.4502 - val_loss: 0.7112 - val_mywloss: 0.7112\n",
      "Epoch 532/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4447 - mywloss: 0.4447 - val_loss: 0.7249 - val_mywloss: 0.7249\n",
      "Epoch 533/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4310 - mywloss: 0.4310 - val_loss: 0.7028 - val_mywloss: 0.7028\n",
      "Epoch 534/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4425 - mywloss: 0.4425 - val_loss: 0.7128 - val_mywloss: 0.7128\n",
      "Epoch 535/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4487 - mywloss: 0.4487 - val_loss: 0.7093 - val_mywloss: 0.7093\n",
      "Epoch 536/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4688 - mywloss: 0.4688 - val_loss: 0.7164 - val_mywloss: 0.7164\n",
      "Epoch 537/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4591 - mywloss: 0.4591 - val_loss: 0.7029 - val_mywloss: 0.7029\n",
      "Epoch 538/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4412 - mywloss: 0.4412 - val_loss: 0.6938 - val_mywloss: 0.6938\n",
      "Epoch 539/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4339 - mywloss: 0.4339 - val_loss: 0.6774 - val_mywloss: 0.6774\n",
      "Epoch 540/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4527 - mywloss: 0.4527 - val_loss: 0.6918 - val_mywloss: 0.6918\n",
      "Epoch 541/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4550 - mywloss: 0.4550 - val_loss: 0.7156 - val_mywloss: 0.7156\n",
      "Epoch 542/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4504 - mywloss: 0.4504 - val_loss: 0.7035 - val_mywloss: 0.7035\n",
      "Epoch 543/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4596 - mywloss: 0.4596 - val_loss: 0.7191 - val_mywloss: 0.7191\n",
      "Epoch 544/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4567 - mywloss: 0.4567 - val_loss: 0.7302 - val_mywloss: 0.7302\n",
      "Epoch 545/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4461 - mywloss: 0.4461 - val_loss: 0.7113 - val_mywloss: 0.7113\n",
      "Epoch 546/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4635 - mywloss: 0.4635 - val_loss: 0.7246 - val_mywloss: 0.7246\n",
      "Epoch 547/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4453 - mywloss: 0.4453 - val_loss: 0.7315 - val_mywloss: 0.7315\n",
      "Epoch 548/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4365 - mywloss: 0.4365 - val_loss: 0.7214 - val_mywloss: 0.7214\n",
      "Epoch 549/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4392 - mywloss: 0.4392 - val_loss: 0.7329 - val_mywloss: 0.7329\n",
      "Epoch 550/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4446 - mywloss: 0.4446 - val_loss: 0.7292 - val_mywloss: 0.7292\n",
      "Epoch 551/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4412 - mywloss: 0.4412 - val_loss: 0.7294 - val_mywloss: 0.7294\n",
      "Epoch 552/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4423 - mywloss: 0.4423 - val_loss: 0.7442 - val_mywloss: 0.7442\n",
      "Epoch 553/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4468 - mywloss: 0.4468 - val_loss: 0.7505 - val_mywloss: 0.7505\n",
      "Epoch 554/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4289 - mywloss: 0.4289 - val_loss: 0.7262 - val_mywloss: 0.7262\n",
      "Epoch 555/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4488 - mywloss: 0.4488 - val_loss: 0.7289 - val_mywloss: 0.7289\n",
      "Epoch 556/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4528 - mywloss: 0.4528 - val_loss: 0.7111 - val_mywloss: 0.7111\n",
      "Epoch 557/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4370 - mywloss: 0.4370 - val_loss: 0.6978 - val_mywloss: 0.6978\n",
      "Epoch 558/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4482 - mywloss: 0.4482 - val_loss: 0.7026 - val_mywloss: 0.7026\n",
      "Epoch 559/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4280 - mywloss: 0.4280 - val_loss: 0.7048 - val_mywloss: 0.7048\n",
      "Epoch 560/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4486 - mywloss: 0.4486 - val_loss: 0.7061 - val_mywloss: 0.7061\n",
      "Epoch 561/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4544 - mywloss: 0.4544 - val_loss: 0.7125 - val_mywloss: 0.7125\n",
      "Epoch 562/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4434 - mywloss: 0.4434 - val_loss: 0.7502 - val_mywloss: 0.7502\n",
      "Epoch 563/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4551 - mywloss: 0.4551 - val_loss: 0.7363 - val_mywloss: 0.7363\n",
      "Epoch 564/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4554 - mywloss: 0.4554 - val_loss: 0.7362 - val_mywloss: 0.7362\n",
      "Epoch 565/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4448 - mywloss: 0.4448 - val_loss: 0.7149 - val_mywloss: 0.7149\n",
      "Epoch 566/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4556 - mywloss: 0.4556 - val_loss: 0.7118 - val_mywloss: 0.7118\n",
      "Epoch 567/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4490 - mywloss: 0.4490 - val_loss: 0.7180 - val_mywloss: 0.7180\n",
      "Epoch 568/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4512 - mywloss: 0.4512 - val_loss: 0.7074 - val_mywloss: 0.7074\n",
      "Epoch 569/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4436 - mywloss: 0.4436 - val_loss: 0.7022 - val_mywloss: 0.7022\n",
      "Epoch 570/600\n",
      "6284/6284 [==============================] - 0s 70us/step - loss: 0.4594 - mywloss: 0.4594 - val_loss: 0.7037 - val_mywloss: 0.7037\n",
      "Epoch 571/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4463 - mywloss: 0.4463 - val_loss: 0.7026 - val_mywloss: 0.7026\n",
      "Epoch 572/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4648 - mywloss: 0.4648 - val_loss: 0.6867 - val_mywloss: 0.6867\n",
      "Epoch 573/600\n",
      "6284/6284 [==============================] - 0s 71us/step - loss: 0.4462 - mywloss: 0.4462 - val_loss: 0.7167 - val_mywloss: 0.7167\n",
      "Epoch 574/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4385 - mywloss: 0.4385 - val_loss: 0.7013 - val_mywloss: 0.7013\n",
      "Epoch 575/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4443 - mywloss: 0.4443 - val_loss: 0.6971 - val_mywloss: 0.6971\n",
      "Epoch 576/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4334 - mywloss: 0.4334 - val_loss: 0.6998 - val_mywloss: 0.6998\n",
      "Epoch 577/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4460 - mywloss: 0.4460 - val_loss: 0.7128 - val_mywloss: 0.7128\n",
      "Epoch 578/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4479 - mywloss: 0.4479 - val_loss: 0.7045 - val_mywloss: 0.7045\n",
      "Epoch 579/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4343 - mywloss: 0.4343 - val_loss: 0.6910 - val_mywloss: 0.6910\n",
      "Epoch 580/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4344 - mywloss: 0.4344 - val_loss: 0.7132 - val_mywloss: 0.7132\n",
      "Epoch 581/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4427 - mywloss: 0.4427 - val_loss: 0.7210 - val_mywloss: 0.7210\n",
      "Epoch 582/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4410 - mywloss: 0.4410 - val_loss: 0.7158 - val_mywloss: 0.7158\n",
      "Epoch 583/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4395 - mywloss: 0.4395 - val_loss: 0.7157 - val_mywloss: 0.7157\n",
      "Epoch 584/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4261 - mywloss: 0.4261 - val_loss: 0.6986 - val_mywloss: 0.6986\n",
      "Epoch 585/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4475 - mywloss: 0.4475 - val_loss: 0.7129 - val_mywloss: 0.7129\n",
      "Epoch 586/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4516 - mywloss: 0.4516 - val_loss: 0.7081 - val_mywloss: 0.7081\n",
      "Epoch 587/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4421 - mywloss: 0.4421 - val_loss: 0.7174 - val_mywloss: 0.7174\n",
      "Epoch 588/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4435 - mywloss: 0.4435 - val_loss: 0.6952 - val_mywloss: 0.6952\n",
      "Epoch 589/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4290 - mywloss: 0.4290 - val_loss: 0.7107 - val_mywloss: 0.7107\n",
      "Epoch 590/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4191 - mywloss: 0.4191 - val_loss: 0.7087 - val_mywloss: 0.7087\n",
      "Epoch 591/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4364 - mywloss: 0.4364 - val_loss: 0.7135 - val_mywloss: 0.7135\n",
      "Epoch 592/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4241 - mywloss: 0.4241 - val_loss: 0.6928 - val_mywloss: 0.6928\n",
      "Epoch 593/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4081 - mywloss: 0.4081 - val_loss: 0.7002 - val_mywloss: 0.7002\n",
      "Epoch 594/600\n",
      "6284/6284 [==============================] - 0s 72us/step - loss: 0.4319 - mywloss: 0.4319 - val_loss: 0.6922 - val_mywloss: 0.6922\n",
      "Epoch 595/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4238 - mywloss: 0.4238 - val_loss: 0.6981 - val_mywloss: 0.6981\n",
      "Epoch 596/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4332 - mywloss: 0.4332 - val_loss: 0.6903 - val_mywloss: 0.6903\n",
      "Epoch 597/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4118 - mywloss: 0.4118 - val_loss: 0.7163 - val_mywloss: 0.7163\n",
      "Epoch 598/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4353 - mywloss: 0.4353 - val_loss: 0.7459 - val_mywloss: 0.7459\n",
      "Epoch 599/600\n",
      "6284/6284 [==============================] - 0s 69us/step - loss: 0.4288 - mywloss: 0.4288 - val_loss: 0.7176 - val_mywloss: 0.7176\n",
      "Epoch 600/600\n",
      "6284/6284 [==============================] - 0s 68us/step - loss: 0.4364 - mywloss: 0.4364 - val_loss: 0.7163 - val_mywloss: 0.7163\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb43ac1d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model\n",
      "0.6272995203545279\n",
      "MULTI WEIGHTED LOG LOSS : 0.69846 \n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "clfs = []\n",
    "oof_preds = np.zeros((len(full_train_ss), len(classes)))\n",
    "epochs = 600\n",
    "batch_size = 100\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "    \n",
    "    model = build_model(dropout_rate=0.5,activation='tanh')    \n",
    "    model.compile(loss=mywloss, optimizer='adam', metrics=[mywloss])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,shuffle=True,verbose=1,callbacks=[checkPoint])       \n",
    "    \n",
    "    plot_loss_acc(history)\n",
    "    \n",
    "    print('Loading Best Model')\n",
    "    model.load_weights('./keras.model')\n",
    "    # # Get predicted probaScalerbilities for each class\n",
    "    oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "    clfs.append(model)\n",
    "    \n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def save_cm(y, oof_preds, path):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i, val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "\n",
    "    y_map = np.zeros((y.shape[0], ))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds, axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    sample_sub = pd.read_csv(path)\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title='Confusion matrix')\n",
    "    plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    }
   ],
   "source": [
    "save_cm(y, oof_preds, data_dir + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
