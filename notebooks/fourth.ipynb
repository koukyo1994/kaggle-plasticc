{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hidehisa/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from multiprocessing import Pool\n",
    "tqdm.pandas(desc=\"apply progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hidehisa/.kaggle/competitions/plasticc\"\n",
    "train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "    agg_flux_diff = agg_df.reset_index()[[\"object_id\", \"flux_diff\"]]\n",
    "    df2 = pd.merge(df, agg_df, how=\"left\", on=\"object_id\")\n",
    "    df2[\"flux_norm\"] = df2.flux / df2.flux_diff\n",
    "    del df2[\"flux\"]\n",
    "    fcp = {\n",
    "        'fft_coefficient': [{\n",
    "            'coeff': 0,\n",
    "            'attr': 'abs'\n",
    "        }, {\n",
    "            'coeff': 1,\n",
    "            'attr': 'abs'\n",
    "        }],\n",
    "        'kurtosis':\n",
    "        None,\n",
    "        'skewness':\n",
    "        None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp2 = {\n",
    "        \"fft_coefficient\": [{\n",
    "            \"coeff\": 0,\n",
    "            \"attr\": \"abs\"\n",
    "        }, {\n",
    "            \"coeff\": 1,\n",
    "            \"attr\": \"abs\"\n",
    "        }],\n",
    "        \"abs_energy\": None,\n",
    "        \"sample_entropy\": None\n",
    "    }\n",
    "    fcp_flux = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp_flux_by_flux_ratio_sq = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None\n",
    "    }\n",
    "    agg_df_ts = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp,\n",
    "        n_jobs=6)\n",
    "    agg_df_ts2 = extract_features(\n",
    "        df2,\n",
    "        column_id=\"object_id\",\n",
    "        column_sort=\"mjd\",\n",
    "        column_kind=\"passband\",\n",
    "        column_value=\"flux_norm\",\n",
    "        default_fc_parameters=fcp2,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_flux = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux\",\n",
    "        default_fc_parameters=fcp_flux,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_ffrs = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux_by_flux_ratio_sq\",\n",
    "        default_fc_parameters=fcp_flux_by_flux_ratio_sq,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters={\n",
    "            'maximum': None,\n",
    "            'minimum': None\n",
    "        },\n",
    "        n_jobs=8)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd[\n",
    "        'mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts2.columns = pd.Index([e + \"_norm\" for e in agg_df_ts2.columns])\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd, on='id')\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ts2, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_flux, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ffrs, on=\"id\")\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    agg_df_ts.index.rename('object_id', inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts, on='object_id')\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def cluster_mean_diff(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\"]).agg({\n",
    "        \"flux\": [\"mean\", \"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in new_df.columns.tolist()])\n",
    "    new_df[\"normalized_mean\"] = new_df[\"flux_mean\"] / (\n",
    "        new_df[\"flux_max\"] - new_df[\"flux_min\"])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df.groupby(\"object_id\").agg({\"normalized_mean\": \"std\"})\n",
    "\n",
    "\n",
    "def passband_std_difference(df):\n",
    "    std_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": \"std\"\n",
    "    }).reset_index().groupby([\"object_id\",\n",
    "                              \"passband\"])[\"flux\"].mean().reset_index()\n",
    "    std_df_max = std_df.groupby(\"object_id\")[\"flux\"].max()\n",
    "    std_df_min = std_df.groupby(\"object_id\")[\"flux\"].min()\n",
    "    return (std_df_max / std_df_min).reset_index()\n",
    "\n",
    "\n",
    "def num_outliers(df):\n",
    "    new_df = df.groupby(\"object_id\").agg({\"flux\": [\"mean\", \"std\"]})\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df[\"upper_sigma\"] = new_df[\"flux_mean\"] + new_df[\"flux_std\"]\n",
    "    new_df[\"upper_2sigma\"] = new_df[\"flux_mean\"] + 2 * new_df[\"flux_std\"]\n",
    "    new_df[\"lower_sigma\"] = new_df[\"flux_mean\"] - new_df[\"flux_std\"]\n",
    "    new_df[\"lower_2sigma\"] = new_df[\"flux_mean\"] - 2 * new_df[\"flux_std\"]\n",
    "    new_df.drop([\"flux_mean\", \"flux_std\"], axis=1, inplace=True)\n",
    "    new_df = pd.merge(df, new_df, how=\"left\", on=\"object_id\")\n",
    "    new_df[\"outside_sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_sigma\"])).astype(int)\n",
    "    new_df[\"outside_2sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_2sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_2sigma\"])).astype(int)\n",
    "\n",
    "    return_df = new_df.groupby(\"object_id\").agg({\n",
    "        \"outside_sigma\": \"sum\",\n",
    "        \"outside_2sigma\": \"sum\"\n",
    "    })\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full(df, meta):\n",
    "    agg_basic = basic(df)\n",
    "    cl_mean_diff = cluster_mean_diff(df)\n",
    "    ps_std_diff = passband_std_difference(df)\n",
    "    num_out = num_outliers(df)\n",
    "\n",
    "    full = pd.merge(agg_basic, cl_mean_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, ps_std_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, num_out, how=\"left\", on=\"object_id\")\n",
    "\n",
    "    full = pd.merge(full, meta, how=\"left\", on=\"object_id\")\n",
    "    if \"target\" in full.columns:\n",
    "        full.drop(\"target\", axis=1, inplace=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_data(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = meta.target\n",
    "    classes = sorted(y.unique())\n",
    "    class_weight = {c: 1 for c in classes}\n",
    "\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "    oof_df = full[[\"object_id\"]]\n",
    "    del full['object_id'], full['distmod'], full['hostgal_specz']\n",
    "    del full['ra'], full['decl'], full['gal_l'], full['gal_b'], full['ddf']\n",
    "    return full, y, classes, class_weight, oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  6.97it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:15<00:00,  2.66it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 17.48it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 30.27it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 99.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 1.36 s, total: 22.8 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full, y, classes, class_weight, oof_df = train_data(train, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = full.mean(axis=0)\n",
    "full.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {\n",
    "        6: 1,\n",
    "        15: 2,\n",
    "        16: 1,\n",
    "        42: 1,\n",
    "        52: 1,\n",
    "        53: 1,\n",
    "        62: 1,\n",
    "        64: 2,\n",
    "        65: 1,\n",
    "        67: 1,\n",
    "        88: 1,\n",
    "        90: 1,\n",
    "        92: 1,\n",
    "        95: 1\n",
    "    }\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array(\n",
    "        [class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = -np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(\n",
    "        x='gain',\n",
    "        y='feature',\n",
    "        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances_2.png')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def save_cm(y, oof_preds, path):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i, val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "\n",
    "    y_map = np.zeros((y.shape[0], ))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds, axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    sample_sub = pd.read_csv(path)\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title='Confusion matrix')\n",
    "    plt.savefig(\"confusion_matrix_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.758303\ttraining's wloss: 0.744405\tvalid_1's multi_logloss: 1.1007\tvalid_1's wloss: 0.893931\n",
      "[200]\ttraining's multi_logloss: 0.493922\ttraining's wloss: 0.479925\tvalid_1's multi_logloss: 0.864227\tvalid_1's wloss: 0.692595\n",
      "[300]\ttraining's multi_logloss: 0.378858\ttraining's wloss: 0.365139\tvalid_1's multi_logloss: 0.774035\tvalid_1's wloss: 0.637011\n",
      "[400]\ttraining's multi_logloss: 0.310429\ttraining's wloss: 0.297079\tvalid_1's multi_logloss: 0.725361\tvalid_1's wloss: 0.617738\n",
      "[500]\ttraining's multi_logloss: 0.262275\ttraining's wloss: 0.249548\tvalid_1's multi_logloss: 0.693754\tvalid_1's wloss: 0.611711\n",
      "Early stopping, best iteration is:\n",
      "[485]\ttraining's multi_logloss: 0.268942\ttraining's wloss: 0.256063\tvalid_1's multi_logloss: 0.697398\tvalid_1's wloss: 0.611094\n",
      "0.611093606570106\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.761706\ttraining's wloss: 0.749313\tvalid_1's multi_logloss: 1.08524\tvalid_1's wloss: 0.896304\n",
      "[200]\ttraining's multi_logloss: 0.496156\ttraining's wloss: 0.483261\tvalid_1's multi_logloss: 0.849314\tvalid_1's wloss: 0.702513\n",
      "[300]\ttraining's multi_logloss: 0.380316\ttraining's wloss: 0.367221\tvalid_1's multi_logloss: 0.75909\tvalid_1's wloss: 0.665028\n",
      "[400]\ttraining's multi_logloss: 0.311212\ttraining's wloss: 0.298264\tvalid_1's multi_logloss: 0.710326\tvalid_1's wloss: 0.662921\n",
      "Early stopping, best iteration is:\n",
      "[379]\ttraining's multi_logloss: 0.324075\ttraining's wloss: 0.311028\tvalid_1's multi_logloss: 0.720066\tvalid_1's wloss: 0.66158\n",
      "0.6615803123697305\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.750669\ttraining's wloss: 0.737939\tvalid_1's multi_logloss: 1.11445\tvalid_1's wloss: 0.886025\n",
      "[200]\ttraining's multi_logloss: 0.486022\ttraining's wloss: 0.472893\tvalid_1's multi_logloss: 0.876962\tvalid_1's wloss: 0.681224\n",
      "[300]\ttraining's multi_logloss: 0.373352\ttraining's wloss: 0.360037\tvalid_1's multi_logloss: 0.786885\tvalid_1's wloss: 0.62716\n",
      "[400]\ttraining's multi_logloss: 0.305384\ttraining's wloss: 0.292369\tvalid_1's multi_logloss: 0.738436\tvalid_1's wloss: 0.612583\n",
      "[500]\ttraining's multi_logloss: 0.257504\ttraining's wloss: 0.245107\tvalid_1's multi_logloss: 0.705682\tvalid_1's wloss: 0.607669\n",
      "Early stopping, best iteration is:\n",
      "[477]\ttraining's multi_logloss: 0.267405\ttraining's wloss: 0.254858\tvalid_1's multi_logloss: 0.711721\tvalid_1's wloss: 0.605578\n",
      "0.6055778122501314\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.75662\ttraining's wloss: 0.742652\tvalid_1's multi_logloss: 1.10487\tvalid_1's wloss: 0.898171\n",
      "[200]\ttraining's multi_logloss: 0.490653\ttraining's wloss: 0.477771\tvalid_1's multi_logloss: 0.862641\tvalid_1's wloss: 0.68548\n",
      "[300]\ttraining's multi_logloss: 0.373394\ttraining's wloss: 0.360494\tvalid_1's multi_logloss: 0.768963\tvalid_1's wloss: 0.627521\n",
      "[400]\ttraining's multi_logloss: 0.304301\ttraining's wloss: 0.291841\tvalid_1's multi_logloss: 0.718305\tvalid_1's wloss: 0.608805\n",
      "[500]\ttraining's multi_logloss: 0.255906\ttraining's wloss: 0.243965\tvalid_1's multi_logloss: 0.685125\tvalid_1's wloss: 0.603673\n",
      "[600]\ttraining's multi_logloss: 0.219653\ttraining's wloss: 0.208495\tvalid_1's multi_logloss: 0.659548\tvalid_1's wloss: 0.604162\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttraining's multi_logloss: 0.227492\ttraining's wloss: 0.216119\tvalid_1's multi_logloss: 0.664584\tvalid_1's wloss: 0.6025\n",
      "0.6024995285608932\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.753851\ttraining's wloss: 0.739155\tvalid_1's multi_logloss: 1.1135\tvalid_1's wloss: 0.944283\n",
      "[200]\ttraining's multi_logloss: 0.488616\ttraining's wloss: 0.474418\tvalid_1's multi_logloss: 0.870104\tvalid_1's wloss: 0.746334\n",
      "[300]\ttraining's multi_logloss: 0.376536\ttraining's wloss: 0.362438\tvalid_1's multi_logloss: 0.773626\tvalid_1's wloss: 0.696089\n",
      "[400]\ttraining's multi_logloss: 0.309433\ttraining's wloss: 0.295873\tvalid_1's multi_logloss: 0.718643\tvalid_1's wloss: 0.676747\n",
      "[500]\ttraining's multi_logloss: 0.261886\ttraining's wloss: 0.249089\tvalid_1's multi_logloss: 0.68238\tvalid_1's wloss: 0.672024\n",
      "Early stopping, best iteration is:\n",
      "[506]\ttraining's multi_logloss: 0.25957\ttraining's wloss: 0.246825\tvalid_1's multi_logloss: 0.680551\tvalid_1's wloss: 0.671645\n",
      "0.6716450933162458\n",
      "MULTI WEIGHTED LOG LOSS : 0.63032 \n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 14,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': .9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'reg_alpha': .01,\n",
    "    'reg_lambda': .01,\n",
    "    'min_split_gain': 0.01,\n",
    "    'min_child_weight': 10,\n",
    "    'n_estimators': 1000,\n",
    "    'silent': -1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i: np.sum(w) / w[i] for i in w.index}\n",
    "oof_preds = np.zeros((len(full), np.unique(y).shape[0]))\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = full.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = full.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x,\n",
    "        trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights))\n",
    "    oof_preds[val_, :] = clf.predict_proba(\n",
    "        val_x, num_iteration=clf.best_iteration_)\n",
    "    print(multi_weighted_logloss(val_y, oof_preds[val_, :]))\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = full.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    clfs.append(clf)\n",
    "\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(\n",
    "    y_true=y, y_preds=oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    }
   ],
   "source": [
    "save_importances(importances_=importances)\n",
    "save_cm(y, oof_preds, data_dir + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(df_, clfs_, meta_, features, train_mean, i_c):\n",
    "    # Group by object id\n",
    "    agg_ = get_full(df_, meta_)\n",
    "\n",
    "    full_test = agg_.fillna(0)\n",
    "    \n",
    "    if i_c == 0:\n",
    "        full_test.to_csv('full_test4.csv', header=True, mode='a', index=False)\n",
    "    else:\n",
    "        full_test.to_csv('full_test4.csv', header=False, mode='a', index=False)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    return preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [01:56<00:00,  2.90s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [03:45<00:00,  5.24s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.01s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  2.06it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2_28321978.csv done in 12.434533886114757 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [01:57<00:00,  3.31s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:28<00:00,  7.51s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:22<00:00,  1.14it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:11<00:00,  2.06it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:09<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_28321979_49999913.csv done in 23.10989718437195 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:11<00:00,  3.55s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:50<00:00,  8.58s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:27<00:00,  1.18s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.55it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_49999914_74995602.csv done in 35.397408219178516 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:12<00:00,  3.77s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:46<00:00,  6.62s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:25<00:00,  1.06it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.65it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_74995603_99999902.csv done in 47.58093222379684 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:13<00:00,  3.59s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:43<00:00,  6.03s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.03it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.66it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_99999903_125006991.csv done in 59.70303700764974 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:08<00:00,  3.27s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:44<00:00,  6.46s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.24s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.86it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_125006992_149999831.csv done in 71.80921949545542 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:13<00:00,  3.62s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:44<00:00,  6.70s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.12s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.76it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_149999832_174999676.csv done in 83.9979857524236 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:11<00:00,  3.51s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:44<00:00,  6.09s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:25<00:00,  1.04it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.72it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_174999677_199999735.csv done in 96.13206797043482 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:14<00:00,  3.61s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:45<00:00,  6.47s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.25s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.73it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_199999736_225008807.csv done in 108.36638201475144 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:15<00:00,  3.53s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:47<00:00,  7.30s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.11s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.73it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_225008808_249999654.csv done in 120.63337833881378 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:14<00:00,  3.22s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:51<00:00,  8.39s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.03s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.85it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_249999655_275002650.csv done in 132.92960915962854 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:13<00:00,  2.67s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:45<00:00,  6.33s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:25<00:00,  1.04it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.66it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_275002651_299999649.csv done in 145.1011205037435 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:14<00:00,  3.56s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:45<00:00,  7.31s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.02s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.96it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_299999650_325002148.csv done in 157.28565887610117 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:08<00:00,  2.64s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:40<00:00,  6.05s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.08s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.74it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_325002149_349999619.csv done in 169.3471800963084 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:14<00:00,  3.47s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:43<00:00,  6.47s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.01it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.91it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_349999620_374998640.csv done in 181.49672244787217 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:11<00:00,  3.24s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:43<00:00,  5.93s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.03s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.74it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_374998641_399999534.csv done in 193.57399430672328 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:16<00:00,  3.72s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:44<00:00,  6.75s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.01s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.68it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_399999535_424994349.csv done in 205.7602188785871 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [02:09<00:00,  3.12s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [02:46<00:00,  7.56s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:26<00:00,  1.13s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  1.72it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:10<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_424994350_449999411.csv done in 217.92167537212373 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.87it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:11<00:00,  2.33it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:02<00:00, 10.08it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 15.71it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 48.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_449999412_451826374.csv done in 218.82148482402167 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.82it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:12<00:00,  2.02it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.80it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 17.65it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 56.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_451826375_end.csv done in 219.71768608093262 minutes\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "start = time.time()\n",
    "indices = [(2, 28321978), (28321979, 49999913), (49999914, 74995602), (74995603, 99999902),\n",
    "           (99999903, 125006991), (125006992, 149999831), (149999832, 174999676),\n",
    "           (174999677, 199999735), (199999736, 225008807), (225008808, 249999654),\n",
    "           (249999655, 275002650), (275002651, 299999649), (299999650, 325002148),\n",
    "           (325002149, 349999619), (349999620, 374998640), (374998641, 399999534),\n",
    "           (399999535, 424994349), (424994350, 449999411), (449999412, 451826374),\n",
    "           (451826375, \"end\")]\n",
    "test_files = [f\"test_{e[0]}_{e[1]}.csv\" for e in indices]\n",
    "meta_test = pd.read_csv(data_dir + '/test_set_metadata.csv')\n",
    "for i_c, f in enumerate(test_files):\n",
    "    test = pd.read_csv(f)\n",
    "    preds_df = predict_chunk(\n",
    "        df_=test,\n",
    "        clfs_=clfs,\n",
    "        meta_=meta_test,\n",
    "        features=full.columns,\n",
    "        train_mean=train_mean,\n",
    "        i_c=i_c\n",
    "    )\n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('predictions4.csv', header=True, mode='a', index=False)\n",
    "    else:\n",
    "        preds_df.to_csv('predictions4.csv', header=False, mode='a', index=False)\n",
    "\n",
    "    del preds_df\n",
    "    print(f'{f} done in {(time.time() - start) / 60} minutes', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions4.csv (deflated 56%)\n",
      "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /home/hidehisa/.kaggle/kaggle.json'\n",
      "  2%|▉                                    | 12.0M/481M [02:39<1:41:47, 80.5kB/s]2018-11-24 05:29:30,865 WARNING Retrying (Retry(total=9, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",))': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "  5%|█▊                                   | 24.0M/481M [05:59<1:28:38, 90.1kB/s]2018-11-24 05:32:30,023 WARNING Retrying (Retry(total=8, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",))': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      " 52%|█████████████████████▏                   | 249M/481M [39:40<31:21, 130kB/s]2018-11-24 06:06:12,405 WARNING Retrying (Retry(total=7, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",))': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      " 90%|███████████████████████████████████▏   | 434M/481M [1:05:41<05:26, 151kB/s]2018-11-24 06:47:18,552 WARNING Retrying (Retry(total=6, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(22, 'EINVAL')\",))': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:47:26,562 WARNING Retrying (Retry(total=5, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc6f2a20>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:47:42,580 WARNING Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc702710>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:48:14,606 WARNING Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc7025c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:49:18,672 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc7023c8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:51:18,755 WARNING Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc7029e8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      "2018-11-24 06:53:18,840 WARNING Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc702978>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g\n",
      " 90%|██████████████████████████████████▎   | 434M/481M [1:27:18<09:27, 86.9kB/s]\n",
      "HTTPSConnectionPool(host='www.googleapis.com', port=443): Max retries exceeded with url: /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2Up7oZAwAtkE361rnOddcBWqsppXBao06xp4f3jA9BpLyhKGz9Gmepcp6uTmnrnt_VVMYb-21_6LZN31fUwju6UtxTsF7g (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f20dc702ac8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))\n",
      "Could not submit to competition"
     ]
    }
   ],
   "source": [
    "!zip predictions4.csv.zip predictions4.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /home/hidehisa/.kaggle/kaggle.json'\n",
      "  3%|█                                    | 13.4M/481M [03:20<2:05:06, 65.3kB/s]2018-11-24 08:07:33,552 WARNING Retrying (Retry(total=9, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",))': /upload/storage/v1/b/kaggle-competitions-submissions/o?uploadType=resumable&upload_id=AEnB2UrYLwNw__BpWA3GyJPpU2TAdJqTDPt0wu3-4B29rzsItzOMNqKE6qRqpKW5LHqxtmmbxn8DviwITG8v84lKze1S7ofb3T17s-nq67FEqBLqmbpGNGA\n",
      "494MB [1:52:20, 76.9kB/s]\n",
      "Successfully submitted to PLAsTiCC Astronomical Classification"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c PLAsTiCC-2018 -f predictions4.csv.zip -m \"Fourth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
