{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from multiprocessing import Pool\n",
    "tqdm.pandas(desc=\"apply progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/hidehisa/.kaggle/competitions/plasticc\"\n",
    "train = pd.read_csv(data_dir + \"/train_with_cluster.csv\")\n",
    "meta = pd.read_csv(data_dir + \"/training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova = [15, 42, 52, 62, 67, 90]\n",
    "novaes = meta.query(\"target == @nova\")\n",
    "train[\"novae\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train.query(\"object_id in @novaes.object_id\").index\n",
    "train.loc[ind, \"novae\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "      <th>cluster</th>\n",
       "      <th>novae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected  cluster  \\\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1        2   \n",
       "1        615  59750.4306         1 -816.434326   5.553370         1        2   \n",
       "2        615  59750.4383         3 -471.385529   3.801213         1        2   \n",
       "3        615  59750.4450         4 -388.984985  11.395031         1        2   \n",
       "4        615  59752.4070         2 -681.858887   4.041204         1        2   \n",
       "\n",
       "   novae  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object_id\n",
       "615     0\n",
       "713     0\n",
       "730     1\n",
       "745     1\n",
       "1124    1\n",
       "Name: novae, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"object_id\")[\"novae\"].mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq': ['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq': ['sum', 'skew'],\n",
    "    }\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df[\n",
    "        'flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (\n",
    "        agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "\n",
    "    per_passband_aggs = {\n",
    "        \"flux\": [\"min\", \"max\", \"mean\", \"std\"],\n",
    "        \"flux_ratio_sq\": [\"sum\", \"skew\"],\n",
    "        \"flux_by_flux_ratio_sq\": [\"sum\", \"skew\"]\n",
    "    }\n",
    "    per_pass_agg_df = df.groupby([\"object_id\", \"passband\"]).agg(per_passband_aggs)\n",
    "    per_pass_agg_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in per_pass_agg_df.columns])\n",
    "    per_pass_agg_df[\"flux_diff\"] = per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]\n",
    "    per_pass_agg_df[\"flux_diff2\"] = (\n",
    "        per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_mean\"]\n",
    "    per_pass_agg_df[\"flux_w_mean\"] = per_pass_agg_df[\"flux_by_flux_ratio_sq_sum\"] / per_pass_agg_df[\n",
    "        \"flux_ratio_sq_sum\"\n",
    "    ]\n",
    "    per_pass_agg_df[\"flux_dif3\"] = (\n",
    "    per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_w_mean\"]\n",
    "    per_pass_agg_df = per_pass_agg_df.unstack()\n",
    "    per_pass_agg_df.columns = pd.Index([str(e[1]) + \"__\" + e[0] for e in per_pass_agg_df.columns])\n",
    "    \n",
    "    basic_columns = [f\"{i}__{j}\" for i in range(6) for j in [\n",
    "        \"flux_min\",\n",
    "        \"flux_max\",\n",
    "        \"flux_mean\",\n",
    "        \"flux_std\",\n",
    "        \"flux_ratio_sq_sum\",\n",
    "        \"flux_ratio_sq_skew\",\n",
    "        \"flux_w_mean\",\n",
    "        \"flux_diff2\"\n",
    "    ]]\n",
    "    per_pass_agg_df.drop(basic_columns, axis=1, inplace=True)\n",
    "    \n",
    "    agg_df = pd.merge(agg_df, per_pass_agg_df, how=\"left\", on=\"object_id\")\n",
    "    \n",
    "    agg_flux_diff = agg_df.reset_index()[[\"object_id\", \"flux_diff\"]]\n",
    "    df2 = pd.merge(df, agg_df, how=\"left\", on=\"object_id\")\n",
    "    df2[\"flux_norm\"] = df2.flux / df2.flux_diff\n",
    "    del df2[\"flux\"]\n",
    "    fcp = {\n",
    "        'fft_coefficient': [{\n",
    "            'coeff': 0,\n",
    "            'attr': 'abs'\n",
    "        }, {\n",
    "            'coeff': 1,\n",
    "            'attr': 'abs'\n",
    "        }],\n",
    "        'kurtosis':\n",
    "        None,\n",
    "        'skewness':\n",
    "        None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp2 = {\n",
    "        \"fft_coefficient\": [{\n",
    "            \"coeff\": 0,\n",
    "            \"attr\": \"abs\"\n",
    "        }, {\n",
    "            \"coeff\": 1,\n",
    "            \"attr\": \"abs\"\n",
    "        }],\n",
    "        \"abs_energy\": None,\n",
    "        \"sample_entropy\": None\n",
    "    }\n",
    "    fcp_flux = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None,\n",
    "        \"mean_change\": None,\n",
    "        \"mean_abs_change\": None,\n",
    "        \"cid_ce\": [{\"normalize\": True}]\n",
    "    }\n",
    "    fcp_flux_by_flux_ratio_sq = {\n",
    "        \"longest_strike_above_mean\": None,\n",
    "        \"longest_strike_below_mean\": None\n",
    "    }\n",
    "    agg_df_ts = extract_features(\n",
    "        df,\n",
    "        column_id='object_id',\n",
    "        column_sort='mjd',\n",
    "        column_kind='passband',\n",
    "        column_value='flux',\n",
    "        default_fc_parameters=fcp,\n",
    "        n_jobs=6)\n",
    "    agg_df_ts2 = extract_features(\n",
    "        df2,\n",
    "        column_id=\"object_id\",\n",
    "        column_sort=\"mjd\",\n",
    "        column_kind=\"passband\",\n",
    "        column_value=\"flux_norm\",\n",
    "        default_fc_parameters=fcp2,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_flux = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux\",\n",
    "        default_fc_parameters=fcp_flux,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    agg_df_ffrs = extract_features(\n",
    "        df,\n",
    "        column_id=\"object_id\",\n",
    "        column_value=\"flux_by_flux_ratio_sq\",\n",
    "        default_fc_parameters=fcp_flux_by_flux_ratio_sq,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    df_det = df[df['detected'] == 1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(\n",
    "        df_det,\n",
    "        column_id='object_id',\n",
    "        column_value='mjd',\n",
    "        default_fc_parameters={\n",
    "            'maximum': None,\n",
    "            'minimum': None\n",
    "        },\n",
    "        n_jobs=8)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd[\n",
    "        'mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts2.columns = pd.Index([e + \"_norm\" for e in agg_df_ts2.columns])\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd, on='id')\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ts2, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_flux, on=\"id\")\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_ffrs, on=\"id\")\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    agg_df_ts.index.rename('object_id', inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts, on='object_id')\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def cluster_mean_diff(df):\n",
    "    new_df = df.groupby([\"object_id\", \"cluster\"]).agg({\n",
    "        \"flux\": [\"mean\", \"max\", \"min\"]\n",
    "    })\n",
    "    new_df.columns = pd.Index(\n",
    "        [e[0] + \"_\" + e[1] for e in new_df.columns.tolist()])\n",
    "    new_df[\"normalized_mean\"] = new_df[\"flux_mean\"] / (\n",
    "        new_df[\"flux_max\"] - new_df[\"flux_min\"])\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df.groupby(\"object_id\").agg({\"normalized_mean\": \"std\"})\n",
    "\n",
    "\n",
    "def passband_std_difference(df):\n",
    "    std_df = df.groupby([\"object_id\", \"cluster\", \"passband\"]).agg({\n",
    "        \"flux\": \"std\"\n",
    "    }).reset_index().groupby([\"object_id\",\n",
    "                              \"passband\"])[\"flux\"].mean().reset_index()\n",
    "    std_df_max = std_df.groupby(\"object_id\")[\"flux\"].max()\n",
    "    std_df_min = std_df.groupby(\"object_id\")[\"flux\"].min()\n",
    "    return (std_df_max / std_df_min).reset_index()\n",
    "\n",
    "\n",
    "def num_outliers(df):\n",
    "    new_df = df.groupby(\"object_id\").agg({\"flux\": [\"mean\", \"std\"]})\n",
    "    new_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_df.columns])\n",
    "    new_df[\"upper_sigma\"] = new_df[\"flux_mean\"] + new_df[\"flux_std\"]\n",
    "    new_df[\"upper_2sigma\"] = new_df[\"flux_mean\"] + 2 * new_df[\"flux_std\"]\n",
    "    new_df[\"lower_sigma\"] = new_df[\"flux_mean\"] - new_df[\"flux_std\"]\n",
    "    new_df[\"lower_2sigma\"] = new_df[\"flux_mean\"] - 2 * new_df[\"flux_std\"]\n",
    "    new_df.drop([\"flux_mean\", \"flux_std\"], axis=1, inplace=True)\n",
    "    new_df = pd.merge(df, new_df, how=\"left\", on=\"object_id\")\n",
    "    new_df[\"outside_sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_sigma\"])).astype(int)\n",
    "    new_df[\"outside_2sigma\"] = (\n",
    "        (new_df[\"flux\"] > new_df[\"upper_2sigma\"]) |\n",
    "        (new_df[\"flux\"] < new_df[\"lower_2sigma\"])).astype(int)\n",
    "\n",
    "    return_df = new_df.groupby(\"object_id\").agg({\n",
    "        \"outside_sigma\": \"sum\",\n",
    "        \"outside_2sigma\": \"sum\"\n",
    "    })\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "\n",
    "\n",
    "def process_meta(meta_df):\n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df\n",
    "\n",
    "\n",
    "def add_rank_bottom_and_top(df, feature_name):\n",
    "    objid = [\"object_id\"]\n",
    "    columns = [f\"{i}{feature_name}\" for i in range(6)]\n",
    "    partial = df[objid+columns]\n",
    "    partial_values = partial.melt(id_vars=objid, value_vars=columns).sort_values([\"object_id\", \"value\"])\n",
    "    \n",
    "    top_and_bottom = partial_values.groupby(\"object_id\").agg({\n",
    "        \"variable\": [\"first\", \"last\"]\n",
    "    })\n",
    "    top_and_bottom.columns = [\"top\"+feature_name, \"bottom\"+feature_name]\n",
    "    for i, n in zip([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"], columns):\n",
    "        top_and_bottom = top_and_bottom.replace(n, i)\n",
    "    top_and_bottom = top_and_bottom.astype(int)\n",
    "    return top_and_bottom\n",
    "\n",
    "\n",
    "def rank(df, feature_name, thres=20):\n",
    "    objid = [\"object_id\"]\n",
    "    columns = [f\"{i}{feature_name}\" for i in range(6)]\n",
    "    partial = df[objid+columns]\n",
    "    partial_values = partial.melt(id_vars=objid, value_vars=columns).sort_values([\"object_id\", \"value\"])\n",
    "    for i, n in zip([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"], columns):\n",
    "        partial_values.replace(n, i, inplace=True)\n",
    "    partial_values[\"dummy\"] = 1\n",
    "    rank_feats = partial_values.groupby([\"object_id\", \"dummy\"]).agg({\n",
    "        \"variable\": \"sum\"\n",
    "    })\n",
    "    d = dict()\n",
    "    cnt = 0\n",
    "    for i in rank_feats[\"variable\"]:\n",
    "        if i not in d.keys():\n",
    "            d[i] = cnt\n",
    "            cnt += 1\n",
    "    rank_feats.reset_index(inplace=True)\n",
    "    rank_feats.drop(\"dummy\", axis=1, inplace=True)\n",
    "    rank_feats.rename(columns={\"variable\": f\"rank{feature_name}\"}, inplace=True)\n",
    "    rank_feats[f\"rank{feature_name}\"].replace(d, inplace=True)\n",
    "    rank_dict = (rank_feats[f\"rank{feature_name}\"].value_counts() > thres).to_dict()\n",
    "    rank_feats[f\"rank{feature_name}\"] = rank_feats[f\"rank{feature_name}\"].map(\n",
    "        lambda x: x if rank_dict[x] else cnt+1\n",
    "    )\n",
    "    \n",
    "    return rank_feats\n",
    "\n",
    "\n",
    "def add_by_features(df, feature_name, new_feat_name):\n",
    "    for i in range(5):\n",
    "        for j in range(1, 6):\n",
    "            if j > i:\n",
    "                df[f\"{new_feat_name}{j}_by_{i}\"] = df[f\"{j}{feature_name}\"] / df[f\"{i}{feature_name}\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_per_passband(d):\n",
    "    df = d.copy()\n",
    "    df[\"flux_ratio_sq\"] = np.power(df[\"flux\"] / df[\"flux_err\"], 2)\n",
    "    df[\"flux_by_flux_ratio_sq\"] = df[\"flux\"] * df[\"flux_ratio_sq\"]\n",
    "    per_passband_aggs = {\n",
    "        \"flux\": [\"min\", \"max\", \"mean\", \"std\"],\n",
    "        \"flux_ratio_sq\": [\"sum\", \"skew\"],\n",
    "        \"flux_by_flux_ratio_sq\": [\"sum\", \"skew\"]\n",
    "    }\n",
    "    per_pass_agg_df = df.groupby([\"object_id\", \"passband\"]).agg(per_passband_aggs)\n",
    "    per_pass_agg_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in per_pass_agg_df.columns])\n",
    "    per_pass_agg_df[\"flux_diff\"] = per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]\n",
    "    per_pass_agg_df[\"flux_diff2\"] = (\n",
    "        per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_mean\"]\n",
    "    per_pass_agg_df[\"flux_w_mean\"] = per_pass_agg_df[\"flux_by_flux_ratio_sq_sum\"] / per_pass_agg_df[\n",
    "        \"flux_ratio_sq_sum\"\n",
    "    ]\n",
    "    per_pass_agg_df[\"flux_dif3\"] = (\n",
    "    per_pass_agg_df[\"flux_max\"] - per_pass_agg_df[\"flux_min\"]) / per_pass_agg_df[\"flux_w_mean\"]\n",
    "    per_pass_agg_df = per_pass_agg_df.unstack()\n",
    "    per_pass_agg_df.columns = pd.Index([str(e[1]) + \"__\" + e[0] for e in per_pass_agg_df.columns])\n",
    "    basic_columns = [f\"{i}__{j}\" for i in range(6) for j in [\n",
    "        \"flux_min\",\n",
    "        \"flux_max\",\n",
    "        \"flux_mean\",\n",
    "        \"flux_std\",\n",
    "        \"flux_ratio_sq_sum\",\n",
    "        \"flux_ratio_sq_skew\",\n",
    "        \"flux_w_mean\",\n",
    "        \"flux_diff2\"\n",
    "    ]]\n",
    "    per_pass_agg_df.drop(basic_columns, axis=1, inplace=True)\n",
    "    return per_pass_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full(df, meta):\n",
    "    agg_basic = basic(df)\n",
    "    cl_mean_diff = cluster_mean_diff(df)\n",
    "    ps_std_diff = passband_std_difference(df)\n",
    "    num_out = num_outliers(df)\n",
    "\n",
    "    full = pd.merge(agg_basic, cl_mean_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, ps_std_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(agg_basic, num_out, how=\"left\", on=\"object_id\")\n",
    "    meta = process_meta(meta)\n",
    "    full = pd.merge(full, meta, how=\"left\", on=\"object_id\")\n",
    "    full = add_by_features(full, \"__fft_coefficient__coeff_0__attr_\\\"abs\\\"_norm\", \"flux_norm_fft_\")\n",
    "    full = add_by_features(full, \"__abs_energy_norm\", \"abs_energy_\")\n",
    "    full = add_by_features(full, \"__flux_diff\", \"flux_diff_\")\n",
    "    abs_en_rank = rank(full, \"__abs_energy_norm\", 0)\n",
    "    flux_dif_rank = rank(full, \"__flux_diff\")\n",
    "    \n",
    "    flux_diff = add_rank_bottom_and_top(full, \"__flux_diff\")\n",
    "    flux_dif3 = add_rank_bottom_and_top(full, \"__flux_dif3\")\n",
    "    full = pd.merge(full, abs_en_rank, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, flux_dif_rank, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, flux_diff, how=\"left\", on=\"object_id\")\n",
    "    full = pd.merge(full, flux_dif3, how=\"left\", on=\"object_id\")\n",
    "    if \"target\" in full.columns:\n",
    "        full.drop(\"target\", axis=1, inplace=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_data(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = meta.target\n",
    "    classes = sorted(y.unique())\n",
    "    class_weight = {c: 1 for c in classes}\n",
    "\n",
    "    for c in [64, 15]:\n",
    "        class_weight[c] = 2\n",
    "    oof_df = full[[\"object_id\"]]\n",
    "    del full['object_id'], full['distmod'], full['hostgal_specz']\n",
    "    del full['ra'], full['decl'], full['gal_l'], full['gal_b'], full['ddf']\n",
    "    return full, y, classes, class_weight, oof_df\n",
    "\n",
    "\n",
    "def train_data_n(df, meta):\n",
    "    full = get_full(df, meta)\n",
    "    y = df.groupby(\"object_id\").novae.mean()\n",
    "    del full[\"object_id\"], full[\"distmod\"], full[\"hostgal_specz\"]\n",
    "    del full[\"ra\"], full[\"decl\"], full[\"gal_l\"], full[\"gal_b\"], full[\"ddf\"]\n",
    "    return full, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  5.42it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:24<00:00,  1.66it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.26it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 14.73it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:00<00:00, 60.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.2 s, sys: 5.35 s, total: 1min 4s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full, y = train_data_n(train, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>...</th>\n",
       "      <th>flux_diff_5_by_2</th>\n",
       "      <th>flux_diff_4_by_3</th>\n",
       "      <th>flux_diff_5_by_3</th>\n",
       "      <th>flux_diff_5_by_4</th>\n",
       "      <th>rank__abs_energy_norm</th>\n",
       "      <th>rank__flux_diff</th>\n",
       "      <th>top__flux_diff</th>\n",
       "      <th>bottom__flux_diff</th>\n",
       "      <th>top__flux_dif3</th>\n",
       "      <th>bottom__flux_dif3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>3.835268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619088</td>\n",
       "      <td>0.823590</td>\n",
       "      <td>0.820379</td>\n",
       "      <td>0.996101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>1.998217</td>\n",
       "      <td>...</td>\n",
       "      <td>1.407103</td>\n",
       "      <td>0.932131</td>\n",
       "      <td>1.221587</td>\n",
       "      <td>1.310531</td>\n",
       "      <td>1</td>\n",
       "      <td>548</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>1.990851</td>\n",
       "      <td>...</td>\n",
       "      <td>2.787751</td>\n",
       "      <td>1.204789</td>\n",
       "      <td>1.704010</td>\n",
       "      <td>1.414364</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>1.819875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680688</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>0.728946</td>\n",
       "      <td>0.762138</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>2.214854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.103552</td>\n",
       "      <td>1.122882</td>\n",
       "      <td>0.841532</td>\n",
       "      <td>0.749439</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      flux_min    flux_max   flux_mean  flux_median    flux_std  flux_skew  \\\n",
       "0 -1100.440063  660.626343 -123.096998   -89.477524  394.109851  -0.349540   \n",
       "1   -14.735178   14.770886   -1.423351    -0.873033    6.471144   0.014989   \n",
       "2   -19.159811   47.310059    2.267434     0.409172    8.022239   3.177854   \n",
       "3   -15.494463  220.795212    8.909206     1.035895   27.558208   4.979826   \n",
       "4   -16.543753  143.600189    7.145702     1.141288   20.051722   4.406298   \n",
       "\n",
       "   flux_err_min  flux_err_max  flux_err_mean  flux_err_median  \\\n",
       "0      2.130510     12.845472       4.482743         3.835268   \n",
       "1      0.639458      9.115748       2.359620         1.998217   \n",
       "2      0.695106     11.281384       2.471061         1.990851   \n",
       "3      0.567170     55.892746       2.555576         1.819875   \n",
       "4      0.695277     11.383690       2.753004         2.214854   \n",
       "\n",
       "         ...          flux_diff_5_by_2  flux_diff_4_by_3  flux_diff_5_by_3  \\\n",
       "0        ...                  0.619088          0.823590          0.820379   \n",
       "1        ...                  1.407103          0.932131          1.221587   \n",
       "2        ...                  2.787751          1.204789          1.704010   \n",
       "3        ...                  0.680688          0.956449          0.728946   \n",
       "4        ...                  1.103552          1.122882          0.841532   \n",
       "\n",
       "   flux_diff_5_by_4  rank__abs_energy_norm  rank__flux_diff  top__flux_diff  \\\n",
       "0          0.996101                      0                0               0   \n",
       "1          1.310531                      1              548               2   \n",
       "2          1.414364                      2                2               1   \n",
       "3          0.762138                      3                3               0   \n",
       "4          0.749439                      4                4               0   \n",
       "\n",
       "   bottom__flux_diff  top__flux_dif3  bottom__flux_dif3  \n",
       "0                  1               0                  1  \n",
       "1                  0               5                  1  \n",
       "2                  5               4                  0  \n",
       "3                  2               1                  5  \n",
       "4                  4               3                  0  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean = full.mean(axis=0)\n",
    "full.fillna(0, inplace=True)\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object_id\n",
       "615     0\n",
       "713     0\n",
       "730     1\n",
       "745     1\n",
       "1124    1\n",
       "Name: novae, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(full, seed=7):\n",
    "    folds = StratifiedKFold(n_splits=7, shuffle=True, random_state=seed)\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "\n",
    "    lgb_params = {\n",
    "        'device': 'cpu', \n",
    "        'objective': 'binary',  \n",
    "        'boosting_type': 'gbdt', \n",
    "        'n_jobs': -1, \n",
    "        'max_depth': 7, \n",
    "        'n_estimators': 500, \n",
    "        'subsample_freq': 2, \n",
    "        'subsample_for_bin': 5000, \n",
    "        'min_data_per_group': 100, \n",
    "        'max_cat_to_onehot': 4, \n",
    "        'cat_l2': 1.0, \n",
    "        'cat_smooth': 59.5, \n",
    "        'max_cat_threshold': 32, \n",
    "        'metric_freq': 10, \n",
    "        'verbosity': -1, \n",
    "        'metric': 'binary', \n",
    "        'xgboost_dart_mode': False, \n",
    "        'uniform_drop': False, \n",
    "        'colsample_bytree': 0.5, \n",
    "        'drop_rate': 0.173, \n",
    "        'learning_rate': 0.0267, \n",
    "        'max_drop': 5, \n",
    "        'min_child_samples': 10, \n",
    "        'min_child_weight': 100.0, \n",
    "        'min_split_gain': 0.1, \n",
    "        'num_leaves': 7, \n",
    "        'reg_alpha': 0.1, \n",
    "        'reg_lambda': 0.00023, \n",
    "        'skip_drop': 0.44, \n",
    "        'subsample': 0.75\n",
    "    }\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i: np.sum(w) / w[i] for i in w.index}\n",
    "    oof_preds = np.zeros((len(full), np.unique(y).shape[0]))\n",
    "\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x,\n",
    "            trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights),\n",
    "            categorical_feature=[\n",
    "                \"top__flux_diff\",\n",
    "                \"bottom__flux_diff\",\n",
    "                \"top__flux_dif3\",\n",
    "                \"bottom__flux_dif3\",\n",
    "                \"rank__abs_energy_norm\",\n",
    "                \"rank__flux_diff\"\n",
    "            ]\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(\n",
    "            val_x, num_iteration=clf.best_iteration_)\n",
    "        print(f1_score(val_y, np.argmax(oof_preds[val_, :], axis=1)))\n",
    "\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = full.columns\n",
    "        imp_df['gain'] = clf.feature_importances_\n",
    "        imp_df['fold'] = fold_ + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "        clfs.append(clf)\n",
    "\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % f1_score(\n",
    "        y, np.argmax(oof_preds, axis=1)))\n",
    "    return clfs, importances, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_av_feature_selection(features, seeds=[1,3,5,7,9]):\n",
    "    clfs_ = []\n",
    "    importances_ = []\n",
    "    oofs = []\n",
    "    for s in seeds:\n",
    "        clfs, imp, oof = model_(features, s)\n",
    "        clfs_.append(clfs)\n",
    "        importances_.append(imp)\n",
    "        oofs.append(oof)\n",
    "    return clfs_, importances_, oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9856938483547926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857346647646219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765124555160143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801136363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977904490377762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984240687679083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/Users/hidehisa/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1188: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['bottom__flux_dif3', 'bottom__flux_diff', 'rank__abs_energy_norm', 'rank__flux_diff', 'top__flux_dif3', 'top__flux_diff']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801699716713882\n",
      "MULTI WEIGHTED LOG LOSS : 0.98147 \n"
     ]
    }
   ],
   "source": [
    "clfs_, importances_, oofs = seed_av_feature_selection(full, [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.logical_and(0.2 < oofs[0][:, 0], 0.8 > oofs[0][:, 0] )).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3028903, 0.6971097])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oofs[0][7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
